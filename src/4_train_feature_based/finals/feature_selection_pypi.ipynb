{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r'D:\\Projects\\aaa')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed() \n",
    "\n",
    "from src.utils.utils import get_files_in_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "allx = r'D:\\Projects\\aaa\\results\\rq4_results\\features.csv'\n",
    "npm = r'D:\\Projects\\aaa\\results\\rq4_results\\features_npm.csv'\n",
    "pypi = r'D:\\Projects\\aaa\\results\\rq4_results\\features_pypi.csv'\n",
    "mvn = r'D:\\Projects\\aaa\\results\\rq4_results\\features_mvn.csv'\n",
    "\n",
    "df_all = pd.read_csv(pypi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanitize time_to_next_merge where nan is a valid value\n",
    "max_merge = df_all['time_to_next_merge'].max() * 10\n",
    "df_all.loc[df_all['time_to_next_merge'].isna(),'time_to_next_merge'] = max_merge\n",
    "\n",
    "df_all.fillna(0, inplace=True)\n",
    "df_all.fillna(0, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13175\n",
      "268\n"
     ]
    }
   ],
   "source": [
    "print(df_all.shape[0])\n",
    "print((df_all['label_security_related']==True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12536, 161) 250\n",
      "(639, 161) 18\n"
     ]
    }
   ],
   "source": [
    "_byrepo = df_all.groupby('label_repo_full_name')\n",
    "repos = df_all['label_repo_full_name'].unique()\n",
    "train_repos = random.sample(list(repos), int(0.9*len(repos)))\n",
    "df = df_all[df_all.apply(lambda x: x['label_repo_full_name'] in train_repos, axis=1)]\n",
    "eval_df = df_all[df_all.apply(lambda x: x['label_repo_full_name'] not in train_repos, axis=1)]\n",
    "\n",
    "print(df.shape, (df['label_security_related']==True).sum())\n",
    "print(eval_df.shape, (eval_df['label_security_related']==True).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_ratio = {col:0 for col in df}\n",
    "\n",
    "# for col in df:\n",
    "#     nan_counts = df[col].isna().sum()\n",
    "#     nan_ratio[col] = nan_counts/df.shape[0]\n",
    "\n",
    "# nan_ratio = {k: v for k,v in sorted(nan_ratio.items(), key=lambda kv: -kv[1])}\n",
    "# for k,v in nan_ratio.item():\n",
    "#     if v > 0.1:\n",
    "#         print(nan_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_features = [\n",
    "    'commits_to_next_merge',\n",
    "    'commits_since_last_merge',\n",
    "    'commits_to_next_merge',\n",
    "    'commits_since_last_merge',\n",
    "    'file_changed_method_count_avg',\n",
    "    'file_changed_method_count_max',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_variance_features = [\n",
    "    'deadlock_in_patch',\n",
    "    'has_mvn_code',\n",
    "    'has_mvn_like_code',\n",
    "    'has_npm_like_code',\n",
    "    'has_pypi_like_code',\n",
    "\n",
    "    'deadlock_in_message',\n",
    "\n",
    "    'methods_with_attack_count_avg',\n",
    "    'methods_with_attack_count_max',\n",
    "    'methods_with_corrupt_count_avg',\n",
    "    'methods_with_crash_count_avg',\n",
    "    'methods_with_deadlock_count_avg',\n",
    "    'methods_with_exploit_count_avg',\n",
    "    'methods_with_exploit_count_max',\n",
    "    'methods_with_leak_count_avg',\n",
    "    'methods_with_malicious_count_avg',\n",
    "    'methods_with_malicious_count_max',\n",
    "    'methods_with_segfault_count_avg',\n",
    "    'methods_with_segfault_count_max',\n",
    "    'malicious_in_patch',\n",
    "    'segfault_in_patch',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_correlated_features = [\n",
    "    'added_lines_ratio_avg',\n",
    "    'added_lines_ratio_max',\n",
    "\n",
    "    'avg_method_parameter_count_avg',\n",
    "    'avg_method_parameter_count_max',\n",
    "    'avg_method_token_count_avg',\n",
    "    'avg_method_token_count_max',\n",
    "    'avg_method_complexity_avg',\n",
    "\n",
    "    'methods_with_vulnerab_count_max',\n",
    "    'modified_lines_ratio_max',\n",
    "    'max_method_parameter_count_max',\n",
    "\n",
    "    'file_complexity_max',\n",
    "    'file_complexity_avg',\n",
    "    'max_method_nloc_avg',\n",
    "    'max_method_nloc_max',\n",
    "    'file_nloc_avg',\n",
    "    'file_nloc_max',\n",
    "\n",
    "    'secur_in_title',\n",
    "    'vulnerab_in_title',\n",
    "    'exploit_in_title',\n",
    "    'certificat_in_title',\n",
    "    'authent_in_title',\n",
    "    'leak_in_title',\n",
    "    'sensit_in_title',\n",
    "    'crash_in_title',\n",
    "    'attack_in_title',\n",
    "    'deadlock_in_title',\n",
    "    'segfault_in_title',\n",
    "    'malicious_in_title',\n",
    "    'corrupt_in_title',\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_selected = df\n",
    "df_selected = df_selected[df_selected.columns.difference(broken_features)]\n",
    "df_selected = df_selected[df_selected.columns.difference(no_variance_features)]\n",
    "df_selected = df_selected[df_selected.columns.difference(highly_correlated_features)]\n",
    "\n",
    "X = df_selected[df_selected.columns.difference(['label_repo_full_name', 'label_sha', 'label_commit_date', 'label_security_related'])]\n",
    "y = df_selected['label_security_related']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reject 'attack_in_title',\n",
      "reject 'corrupt_in_message',\n",
      "reject 'corrupt_in_title',\n",
      "reject 'deadlock_in_title',\n",
      "reject 'exploit_in_message',\n",
      "reject 'exploit_in_title',\n",
      "reject 'malicious_in_message',\n",
      "reject 'malicious_in_title',\n",
      "reject 'segfault_in_message',\n",
      "reject 'segfault_in_title',\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0.001)\n",
    "sel.fit(X)\n",
    "sup = sel.get_support()\n",
    "i = -1\n",
    "for x in X:\n",
    "       i+=1\n",
    "       if not sup[i]:\n",
    "              print(f'reject \\'{x}\\',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = ['corrupt_in_message', 'deadlock_in_message', 'exploit_in_message', 'malicious_in_message', 'segfault_in_message']\n",
    "# df_sec = df[df['label_security_related'] == True]\n",
    "# for col in columns:\n",
    "#     print(df_sec[col].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reject added_lines_count_avg   added_lines_count_max   0.8946420745215757\n",
      "reject added_lines_count_avg   changed_lines_count_max   0.8377765440316571\n",
      "reject added_lines_count_max   changed_lines_count_max   0.9342811754748904\n",
      "reject added_lines_ratio_avg   added_lines_ratio_max   0.7909161138486801\n",
      "reject added_lines_ratio_avg   removed_lines_ratio_avg   -0.9898624144081328\n",
      "reject added_lines_ratio_max   removed_lines_ratio_avg   -0.7789118279226971\n",
      "reject authent_in_message   authent_in_title   0.8372416622156089\n",
      "reject avg_method_complexity_avg   avg_method_nloc_avg   0.7653618291233982\n",
      "reject avg_method_complexity_avg   max_method_complexity_avg   0.9017928232390353\n",
      "reject avg_method_complexity_max   avg_method_nloc_max   0.8009976404570783\n",
      "reject avg_method_nloc_avg   avg_method_token_count_avg   0.8585447726461289\n",
      "reject avg_method_nloc_avg   max_method_nloc_avg   0.7607107750328206\n",
      "reject avg_method_nloc_max   max_method_nloc_max   0.8144154857698844\n",
      "reject avg_method_parameter_count_avg   max_method_parameter_count_avg   0.9079217034194474\n",
      "reject avg_method_parameter_count_max   max_method_parameter_count_max   0.8755214278613368\n",
      "reject avg_method_token_count_avg   max_method_token_count_avg   0.7974496656445034\n",
      "reject avg_method_token_count_max   max_method_token_count_max   0.7979777887464234\n",
      "reject certificat_in_message   certificat_in_title   0.8480528285431285\n",
      "reject changed_lines_count_avg   file_size_avg   0.7509565369588523\n",
      "reject changed_lines_count_avg   file_size_max   0.812814361702711\n",
      "reject changed_lines_count_avg   removed_lines_count_avg   0.9630772179761932\n",
      "reject changed_lines_count_avg   removed_lines_count_max   0.9404524950897435\n",
      "reject changes_to_file_in_next_50_commits_avg   changes_to_file_in_next_50_commits_max   0.78505531412483\n",
      "reject changes_to_file_in_prev_50_commits_avg   changes_to_file_in_prev_50_commits_max   0.7726045620093953\n",
      "reject crash_in_message   crash_in_title   0.8768477692694145\n",
      "reject deadlock_in_message   deadlock_in_title   0.919792765663681\n",
      "reject exploit_in_message   exploit_in_title   0.7745348622061378\n",
      "reject file_complexity_avg   file_complexity_max   0.8349853727831357\n",
      "reject file_complexity_avg   file_token_count_avg   0.979388347290799\n",
      "reject file_complexity_avg   file_token_count_max   0.8445272114364236\n",
      "reject file_complexity_avg   total_methods_count_avg   0.9777735766895211\n",
      "reject file_complexity_avg   total_methods_count_max   0.830208894929856\n",
      "reject file_complexity_max   file_token_count_avg   0.8126962268234468\n",
      "reject file_complexity_max   file_token_count_max   0.9671456377722363\n",
      "reject file_complexity_max   total_methods_count_avg   0.8032762718857624\n",
      "reject file_complexity_max   total_methods_count_max   0.9716125028121132\n",
      "reject file_nloc_avg   file_nloc_max   0.752811223371969\n",
      "reject file_size_avg   file_size_max   0.7702975214365962\n",
      "reject file_size_avg   removed_lines_count_avg   0.7691249280619108\n",
      "reject file_size_avg   removed_lines_count_max   0.7811672123098045\n",
      "reject file_size_max   removed_lines_count_max   0.756891787590721\n",
      "reject file_token_count_avg   file_token_count_max   0.85424909160104\n",
      "reject file_token_count_avg   total_methods_count_avg   0.9690596302405923\n",
      "reject file_token_count_avg   total_methods_count_max   0.8247069274863739\n",
      "reject file_token_count_max   total_methods_count_avg   0.825498338111999\n",
      "reject file_token_count_max   total_methods_count_max   0.9778721655151247\n",
      "reject is_add   is_modify   -0.8145500874228622\n",
      "reject leak_in_message   leak_in_title   0.8120507934629028\n",
      "reject max_method_nloc_avg   max_method_token_count_avg   0.8489688284159725\n",
      "reject methods_with_corrupt_count_max   methods_with_deadlock_count_max   0.8009977217916286\n",
      "reject modified_lines_ratio_avg   modified_lines_ratio_max   0.7620937619085679\n",
      "reject removed_lines_count_avg   removed_lines_count_max   0.9784003999565427\n",
      "reject secur_in_message   secur_in_title   0.8253709599070889\n",
      "reject test_in_filename   test_in_path   0.8116537876663589\n",
      "reject total_methods_count_avg   total_methods_count_max   0.8284841821058146\n",
      "reject vulnerab_in_message   vulnerab_in_title   0.8788735124711214\n"
     ]
    }
   ],
   "source": [
    "correlation_matrix = X.corr()\n",
    "corr = correlation_matrix.values\n",
    "column_names = correlation_matrix.columns\n",
    "\n",
    "for i in range(len(column_names)):\n",
    "    for j in range(i+1, len(column_names)):\n",
    "        if abs(corr[i,j])> 0.75:\n",
    "            print('reject', column_names[i], ' ', column_names[j], ' ', corr[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # 2:4 or 1:2\n",
    "# # manual rebalancing\n",
    "# oversample_ratio = 2\n",
    "# undersample_ratio = 4\n",
    "\n",
    "# x_positive = X.where(y).dropna()\n",
    "# x_negative = X.where(y==False).dropna()\n",
    "\n",
    "# x_positive = pd.concat([x_positive for i in range(oversample_ratio)])\n",
    "# x_negative = x_negative.sample(len(x_positive)*undersample_ratio)\n",
    "\n",
    "# X_resampled = pd.concat([x_negative, x_positive])\n",
    "# y_resampled = np.array([False]*len(x_negative) + [True]*len(x_positive))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "\n",
    "\n",
    "# 2:4 or 1:2\n",
    "# rebalancing with SMOTE AID\n",
    "oversample_ratio = 1\n",
    "undersample_ratio = 2\n",
    "\n",
    "sampler = SMOTE()\n",
    "\n",
    "x_positive = X.where(y).dropna()\n",
    "x_negative = X.where(y==False).dropna()\n",
    "\n",
    "x_negative_temp = x_negative.sample(len(x_positive)*oversample_ratio)\n",
    "X_resampled_temp = pd.concat([x_negative_temp, x_positive])\n",
    "y_resampled_temp = np.array([False]*len(x_negative_temp) + [True]*len(x_positive))\n",
    "\n",
    "X_resampled_temp, y_resampled_temp = sampler.fit_resample(X_resampled_temp, y_resampled_temp)\n",
    "\n",
    "x_negative_temp2 = x_negative.sample(len(X_resampled_temp)*undersample_ratio)\n",
    "X_resampled = np.concatenate([X_resampled_temp, x_negative_temp2])\n",
    "y_resampled = np.concatenate([y_resampled_temp, [False]*(len(X_resampled_temp)*undersample_ratio)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_positive.shape, x_negative.shape, X_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler().fit(X_resampled)\n",
    "X_resampled = scaler.transform(X_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "VERBOSE_LVL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 97 features.\n",
      "Fitting estimator with 96 features.\n",
      "Fitting estimator with 95 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 91 features.\n",
      "Fitting estimator with 90 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 87 features.\n",
      "Fitting estimator with 86 features.\n",
      "Fitting estimator with 85 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 81 features.\n",
      "Fitting estimator with 80 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 77 features.\n",
      "Fitting estimator with 76 features.\n",
      "Fitting estimator with 75 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 71 features.\n",
      "Fitting estimator with 70 features.\n",
      "max f1: 0.6501444736281481\n"
     ]
    }
   ],
   "source": [
    "# Train selector & RF\n",
    "    \n",
    "model = RandomForestClassifier(\n",
    "    n_estimators = 100,\n",
    "    random_state=42)\n",
    "\n",
    "selector = RFECV(model, \n",
    "    step=1, \n",
    "    cv=5,\n",
    "    min_features_to_select=40,\n",
    "    scoring = 'f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=VERBOSE_LVL)\n",
    "\n",
    "selector.fit(X_resampled, y_resampled)\n",
    "\n",
    "print('max f1:', max(selector.cv_results_['mean_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_keys(['mean_test_score', 'std_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score'])\n",
    "# print(selector.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features\n",
      "added_lines_count_avg\n",
      "added_lines_count_max\n",
      "attack_in_file_content\n",
      "attack_in_message\n",
      "authent_in_file_content\n",
      "authent_in_message\n",
      "authent_in_patch\n",
      "author_in_top_100\n",
      "author_to_commiter_date_diff\n",
      "avg_method_complexity_max\n",
      "avg_method_nloc_avg\n",
      "avg_method_nloc_max\n",
      "certificat_in_file_content\n",
      "changed_files\n",
      "changed_lines_count_avg\n",
      "changed_lines_count_max\n",
      "changed_methods_count_avg\n",
      "changed_methods_count_max\n",
      "changes_to_file_in_next_50_commits_avg\n",
      "changes_to_file_in_next_50_commits_max\n",
      "changes_to_file_in_prev_50_commits_avg\n",
      "changes_to_file_in_prev_50_commits_max\n",
      "commits_next_30_days\n",
      "commits_next_7_days\n",
      "commits_prev_7_days\n",
      "corrupt_in_file_content\n",
      "crash_in_file_content\n",
      "dmm_unit_complexity\n",
      "dmm_unit_interfacing\n",
      "dmm_unit_size\n",
      "file_size_avg\n",
      "file_size_max\n",
      "file_token_count_avg\n",
      "file_token_count_max\n",
      "has_pypi_code\n",
      "is_add\n",
      "is_file_recently_added\n",
      "is_modify\n",
      "leak_in_file_content\n",
      "leak_in_message\n",
      "malicious_in_file_content\n",
      "max_method_complexity_avg\n",
      "max_method_complexity_max\n",
      "max_method_parameter_count_avg\n",
      "max_method_token_count_avg\n",
      "max_method_token_count_max\n",
      "modified_lines_count_avg\n",
      "modified_lines_count_max\n",
      "modified_lines_ratio_avg\n",
      "removed_lines_count_avg\n",
      "removed_lines_count_max\n",
      "removed_lines_ratio_avg\n",
      "removed_lines_ratio_max\n",
      "same_author_as_commiter\n",
      "secur_in_file_content\n",
      "secur_in_message\n",
      "secur_in_patch\n",
      "segfault_in_message\n",
      "sensit_in_file_content\n",
      "test_in_filename\n",
      "test_in_path\n",
      "time_to_next_commit\n",
      "time_to_next_merge\n",
      "time_to_prev_commit\n",
      "total_methods_count_avg\n",
      "total_methods_count_max\n",
      "vulnerab_in_file_content\n",
      "vulnerab_in_message\n",
      "vulnerab_in_patch\n"
     ]
    }
   ],
   "source": [
    "colums = list(X.columns)\n",
    "\n",
    "print('Selected features')\n",
    "for i in range(len(selector.support_)):\n",
    "    if selector.support_[i]:\n",
    "        print(colums[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 104) (1258, 104)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_eval_selected = eval_df\n",
    "df_eval_selected = df_eval_selected[df_eval_selected.columns.difference(broken_features)]\n",
    "df_eval_selected = df_eval_selected[df_eval_selected.columns.difference(no_variance_features)]\n",
    "df_eval_selected = df_eval_selected[df_eval_selected.columns.difference(highly_correlated_features)]\n",
    "\n",
    "eval_X = df_eval_selected[df_eval_selected.columns.difference(['label_repo_full_name', 'label_sha', 'label_commit_date', 'label_security_related'])]\n",
    "eval_y = df_eval_selected['label_security_related']\n",
    "\n",
    "\n",
    "eval_x_positive = eval_X.where(eval_y).dropna()\n",
    "eval_x_negative = eval_X.where(eval_y==False).dropna()\n",
    "print(eval_x_positive.shape, eval_x_negative.shape)\n",
    "\n",
    "eval_x_negative_sampled =  eval_x_negative.sample(len(eval_x_positive)*undersample_ratio)\n",
    "\n",
    "eval_X_balanced = pd.concat([eval_x_negative_sampled, eval_x_positive])\n",
    "eval_y_balanced = np.array([False]*len(eval_x_negative_sampled) + [True]*len(eval_x_positive))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF train 1.0 1.0\n",
      "RF test 0.0 0.0\n",
      "RF eval 0.15625 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but RobustScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but RFECV was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but RFECV was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# eval RF selector\n",
    "\n",
    "eval_scaled = scaler.transform(eval_X)\n",
    "eval_selected_scaled = selector.transform(eval_scaled)\n",
    "X_resampled_selected = selector.transform(X_resampled)\n",
    "eval_X_balanced_selected = selector.transform(eval_X_balanced)\n",
    "\n",
    "train_preds = selector.predict(X_resampled)\n",
    "recall = recall_score(y_resampled, train_preds)\n",
    "precision = precision_score(y_resampled, train_preds)\n",
    "print('RF train', recall, precision)\n",
    "\n",
    "eval_balanced_y_pred = selector.predict(eval_X_balanced)\n",
    "recall = recall_score(eval_y_balanced, eval_balanced_y_pred)\n",
    "precision = precision_score(eval_y_balanced, eval_balanced_y_pred)\n",
    "print('RF test', recall, precision)\n",
    "\n",
    "eval_y_pred = selector.predict(eval_scaled)\n",
    "recall = recall_score(eval_y, eval_y_pred)\n",
    "precision = precision_score(eval_y, eval_y_pred)\n",
    "\n",
    "print('RF eval', recall, precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC_model = SVC(\n",
    "#     kernel ='linear',\n",
    "#     cache_size=1000,\n",
    "#     random_state=42)\n",
    "\n",
    "# SVC_model.fit(X_resampled_selected, y_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC_model_2 = SVC(\n",
    "#     kernel ='sigmoid',\n",
    "#     random_state=42)\n",
    "\n",
    "# SVC_model_2.fit(X_resampled_selected, y_resampled)\n",
    "\n",
    "# train_preds = SVC_model_2.predict(X_resampled_selected)\n",
    "# recall = recall_score(y_resampled, train_preds)\n",
    "# precision = precision_score(y_resampled, train_preds)\n",
    "# print('SVC2 train', recall, precision)\n",
    "\n",
    "# eval_balanced_y_pred = SVC_model_2.predict(eval_X_balanced_selected)\n",
    "# recall = recall_score(eval_y_balanced, eval_balanced_y_pred)\n",
    "# precision = precision_score(eval_y_balanced, eval_balanced_y_pred)\n",
    "# print('SVC2 test', recall, precision)\n",
    "\n",
    "# eval_y_pred = SVC_model_2.predict(eval_selected_scaled)\n",
    "# recall = recall_score(eval_y, eval_y_pred)\n",
    "# precision = precision_score(eval_y, eval_y_pred)\n",
    "# print('SVC2 eval', recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_preds = SVC_model.predict(X_resampled_selected)\n",
    "# recall = recall_score(y_resampled, train_preds)\n",
    "# precision = precision_score(y_resampled, train_preds)\n",
    "# print('SVC train', recall, precision)\n",
    "\n",
    "# eval_balanced_y_pred = SVC_model.predict(eval_X_balanced_selected)\n",
    "# recall = recall_score(eval_y_balanced, eval_balanced_y_pred)\n",
    "# precision = precision_score(eval_y_balanced, eval_balanced_y_pred)\n",
    "# print('SVC test', recall, precision)\n",
    "\n",
    "# eval_y_pred = SVC_model.predict(eval_selected_scaled)\n",
    "# recall = recall_score(eval_y, eval_y_pred)\n",
    "# precision = precision_score(eval_y, eval_y_pred)\n",
    "# print('SVC test', recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=20000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=20000, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=20000, random_state=42)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_model = LogisticRegression(\n",
    "    penalty ='l2',\n",
    "    max_iter = 20000,\n",
    "    random_state=42)\n",
    "\n",
    "LR_model.fit(X_resampled_selected, y_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR train 0.18181818181818182 0.6919831223628692\n",
      "LR test 0.4375 0.19444444444444445\n",
      "LR eval 0.1875 0.4\n"
     ]
    }
   ],
   "source": [
    "train_preds = LR_model.predict(X_resampled_selected)\n",
    "recall = recall_score(y_resampled, train_preds)\n",
    "precision = precision_score(y_resampled, train_preds)\n",
    "print('LR train', recall, precision)\n",
    "\n",
    "\n",
    "eval_balanced_y_pred = LR_model.predict(eval_X_balanced_selected)\n",
    "recall = recall_score(eval_y_balanced, eval_balanced_y_pred)\n",
    "precision = precision_score(eval_y_balanced, eval_balanced_y_pred)\n",
    "print('LR test', recall, precision)\n",
    "\n",
    "eval_y_pred = LR_model.predict(eval_selected_scaled)\n",
    "recall = recall_score(eval_y, eval_y_pred)\n",
    "precision = precision_score(eval_y, eval_y_pred)\n",
    "print('LR eval', recall, precision)\n",
    "\n",
    "# eval_y_pred = LR_model.predict(eval_selected_scaled)\n",
    "# recall = recall_score(eval_y, eval_y_pred)\n",
    "# precision = precision_score(eval_y, eval_y_pred)\n",
    "# print('LR', recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1290\n"
     ]
    }
   ],
   "source": [
    "print(len(eval_selected_scaled)) #3308\n",
    "# (3308, 161) 45 => \n",
    "\n",
    "# 3076 commits ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_y_pred4 = selector.predict(eval_scaled)\n",
    "eval_y_pred = [False]*len(eval_y_pred4)\n",
    "\n",
    "for x in range(len(eval_y_pred4)):\n",
    "    if eval_y_pred4[x]:\n",
    "        eval_y_pred[x]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15625\n"
     ]
    }
   ],
   "source": [
    "x_true = [False]*len(eval_y)\n",
    "eval_y2 = eval_y.reset_index()\n",
    "for x in range(len(eval_y2)):\n",
    "    if eval_y_pred4[x] and eval_y2.iloc[x]['label_security_related']:\n",
    "        x_true[x]=True\n",
    "    \n",
    "true_positives = sum(1  for x in x_true if x)\n",
    "al_positives = (eval_df['label_security_related']==True).sum()\n",
    "print(true_positives/al_positives) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ix = 0\n",
    "shas = []\n",
    "for i, x in eval_df.iterrows():\n",
    "    shas.append({\n",
    "        'label_sha':x['label_sha'],\n",
    "        'label_repo_full_name':x['label_repo_full_name'],\n",
    "        'classification_pred': eval_y_pred[ix]\n",
    "    })\n",
    "\n",
    "    ix += 1\n",
    "\n",
    "with open('classificated_commits.json', 'w') as f:\n",
    "    json.dump(shas, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LR_model_all.joblib']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "dump(LR_model, 'LR_model_all.joblib') \n",
    "# dump(SVC_model_2, 'SVC_model_2_all.joblib') \n",
    "# dump(SVC_model, 'SVC_model_all.joblib') \n",
    "dump(LR_model, 'LR_model_all.joblib') \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
