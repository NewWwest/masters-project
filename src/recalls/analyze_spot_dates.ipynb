{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dateutil.parser as parser\n",
    "from statistics import mean, median\n",
    "import copy\n",
    "\n",
    "path_to_file = r'data\\revall_analysis.json'\n",
    "path_to_repository_info = r'data\\repository_info.json'\n",
    "\n",
    "seconds_in_a_day = 24*3600\n",
    "with open(path_to_file, 'r') as f:\n",
    "    recall_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subset = recall_data\n",
    "npm_subset = {k:v for k,v in recall_data.items() if 'ecosystem' in v and 'npm' in v['ecosystem']}\n",
    "pypi_subset = {k:v for k,v in recall_data.items() if 'ecosystem' in v and 'pypi' in v['ecosystem']}\n",
    "mvn_subset = {k:v for k,v in recall_data.items() if 'ecosystem' in v and 'maven' in v['ecosystem']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delay_no_separation(delays_data_subset, date_key, label):\n",
    "    for x in delays_data_subset:\n",
    "        if 'creation_date' in delays_data_subset[x] and delays_data_subset[x]['creation_date'] and date_key in delays_data_subset[x] and delays_data_subset[x][date_key]:\n",
    "            creation_date = parser.parse(delays_data_subset[x]['creation_date'])\n",
    "            ref_date = min([parser.parse(y[1])for y in delays_data_subset[x][date_key]])\n",
    "            diff = creation_date-ref_date\n",
    "            delays = diff.total_seconds()/seconds_in_a_day\n",
    "            delays_data_subset[x][f'{date_key}_diff'] = delays\n",
    "        \n",
    "    ids_all = len([1 for x in delays_data_subset if 'creation_date' in delays_data_subset[x] and delays_data_subset[x]['creation_date']])\n",
    "    ids_with_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]])\n",
    "    ids_with_positive_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>0])\n",
    "    ids_with_week_old_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>7])\n",
    "\n",
    "    diffs = [delays_data_subset[x][f'{date_key}_diff'] for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]]\n",
    "\n",
    "    print(label)\n",
    "    print(date_key)\n",
    "    print('count', ids_with_diffs)\n",
    "    print('median', median(diffs))\n",
    "    print('recall at 0', ids_with_positive_diffs/ids_all)\n",
    "    print('recall at 7', ids_with_week_old_diffs/ids_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delays_vs_with_issues(delays_data_subset, date_key, label):\n",
    "    for x in delays_data_subset:\n",
    "        if 'creation_date' in delays_data_subset[x] and delays_data_subset[x]['creation_date'] and date_key in delays_data_subset[x] and delays_data_subset[x][date_key]:\n",
    "            creation_date = parser.parse(delays_data_subset[x]['creation_date'])\n",
    "            ref_date = min([parser.parse(y['ref_date'])for y in delays_data_subset[x][date_key]])\n",
    "            # ref_date = min([parser.parse(y[1])for y in delays_data_subset[x][date_key]])\n",
    "            diff = creation_date-ref_date\n",
    "            delays = diff.total_seconds()/seconds_in_a_day\n",
    "            delays_data_subset[x][f'{date_key}_diff'] = delays\n",
    "        \n",
    "    ids_with_refs = len([1 for x in delays_data_subset if ('first_issue' in delays_data_subset[x] and delays_data_subset[x]['first_issue'])])\n",
    "    ids_with_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]])\n",
    "    ids_with_positive_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>0])\n",
    "    ids_with_week_old_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>7])\n",
    "\n",
    "    diffs = [delays_data_subset[x][f'{date_key}_diff'] for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]]\n",
    "\n",
    "    print(label)\n",
    "    print(date_key)\n",
    "    print('count', ids_with_diffs)\n",
    "    print('median', median(diffs))\n",
    "    print('recall at 0', ids_with_positive_diffs/ids_with_refs)\n",
    "    print('recall at 7', ids_with_week_old_diffs/ids_with_refs)\n",
    "    print('recall at 30', ids_with_week_old_diffs/ids_with_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delays_vs_with_refs(delays_data_subset, date_key, label):\n",
    "    for x in delays_data_subset:\n",
    "        if 'creation_date' in delays_data_subset[x] and delays_data_subset[x]['creation_date'] and date_key in delays_data_subset[x] and delays_data_subset[x][date_key]:\n",
    "            creation_date = parser.parse(delays_data_subset[x]['creation_date'])\n",
    "            ref_date = min([parser.parse(y['ref_date'])for y in delays_data_subset[x][date_key]])\n",
    "            # ref_date = min([parser.parse(y[1])for y in delays_data_subset[x][date_key]])\n",
    "            diff = creation_date-ref_date\n",
    "            delays = diff.total_seconds()/seconds_in_a_day\n",
    "            delays_data_subset[x][f'{date_key}_diff'] = delays\n",
    "        \n",
    "    ids_with_refs = len([1 for x in delays_data_subset if ('first_ref' in delays_data_subset[x] and delays_data_subset[x]['first_ref']) or ('first_commit' in delays_data_subset[x] and delays_data_subset[x]['first_commit']) or ('first_ref' in delays_data_subset[x] and delays_data_subset[x]['first_ref'])])\n",
    "    ids_with_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]])\n",
    "    ids_with_positive_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>0])\n",
    "    ids_with_week_old_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>7])\n",
    "\n",
    "    diffs = [delays_data_subset[x][f'{date_key}_diff'] for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]]\n",
    "\n",
    "    print(label)\n",
    "    print(date_key)\n",
    "    print('count', ids_with_diffs)\n",
    "    print('median', median(diffs))\n",
    "    print('recall at 0', ids_with_positive_diffs/ids_with_refs)\n",
    "    print('recall at 7', ids_with_week_old_diffs/ids_with_refs)\n",
    "    print('recall at 30', ids_with_week_old_diffs/ids_with_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delays_vs_with_commits(delays_data_subset, date_key, label):\n",
    "    for x in delays_data_subset:\n",
    "        if 'creation_date' in delays_data_subset[x] and delays_data_subset[x]['creation_date'] and date_key in delays_data_subset[x] and delays_data_subset[x][date_key]:\n",
    "            creation_date = parser.parse(delays_data_subset[x]['creation_date'])\n",
    "            ref_date = min([parser.parse(y['ref_date'])for y in delays_data_subset[x][date_key]])\n",
    "            # ref_date = min([parser.parse(y[1])for y in delays_data_subset[x][date_key]])\n",
    "            diff = creation_date-ref_date\n",
    "            delays = diff.total_seconds()/seconds_in_a_day\n",
    "            delays_data_subset[x][f'{date_key}_diff'] = delays\n",
    "        \n",
    "    ids_with_refs = len([1 for x in delays_data_subset if ('first_commit' in delays_data_subset[x] and delays_data_subset[x]['first_commit'])])\n",
    "    ids_with_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]])\n",
    "    ids_with_positive_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>0])\n",
    "    ids_with_week_old_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>7])\n",
    "\n",
    "    diffs = [delays_data_subset[x][f'{date_key}_diff'] for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]]\n",
    "\n",
    "    print(label)\n",
    "    print(date_key)\n",
    "    print('count', ids_with_diffs)\n",
    "    print('median', median(diffs))\n",
    "    print('recall at 0', ids_with_positive_diffs/ids_with_refs)\n",
    "    print('recall at 7', ids_with_week_old_diffs/ids_with_refs)\n",
    "    print('recall at 30', ids_with_week_old_diffs/ids_with_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delays_vs_with_commits_and_all_refs(delays_data_subset, date_key, label):\n",
    "    for x in delays_data_subset:\n",
    "        if 'creation_date' in delays_data_subset[x] and delays_data_subset[x]['creation_date'] and date_key in delays_data_subset[x] and delays_data_subset[x][date_key]:\n",
    "            creation_date = parser.parse(delays_data_subset[x]['creation_date'])\n",
    "            ref_date = min([parser.parse(y['ref_date'])for y in delays_data_subset[x][date_key]])\n",
    "            # ref_date = min([parser.parse(y[1])for y in delays_data_subset[x][date_key]])\n",
    "            diff = creation_date-ref_date\n",
    "            delays = diff.total_seconds()/seconds_in_a_day\n",
    "            delays_data_subset[x][f'{date_key}_diff'] = delays\n",
    "        \n",
    "    ids_with_refs = len([1 for x in delays_data_subset if ('first_commit' in delays_data_subset[x] and delays_data_subset[x]['first_commit']) and ('all_refs' in delays_data_subset[x] and delays_data_subset[x]['all_refs'])])\n",
    "    ids_with_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]])\n",
    "    ids_with_positive_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>0])\n",
    "    ids_with_week_old_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>7])\n",
    "\n",
    "    diffs = [delays_data_subset[x][f'{date_key}_diff'] for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]]\n",
    "\n",
    "    print(label)\n",
    "    print(date_key)\n",
    "    print('count', ids_with_diffs)\n",
    "    print('median', median(diffs))\n",
    "    print('recall at 0', ids_with_positive_diffs/ids_with_refs)\n",
    "    print('recall at 7', ids_with_week_old_diffs/ids_with_refs)\n",
    "    print('recall at 30', ids_with_week_old_diffs/ids_with_refs)\n",
    "\n",
    "def calculate_delays_vs_all_refs_field(delays_data_subset, date_key, label):\n",
    "    for x in delays_data_subset:\n",
    "        if 'creation_date' in delays_data_subset[x] and delays_data_subset[x]['creation_date'] and date_key in delays_data_subset[x] and delays_data_subset[x][date_key]:\n",
    "            creation_date = parser.parse(delays_data_subset[x]['creation_date'])\n",
    "            ref_date = min([parser.parse(y['ref_date'])for y in delays_data_subset[x][date_key]])\n",
    "            # ref_date = min([parser.parse(y[1])for y in delays_data_subset[x][date_key]])\n",
    "            diff = creation_date-ref_date\n",
    "            delays = diff.total_seconds()/seconds_in_a_day\n",
    "            delays_data_subset[x][f'{date_key}_diff'] = delays\n",
    "        \n",
    "    ids_with_refs = len([1 for x in delays_data_subset if ('all_refs' in delays_data_subset[x] and delays_data_subset[x]['all_refs'])])\n",
    "    ids_with_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]])\n",
    "    ids_with_positive_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>0])\n",
    "    ids_with_week_old_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>7])\n",
    "\n",
    "    diffs = [delays_data_subset[x][f'{date_key}_diff'] for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]]\n",
    "\n",
    "    print(label)\n",
    "    print(date_key)\n",
    "    print('count', ids_with_diffs)\n",
    "    print('median', median(diffs))\n",
    "    print('recall at 0', ids_with_positive_diffs/ids_with_refs)\n",
    "    print('recall at 7', ids_with_week_old_diffs/ids_with_refs)\n",
    "    print('recall at 30', ids_with_week_old_diffs/ids_with_refs)\n",
    "\n",
    "def calculate_delays_vs_with_issues_and_all_refs(delays_data_subset, date_key, label):\n",
    "    for x in delays_data_subset:\n",
    "        if 'creation_date' in delays_data_subset[x] and delays_data_subset[x]['creation_date'] and date_key in delays_data_subset[x] and delays_data_subset[x][date_key]:\n",
    "            creation_date = parser.parse(delays_data_subset[x]['creation_date'])\n",
    "            ref_date = min([parser.parse(y['ref_date'])for y in delays_data_subset[x][date_key]])\n",
    "            # ref_date = min([parser.parse(y[1])for y in delays_data_subset[x][date_key]])\n",
    "            diff = creation_date-ref_date\n",
    "            delays = diff.total_seconds()/seconds_in_a_day\n",
    "            delays_data_subset[x][f'{date_key}_diff'] = delays\n",
    "        \n",
    "    ids_with_refs = len([1 for x in delays_data_subset if ('first_issue' in delays_data_subset[x] and delays_data_subset[x]['first_issue']) and ('all_refs' in delays_data_subset[x] and delays_data_subset[x]['all_refs'])])\n",
    "    ids_with_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]])\n",
    "    ids_with_positive_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>0])\n",
    "    ids_with_week_old_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>7])\n",
    "\n",
    "    diffs = [delays_data_subset[x][f'{date_key}_diff'] for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]]\n",
    "\n",
    "    print(label)\n",
    "    print(date_key)\n",
    "    print('count', ids_with_diffs)\n",
    "    print('median', median(diffs))\n",
    "    print('recall at 0', ids_with_positive_diffs/ids_with_refs)\n",
    "    print('recall at 7', ids_with_week_old_diffs/ids_with_refs)\n",
    "    print('recall at 30', ids_with_week_old_diffs/ids_with_refs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delays_vs_classified_commits(delays_data_subset, date_key, label):\n",
    "    for x in delays_data_subset:\n",
    "        if date_key not in delays_data_subset[x] or delays_data_subset[x][date_key] == None or not delays_data_subset[x][date_key]:\n",
    "            continue\n",
    "            \n",
    "        if 'creation_date' not in delays_data_subset[x] or delays_data_subset[x]['creation_date'] == None:\n",
    "            continue\n",
    "\n",
    "\n",
    "        creation_date = parser.parse(delays_data_subset[x]['creation_date'])\n",
    "        dates = [matched_ref['ref_date'] for matched_ref in delays_data_subset[x][date_key] if matched_ref['ref_date'] != 'not_spotted']\n",
    "\n",
    "        if len(dates) > 0:\n",
    "            diff = creation_date-min([parser.parse(d) for d in dates])\n",
    "            delays = diff.total_seconds()/seconds_in_a_day\n",
    "            delays_data_subset[x][f'{date_key}_diff'] = delays\n",
    "        \n",
    "    ids_all = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] or \\\n",
    "        (date_key in delays_data_subset[x] and delays_data_subset[x][date_key] != None and delays_data_subset[x][date_key])])\n",
    "    ids_with_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]])\n",
    "    ids_with_positive_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>0])\n",
    "    ids_with_week_old_diffs = len([1 for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x] and delays_data_subset[x][f'{date_key}_diff']>7])\n",
    "\n",
    "    diffs = [delays_data_subset[x][f'{date_key}_diff'] for x in delays_data_subset if f'{date_key}_diff' in delays_data_subset[x]]\n",
    "\n",
    "    print()\n",
    "    print(date_key)\n",
    "    print(label)\n",
    "    print('count', ids_with_diffs)\n",
    "    print('median', median(diffs))\n",
    "    print('recall at 0',ids_with_positive_diffs,  ids_with_positive_diffs/ids_all)\n",
    "    print('recall at 7',ids_with_week_old_diffs, ids_with_week_old_diffs/ids_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall for 'Data' chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_ref(delays_data_subset, label):\n",
    "    calculate_delay_no_separation(delays_data_subset, 'first_ref', label)\n",
    "\n",
    "first_ref(recall_data, 'All')\n",
    "first_ref(npm_subset, 'npm')\n",
    "first_ref(pypi_subset, 'pypi')\n",
    "first_ref(mvn_subset, 'mvn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_issue(delays_data_subset, label):\n",
    "    calculate_delay_no_separation(delays_data_subset, 'first_issue', label)\n",
    "\n",
    "first_issue(recall_data, 'All')\n",
    "first_issue(npm_subset, 'npm')\n",
    "first_issue(pypi_subset, 'pypi')\n",
    "first_issue(mvn_subset, 'mvn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_commit(delays_data_subset, label):\n",
    "    calculate_delay_no_separation(delays_data_subset, 'first_commit', label)\n",
    "\n",
    "first_commit(recall_data, 'All')\n",
    "first_commit(npm_subset, 'npm')\n",
    "first_commit(pypi_subset, 'pypi')\n",
    "first_commit(mvn_subset, 'mvn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall for Experiments 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_sec_phrase(delays_data_subset, label):\n",
    "    calculate_delays_vs_with_refs(delays_data_subset, 'first_sec_phrase', label)\n",
    "\n",
    "first_sec_phrase(recall_data, 'All')\n",
    "first_sec_phrase(npm_subset, 'npm')\n",
    "first_sec_phrase(pypi_subset, 'pypi')\n",
    "first_sec_phrase(mvn_subset, 'mvn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_sec_label(delays_data_subset, label):\n",
    "    calculate_delays_vs_with_issues(delays_data_subset, 'first_sec_label', label)\n",
    "\n",
    "first_sec_label(recall_data, 'All')\n",
    "first_sec_label(npm_subset, 'npm')\n",
    "first_sec_label(pypi_subset, 'pypi')\n",
    "first_sec_label(mvn_subset, 'mvn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fb_commit_classification(delays_data_subsetx, label):\n",
    "    calculate_delays_vs_classified_commits(delays_data_subsetx, 'fb_commit_classification', label)\n",
    "\n",
    "fb_commit_classification(all_subset, 'All')\n",
    "fb_commit_classification(npm_subset, 'npm')\n",
    "fb_commit_classification(pypi_subset, 'pypi')\n",
    "fb_commit_classification(mvn_subset,  'mvn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vfm_combined_classification(delays_data_subset, label):\n",
    "    calculate_delays_vs_classified_commits(delays_data_subset, 'vfm_combined_classification', label)\n",
    "\n",
    "vfm_combined_classification(all_subset, 'All')\n",
    "vfm_combined_classification(npm_subset, 'npm')\n",
    "vfm_combined_classification(pypi_subset, 'pypi')\n",
    "vfm_combined_classification(mvn_subset,  'mvn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: Disclosure Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_file, 'r') as f:\n",
    "    recall_data = json.load(f)\n",
    "\n",
    "with_creation_dates = {k:v for k,v in recall_data.items() if 'creation_date' in v and v['creation_date']}\n",
    "for x in with_creation_dates:\n",
    "    with_creation_dates[x]['year'] = parser.parse(with_creation_dates[x]['creation_date']).year\n",
    "\n",
    "all_2017 = {k:v for k,v in recall_data.items() if 'year' in v and v['year']==2017}\n",
    "all_2018 = {k:v for k,v in recall_data.items() if 'year' in v and v['year']==2018}\n",
    "all_2019 = {k:v for k,v in recall_data.items() if 'year' in v and v['year']==2019}\n",
    "all_2020 = {k:v for k,v in recall_data.items() if 'year' in v and v['year']==2020}\n",
    "all_2021 = {k:v for k,v in recall_data.items() if 'year' in v and v['year']==2021}\n",
    "all_2022 = {k:v for k,v in recall_data.items() if 'year' in v and v['year']==2022}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_year = all_2021\n",
    "selected_year_s = '2021'\n",
    "\n",
    "first_sec_phrase(selected_year, selected_year_s)\n",
    "first_sec_label(selected_year, selected_year_s)\n",
    "fb_commit_classification(selected_year, selected_year_s)\n",
    "vfm_combined_classification(selected_year, selected_year_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_file, 'r') as f:\n",
    "    recall_data = json.load(f)\n",
    "\n",
    "all_low =      {k:v for k,v in recall_data.items() if 'severity' in v and v['severity']=='LOW'}\n",
    "all_medium =   {k:v for k,v in recall_data.items() if 'severity' in v and v['severity']=='MEDIUM'}\n",
    "all_high =     {k:v for k,v in recall_data.items() if 'severity' in v and v['severity']=='HIGH'}\n",
    "all_critical = {k:v for k,v in recall_data.items() if 'severity' in v and v['severity']=='CRITICAL'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_crticality = all_high\n",
    "selected_crticality_s = 'all_high'\n",
    "first_sec_phrase(selected_crticality, selected_crticality_s)\n",
    "first_sec_label(selected_crticality, selected_crticality_s)\n",
    "fb_commit_classification(selected_crticality, selected_crticality_s)\n",
    "vfm_combined_classification(selected_crticality, selected_crticality_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: Commit size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_threshold = 10\n",
    "top_threshold = 40\n",
    "\n",
    "with open(path_to_file, 'r') as f:\n",
    "    recall_data = json.load(f)\n",
    "\n",
    "def split_by_commit_size(data_subset, data_keys):\n",
    "    result_bot = copy.deepcopy(data_subset)\n",
    "    result_mid = copy.deepcopy(data_subset)\n",
    "    result_top = copy.deepcopy(data_subset)\n",
    "    for k,v in data_subset.items():\n",
    "        for data_key in data_keys:\n",
    "            if data_key not in v or v[data_key] == None:\n",
    "                continue\n",
    "\n",
    "            commit_size_bot = [matched_ref for matched_ref in v[data_key] if 'ref_commit_size' in matched_ref and matched_ref['ref_commit_size'] != None and matched_ref['ref_commit_size'] <= bottom_threshold]\n",
    "            commit_size_mid = [matched_ref for matched_ref in v[data_key] if 'ref_commit_size' in matched_ref and matched_ref['ref_commit_size'] != None and matched_ref['ref_commit_size'] > bottom_threshold and matched_ref['ref_commit_size'] <= top_threshold]\n",
    "            commit_size_top = [matched_ref for matched_ref in v[data_key] if 'ref_commit_size' in matched_ref and matched_ref['ref_commit_size'] != None and matched_ref['ref_commit_size'] > top_threshold]\n",
    "\n",
    "            result_bot[k][data_key] = list(commit_size_bot)\n",
    "            result_mid[k][data_key] = list(commit_size_mid)\n",
    "            result_top[k][data_key] = list(commit_size_top)\n",
    "        \n",
    "    return result_bot,result_mid,result_top\n",
    "\n",
    "data_keys=[\n",
    "    'all_refs',\n",
    "    'first_sec_phrase',\n",
    "    'fb_commit_classification',\n",
    "    'vfm_combined_classification'\n",
    "]\n",
    "\n",
    "all_bot_commit_size, all_mid_commit_size, all_top_commit_size = split_by_commit_size(recall_data, data_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_commit_size = all_top_commit_size\n",
    "selected_commit_size_s = 'all_top_commit_size'\n",
    "calculate_delays_vs_with_commits_and_all_refs(selected_commit_size, 'first_sec_phrase', selected_commit_size_s)\n",
    "# first_sec_label(selected_commit_size, selected_commit_size_s)\n",
    "fb_commit_classification(selected_commit_size, selected_commit_size_s)\n",
    "vfm_combined_classification(selected_commit_size, selected_commit_size_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: Repository stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_threshold = 400\n",
    "top_threshold = 3200\n",
    "\n",
    "with open(path_to_file, 'r') as f:\n",
    "    recall_data = json.load(f)\n",
    "\n",
    "with open(path_to_repository_info, 'r') as f:\n",
    "    repository_info = json.load(f)\n",
    "\n",
    "def split_by_repository_stars(data_subset, data_keys):\n",
    "    result_bot = copy.deepcopy(data_subset)\n",
    "    result_mid = copy.deepcopy(data_subset)\n",
    "    result_top = copy.deepcopy(data_subset)\n",
    "    for k,v in data_subset.items():\n",
    "        for data_key in data_keys:\n",
    "            if data_key not in v or v[data_key] == None:\n",
    "                continue\n",
    "\n",
    "            commit_size_bot = []\n",
    "            commit_size_mid = []\n",
    "            commit_size_top = []\n",
    "            for matched_ref in v[data_key]:\n",
    "                if 'ref_repo_full_name' not in matched_ref:\n",
    "                    continue\n",
    "                if matched_ref['ref_repo_full_name'] not in repository_info:\n",
    "                    continue\n",
    "\n",
    "                stars_count = repository_info[matched_ref['ref_repo_full_name']]['repository_info']['stargazers_count']\n",
    "                if stars_count <=bottom_threshold:\n",
    "                    commit_size_bot.append(matched_ref)\n",
    "                elif stars_count > bottom_threshold and stars_count <= top_threshold:\n",
    "                    commit_size_mid.append(matched_ref)\n",
    "                elif stars_count > top_threshold:\n",
    "                    commit_size_top.append(matched_ref)\n",
    "\n",
    "            result_bot[k][data_key] = commit_size_bot\n",
    "            result_mid[k][data_key] = commit_size_mid\n",
    "            result_top[k][data_key] = commit_size_top\n",
    "        \n",
    "    return result_bot,result_mid,result_top\n",
    "\n",
    "data_keys=[\n",
    "    'all_refs',\n",
    "    'first_sec_phrase',\n",
    "    'first_sec_label',\n",
    "    'fb_commit_classification',\n",
    "    'vfm_combined_classification'\n",
    "]\n",
    "\n",
    "all_bot_repo_stars, all_mid_repo_stars, all_top_repo_stars = split_by_repository_stars(recall_data, data_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_repo_class = all_bot_repo_stars\n",
    "selected_repo_class_s = 'all_bot_repo_stars'\n",
    "calculate_delays_vs_all_refs_field(selected_repo_class, 'first_sec_phrase', selected_repo_class_s)\n",
    "calculate_delays_vs_with_issues_and_all_refs(selected_repo_class, 'first_sec_label', selected_repo_class_s)\n",
    "fb_commit_classification(selected_repo_class, selected_repo_class_s)\n",
    "vfm_combined_classification(selected_repo_class, selected_repo_class_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ok')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
