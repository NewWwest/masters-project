{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r'D:\\Projects\\aaa')\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "\n",
    "from src.dl.datasets.CommitLevelRawDataset import CommitLevelRawDataset\n",
    "from src.dl.datasets.supporting.CsvDataset import CsvDataset\n",
    "from src.dl.datasets.sampling.OverSampledDataset import OverSampledDataset\n",
    "from src.dl.datasets.sampling.UnderSampledDataset import UnderSampledDataset\n",
    "\n",
    "from src.dl.dl_utils import save_dataset, read_dataset\n",
    "from src.dl.models.LstmAggregator import LstmAggregator as WorkingModel\n",
    "from src.dl.datasets.load import load_commit_level\n",
    "\n",
    "# Model config\n",
    "batch_size_ = 1\n",
    "num_epochs_ = 10\n",
    "fraction_of_data = 1\n",
    "train_percentage_size = 0.8\n",
    "class_ratio = 1\n",
    "learning_rate = 0.0005\n",
    "\n",
    "save_model_in_each_epoch = True\n",
    "eval_model_in_each_epoch = True\n",
    "model_name = 'new_agg_test'\n",
    "work_dir = f'src/rq5/binaries/{model_name}'\n",
    "\n",
    "# Data config - Set to None if you want to use cached datasets\n",
    "raw_input_path = r'results/dl/java/CodeParserMiner_ast_embedded1'\n",
    "# raw_input_path = None\n",
    "\n",
    "\n",
    "# jsut for 'just_eval':\n",
    "eval_input_path = None\n",
    "model_to_eval = 'final'\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def train_model(model, optimizer, data_loader, loss_module, scheduler, test_loader = None):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    all_samples = 0\n",
    "    positive_samples = 0\n",
    "\n",
    "    for epoch in range(num_epochs_):\n",
    "        print(f'Epoch {epoch}/{num_epochs_}')\n",
    "        accumulated_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        with tqdm.tqdm(total=len(data_loader)) as pbar:\n",
    "            for data_inputs, data_labels in data_loader:\n",
    "                # Step 0: Diagnostics :x\n",
    "                positive_samples += len([1 for x in data_labels if x[0] == 1])\n",
    "                all_samples += len(data_labels)\n",
    "\n",
    "                #TODO different commit mode and sample mode\n",
    "                data_inputs = torch.stack(data_inputs)\n",
    "                \n",
    "                # Step 1: Mode data to device \n",
    "                data_inputs = data_inputs.to(device)\n",
    "                data_labels = data_labels.to(device) \n",
    "\n",
    "                # Step 2: Calculate model output\n",
    "                preds = model(data_inputs)\n",
    "                \n",
    "                #TODO different commit mode and sample mode\n",
    "                # preds = preds.squeeze(dim=0)\n",
    "\n",
    "                # Step 3: Calculate loss\n",
    "                loss = loss_module(preds, data_labels.float())\n",
    "                accumulated_loss += loss.item()\n",
    "\n",
    "                ## Step 4: Perform backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                ## Step 5: Update the parameters\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Step 6: Progress bar\n",
    "                pbar.update()\n",
    "        print('Loss in this epoch:', accumulated_loss)\n",
    "\n",
    "        if save_model_in_each_epoch:\n",
    "            torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_epoch_{epoch}.pickle')\n",
    "\n",
    "        if test_loader != None:\n",
    "            eval_model(model, test_loader)\n",
    "\n",
    "\n",
    "    print(f'Model saw positive samples {positive_samples} times and background samples {all_samples-positive_samples}')\n",
    "    print(f'Ratio 1:{(all_samples-positive_samples)/positive_samples}')\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    data_size = len(data_loader)\n",
    "    with tqdm.tqdm(total=data_size) as pbar:\n",
    "        for data_inputs, data_labels in data_loader:\n",
    "\n",
    "            #TODO different commit mode and sample mode\n",
    "            data_inputs = torch.stack(data_inputs)\n",
    "\n",
    "            data_inputs = data_inputs.to(device)\n",
    "            data_labels = data_labels.to(device)\n",
    "            preds = model(data_inputs)\n",
    "            #TODO different commit mode and sample mode\n",
    "            # preds = preds.squeeze(dim=0)\n",
    "\n",
    "            labels_in_memory = data_labels.cpu().detach().numpy()\n",
    "            if len(labels_in_memory.shape) == 1:\n",
    "                all_labels.append(labels_in_memory)\n",
    "            else:\n",
    "                for x in labels_in_memory:\n",
    "                    all_labels.append(x)\n",
    "                    \n",
    "            preds_in_memory = preds.cpu().detach().numpy()\n",
    "            if labels_in_memory.shape[0] == 1:\n",
    "                all_predictions.append(preds_in_memory)\n",
    "            else:\n",
    "                for x in preds_in_memory:\n",
    "                    all_predictions.append(x)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    #TODO different commit mode and sample mode\n",
    "    predictions_arr = [1 if x[0,0]>x[0,1] else 0 for x in all_predictions]\n",
    "    targets_arr = [1 if x[0]>x[1] else 0 for x in all_labels]\n",
    "    P = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1])\n",
    "    TP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==1])\n",
    "    FP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==0])\n",
    "    FN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==1])\n",
    "    TN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==0])\n",
    "    N = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0])\n",
    "\n",
    "    precission = TP/(TP+FP) if (TP+FP)!=0 else 0\n",
    "    recall = TP/(TP+FN) if (TP+FN)!=0 else 0\n",
    "    print('Precission:',f'{TP}/{TP+FP}', precission)\n",
    "    print('Recall', f'{TP}/{TP+FN}', recall)\n",
    "    print(f'P:{P},', f'TP:{TP},', f'FP:{FP},', f'FN:{FN},', f'TN:{TN},', f'N:{N}')\n",
    "\n",
    "    return precission, recall\n",
    "\n",
    "\n",
    "def load_data(input_path):\n",
    "    # Load Data\n",
    "    dataset, test_dataset = load_commit_level(input_path)\n",
    "\n",
    "    # Data limit for testing\n",
    "    if fraction_of_data < 1:\n",
    "        dataset, rejected_data = dataset.split_data(fraction_of_data)\n",
    "\n",
    "    # Train/Test split & rebalancing\n",
    "    train_dataset, eval_dataset = dataset.split_data(train_percentage_size)\n",
    "\n",
    "    train_dataset = UnderSampledDataset(train_dataset, class_ratio)\n",
    "\n",
    "    # # Save the data\n",
    "    # save_dataset(train_dataset, f'{work_dir}/train_commit_dataset_{model_name}.csv')\n",
    "    # save_dataset(eval_dataset, f'{work_dir}/eval_commit_dataset_{model_name}.csv')\n",
    "    # save_dataset(test_dataset, f'{work_dir}/test_commit_dataset_{model_name}.csv')\n",
    "\n",
    "    return train_dataset, test_dataset, eval_dataset\n",
    "\n",
    "\n",
    "def just_eval():\n",
    "    model = WorkingModel()\n",
    "    model.load_state_dict(torch.load(f'{work_dir}/model_{model_name}_{model_to_eval}.pickle'))\n",
    "    eval_dataset = CommitLevelRawDataset()\n",
    "    eval_dataset.load(eval_input_path)\n",
    "    eval_data_loader = data.DataLoader(eval_dataset, drop_last=True, batch_size=batch_size_)\n",
    "    eval_model(model, eval_data_loader)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "if raw_input_path != None:\n",
    "    train_dataset, test_dataset, eval_dataset = load_data(raw_input_path)\n",
    "else:\n",
    "    train_dataset = read_dataset(f'{work_dir}/train_commit_dataset_{model_name}.csv')\n",
    "    eval_dataset = read_dataset(f'{work_dir}/eval_commit_dataset_{model_name}.csv')\n",
    "    test_dataset = read_dataset(f'{work_dir}/test_commit_dataset_{model_name}.csv')\n",
    "\n",
    "# Define model\n",
    "model = WorkingModel()\n",
    "loss_module = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "    num_warmup_steps=int(len(train_dataset)*0.25), \n",
    "    num_training_steps=len(train_dataset)*num_epochs_)\n",
    "\n",
    "# Prep the loaders\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=batch_size_, drop_last=True, shuffle=True)\n",
    "if eval_model_in_each_epoch:\n",
    "    eval_data_loader = data.DataLoader(eval_dataset, batch_size=batch_size_, drop_last=True, shuffle=True)\n",
    "else:\n",
    "    eval_data_loader = None\n",
    "\n",
    "# Train the model\n",
    "train_model(model, optimizer, train_data_loader, loss_module, scheduler, test_loader=eval_data_loader)\n",
    "torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_final.pickle')\n",
    "\n",
    "# Test the model on test subset\n",
    "eval_data_loader = data.DataLoader(eval_dataset, drop_last=True, batch_size=batch_size_)\n",
    "eval_model(model, eval_data_loader)\n",
    "\n",
    "# Test the model on eval subset\n",
    "test_data_loader = data.DataLoader(test_dataset, drop_last=True, batch_size=batch_size_)\n",
    "eval_model(model, test_data_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
