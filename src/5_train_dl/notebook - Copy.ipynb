{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r'D:\\Projects\\aaa')\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "from src.dl.datasets.CommitLevelRawDataset import CommitLevelRawDataset\n",
    "from src.dl.datasets.SampleLevelRawDataset import SampleLevelRawDataset\n",
    "from src.dl.datasets.supporting.CsvDataset import CsvDataset\n",
    "from src.dl.datasets.sampling.OverSampledDataset import OverSampledDataset\n",
    "from src.dl.datasets.sampling.UnderSampledDataset import UnderSampledDataset\n",
    "\n",
    "from src.dl.dl_utils import save_dataset, read_dataset, get_repo_seminames, get_files_in_set\n",
    "from src.utils.utils import get_files_in_from_directory\n",
    "from src.dl.models.BertAndLinear import BertAndLinear as FineTuningModel\n",
    "from src.dl.models.LstmAggregator import LstmAggregator as AggregatorModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "base_model = 'microsoft/graphcodebert-base'\n",
    "batch_size_ = 4\n",
    "num_epochs_ = 3\n",
    "\n",
    "fraction_of_data = 1\n",
    "\n",
    "sample_limit = 5_000\n",
    "eval_sample_limit = 2_000\n",
    "test_percentage = 0.15\n",
    "eval_percentage = 0.05\n",
    "\n",
    "learning_rate = 1e-6\n",
    "oversampling_ratio = 2\n",
    "class_ratio = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator_num_epochs_ = 15\n",
    "aggregator_class_ratio = 2\n",
    "aggregator_learning_rate = 5e-6\n",
    "\n",
    "save_model_in_each_epoch = True\n",
    "eval_model_in_each_epoch = True\n",
    "\n",
    "model_guid = str(uuid.uuid4())\n",
    "model_name = model_guid\n",
    "\n",
    "work_dir = f'D:\\\\Projects\\\\aaa\\src\\\\rq5\\\\binaries\\\\{model_name}'\n",
    "results_dir = f'D:\\\\Projects\\\\aaa\\src\\\\rq5\\\\binaries\\\\data{model_name}'\n",
    "\n",
    "# Data config - Set to None if you want to use cached datasets\n",
    "raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\CodeParserMiner_ast'\n",
    "# raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\CodeParserMiner_edit'\n",
    "# raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\AddedCodeMiner'\n",
    "# raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\RollingWindowMiner'\n",
    "# raw_input_path = None\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(work_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir(results_dir) \n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1fde43fc650>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, data_loader, loss_module, scheduler, eval_loader = None):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    all_samples = 0\n",
    "    positive_samples = 0\n",
    "\n",
    "    for epoch in range(num_epochs_):\n",
    "        print(f'Epoch {epoch}/{num_epochs_}')\n",
    "        accumulated_loss = 0\n",
    "\n",
    "        with tqdm.tqdm(total=len(data_loader)) as pbar:\n",
    "            for data_inputs, data_labels in data_loader:\n",
    "                # Step 0: Diagnostics :x\n",
    "                positive_samples += len([1 for x in data_labels if x[0] == 1])\n",
    "                all_samples += len(data_labels)\n",
    "                \n",
    "                # Step 1: Mode data to device\n",
    "                data_inputs = data_inputs.to(device)\n",
    "                data_labels = data_labels.to(device)\n",
    "\n",
    "                # Step 2: Calculate model output\n",
    "                preds = model(data_inputs)\n",
    "                preds = preds.squeeze(dim=0)\n",
    "\n",
    "                # Step 3: Calculate loss\n",
    "                loss = loss_module(preds, data_labels.float())\n",
    "                accumulated_loss += loss.item()\n",
    "\n",
    "                ## Step 4: Perform backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                ## Step 5: Update the parameters\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Step 6: Progress bar\n",
    "                pbar.update()\n",
    "        print('Loss in this epoch:', accumulated_loss)\n",
    "\n",
    "        if save_model_in_each_epoch:\n",
    "            torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_epoch_{epoch}.pickle')\n",
    "\n",
    "        if eval_loader != None:\n",
    "            eval_model(model, eval_loader)\n",
    "\n",
    "\n",
    "    print(f'Model saw positive samples {positive_samples} times and background samples {all_samples-positive_samples}')\n",
    "    print(f'Ratio 1:{(all_samples-positive_samples)/positive_samples}')\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    data_size = len(data_loader)\n",
    "    with tqdm.tqdm(total=data_size) as pbar:\n",
    "        for data_inputs, data_labels in data_loader:\n",
    "            data_inputs = data_inputs.to(device)\n",
    "            data_labels = data_labels.to(device)\n",
    "            preds = model(data_inputs)\n",
    "            preds = preds.squeeze(dim=0)\n",
    "\n",
    "            labels_in_memory = data_labels.cpu().detach().numpy()\n",
    "            if len(labels_in_memory.shape) == 1:\n",
    "                all_labels.append(labels_in_memory)\n",
    "            else:\n",
    "                for x in labels_in_memory:\n",
    "                    all_labels.append(x)\n",
    "                    \n",
    "            preds_in_memory = preds.cpu().detach().numpy()\n",
    "            if labels_in_memory.shape[0] == 1:\n",
    "                all_predictions.append(preds_in_memory)\n",
    "            else:\n",
    "                for x in preds_in_memory:\n",
    "                    all_predictions.append(x)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    predictions_arr = [1 if x[0]>x[1] else 0 for x in all_predictions] #TODO softmax\n",
    "    targets_arr = [1 if x[0]>x[1] else 0 for x in all_labels]\n",
    "    P = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1])\n",
    "    TP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==1])\n",
    "    FP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==0])\n",
    "    FN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==1])\n",
    "    TN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==0])\n",
    "    N = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0])\n",
    "\n",
    "    precission = TP/(TP+FP) if (TP+FP)!=0 else 0\n",
    "    recall = TP/(TP+FN) if (TP+FN)!=0 else 0\n",
    "    print('Precission:',f'{TP}/{TP+FP}', precission)\n",
    "    print('Recall', f'{TP}/{TP+FN}', recall)\n",
    "    print(f'P:{P},', f'TP:{TP},', f'FP:{FP},', f'FN:{FN},', f'TN:{TN},', f'N:{N}')\n",
    "\n",
    "    return precission, recall\n",
    "\n",
    "\n",
    "def load_files(input_path, data_fraction=1):\n",
    "    positive_json_files = get_files_in_from_directory(input_path, extension='.json', startswith='positive-encodings')\n",
    "    background_json_files = get_files_in_from_directory(input_path, extension='.json', startswith='background-encodings')\n",
    "\n",
    "    if data_fraction < 1:\n",
    "        positive_json_files = random.sample(positive_json_files, int(len(positive_json_files)*data_fraction))\n",
    "        background_json_files = random.sample(background_json_files, int(len(background_json_files)*data_fraction))\n",
    "\n",
    "\n",
    "    repos_set = get_repo_seminames(positive_json_files)\n",
    "    repos_count = len(repos_set)\n",
    "\n",
    "\n",
    "    repos_test = set(random.sample(list(repos_set), int(repos_count*test_percentage)))\n",
    "    repos_set.difference_update(repos_test)\n",
    "    repos_eval = set(random.sample(list(repos_set), int(repos_count*test_percentage)))\n",
    "    repos_set.difference_update(repos_eval)\n",
    "\n",
    "    positive_train = get_files_in_set(positive_json_files, repos_set)\n",
    "    positive_eval = get_files_in_set(positive_json_files, repos_eval)\n",
    "    positive_test = get_files_in_set(positive_json_files, repos_test)\n",
    "\n",
    "    background_train = get_files_in_set(background_json_files, repos_set)\n",
    "    background_eval = get_files_in_set(background_json_files, repos_eval)\n",
    "    background_test = get_files_in_set(background_json_files, repos_test)\n",
    "\n",
    "    return (positive_json_files, background_json_files), (positive_train, background_train), (positive_eval, background_eval), (positive_test, background_test)\n",
    "\n",
    "\n",
    "def load_data(input_data, oversampling_ratio=None, class_ratio=None, sample_limit=None):\n",
    "    positive_files = input_data[0]\n",
    "    background_files = input_data[1]\n",
    "\n",
    "    dataset = SampleLevelRawDataset()\n",
    "    dataset.load_files(positive_files, background_files)\n",
    "\n",
    "    if oversampling_ratio != None and class_ratio != None and sample_limit != None:\n",
    "        dataset.setup_ratios(oversampling_ratio, class_ratio, sample_limit)   \n",
    "\n",
    "    if oversampling_ratio == None and class_ratio == None and sample_limit != None:\n",
    "        dataset.limit_data(sample_limit)   \n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def embed_files(tokenizer, data_files):\n",
    "    with tqdm.tqdm(total=len(data_files)) as pbar:\n",
    "        for data_file in data_files:\n",
    "            with open(data_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            embeddings = []\n",
    "            for data_point in data:\n",
    "                if 'commit_sample' in data_point and \\\n",
    "                    data_point['commit_sample'] != None and \\\n",
    "                    len(data_point['commit_sample']) > 0:\n",
    "\n",
    "                    tensor = torch.Tensor(data_point['commit_sample']).int()\n",
    "                    tensor = tensor[None, :] # Extend to a batch mode\n",
    "                    tensor = tensor.to(device)\n",
    "                    result = tokenizer(tensor)\n",
    "                    labels = result[0][:,0,:]\n",
    "                    labels_in_memory = labels.cpu()\n",
    "                    res = {\n",
    "                        'commit_id': data_point['commit_id'],\n",
    "                        'file_name': data_point['file_name'],\n",
    "                        'is_security_related': data_point['is_security_related'],\n",
    "                        'commit_sample': labels_in_memory.tolist()\n",
    "                    }\n",
    "                    embeddings.append(res)\n",
    "\n",
    "            if len(embeddings) > 0:\n",
    "                file_name = os.path.basename(data_file)\n",
    "                new_file = os.path.join(results_dir, 'embedded-' + file_name)\n",
    "                with open(new_file, 'w') as f:\n",
    "                    json.dump(embeddings, f)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "\n",
    "def map_files_to_new_repo(data_files):\n",
    "    new_data_files = []\n",
    "    for data_file in data_files:\n",
    "        file_name = os.path.basename(data_file)\n",
    "        new_file = os.path.join(results_dir, 'embedded-' + file_name)\n",
    "        if os.path.exists(new_file):\n",
    "            new_data_files.append(new_file)\n",
    "\n",
    "    return new_data_files\n",
    "\n",
    "\n",
    "def train_model2(model, optimizer, data_loader, loss_module, scheduler, test_loader = None):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    all_samples = 0\n",
    "    positive_samples = 0\n",
    "\n",
    "    for epoch in range(aggregator_num_epochs_):\n",
    "        print(f'Epoch {epoch}/{aggregator_num_epochs_}')\n",
    "        accumulated_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        with tqdm.tqdm(total=len(data_loader)) as pbar:\n",
    "            for data_inputs, data_labels in data_loader:\n",
    "                # Step 0: Diagnostics :x\n",
    "                positive_samples += len([1 for x in data_labels if x[0] == 1])\n",
    "                all_samples += len(data_labels)\n",
    "\n",
    "                #TODO different commit mode and sample mode\n",
    "                data_inputs = torch.stack(data_inputs)\n",
    "                \n",
    "                # Step 1: Mode data to device \n",
    "                data_inputs = data_inputs.to(device)\n",
    "                data_labels = data_labels.to(device) \n",
    "\n",
    "                # Step 2: Calculate model output\n",
    "                preds = model(data_inputs)\n",
    "                \n",
    "                #TODO different commit mode and sample mode\n",
    "                # preds = preds.squeeze(dim=0)\n",
    "\n",
    "                # Step 3: Calculate loss\n",
    "                loss = loss_module(preds, data_labels.float())\n",
    "                accumulated_loss += loss.item()\n",
    "\n",
    "                ## Step 4: Perform backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                ## Step 5: Update the parameters\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Step 6: Progress bar\n",
    "                pbar.update()\n",
    "        print('Loss in this epoch:', accumulated_loss)\n",
    "\n",
    "        if save_model_in_each_epoch:\n",
    "            torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_epoch_{epoch}.pickle')\n",
    "\n",
    "        if test_loader != None:\n",
    "            eval_model2(model, test_loader)\n",
    "\n",
    "\n",
    "    print(f'Model saw positive samples {positive_samples} times and background samples {all_samples-positive_samples}')\n",
    "    print(f'Ratio 1:{(all_samples-positive_samples)/positive_samples}')\n",
    "\n",
    "\n",
    "def eval_model2(model, data_loader):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    data_size = len(data_loader)\n",
    "    with tqdm.tqdm(total=data_size) as pbar:\n",
    "        for data_inputs, data_labels in data_loader:\n",
    "\n",
    "            #TODO different commit mode and sample mode\n",
    "            data_inputs = torch.stack(data_inputs)\n",
    "\n",
    "            data_inputs = data_inputs.to(device)\n",
    "            data_labels = data_labels.to(device)\n",
    "            preds = model(data_inputs)\n",
    "            #TODO different commit mode and sample mode\n",
    "            # preds = preds.squeeze(dim=0)\n",
    "\n",
    "            labels_in_memory = data_labels.cpu().detach().numpy()\n",
    "            if len(labels_in_memory.shape) == 1:\n",
    "                all_labels.append(labels_in_memory)\n",
    "            else:\n",
    "                for x in labels_in_memory:\n",
    "                    all_labels.append(x)\n",
    "                    \n",
    "            preds_in_memory = preds.cpu().detach().numpy()\n",
    "            if labels_in_memory.shape[0] == 1:\n",
    "                all_predictions.append(preds_in_memory)\n",
    "            else:\n",
    "                for x in preds_in_memory:\n",
    "                    all_predictions.append(x)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    #TODO different commit mode and sample mode\n",
    "    predictions_arr = [1 if x[0,0]>x[0,1] else 0 for x in all_predictions]\n",
    "    targets_arr = [1 if x[0]>x[1] else 0 for x in all_labels]\n",
    "    P = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1])\n",
    "    TP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==1])\n",
    "    FP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==0])\n",
    "    FN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==1])\n",
    "    TN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==0])\n",
    "    N = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0])\n",
    "\n",
    "    precission = TP/(TP+FP) if (TP+FP)!=0 else 0\n",
    "    recall = TP/(TP+FN) if (TP+FN)!=0 else 0\n",
    "    print('Precission:',f'{TP}/{TP+FP}', precission)\n",
    "    print('Recall', f'{TP}/{TP+FN}', recall)\n",
    "    print(f'P:{P},', f'TP:{TP},', f'FP:{FP},', f'FN:{FN},', f'TN:{TN},', f'N:{N}')\n",
    "\n",
    "    return precission, recall\n",
    "\n",
    "\n",
    "def save_file_datasets(file_dataset, dataset_type):\n",
    "    data = {\n",
    "        'positive_files': file_dataset[0],\n",
    "        'background_files': file_dataset[1]\n",
    "    }\n",
    "    with open(os.path.join(results_dir, f'{dataset_type}-files.json'), 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "def load_file_dataset(dataset_type):\n",
    "    with open(os.path.join(results_dir, f'{dataset_type}-files.json'), 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return (data['positive_files'], data['background_files'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, train_data, eval_data, test_data = load_files(raw_input_path, fraction_of_data)\n",
    "save_file_datasets(train_data, 'train_data')\n",
    "save_file_datasets(eval_data, 'eval_data')\n",
    "save_file_datasets(test_data, 'test_data')\n",
    "\n",
    "train_data = load_file_dataset('train_data')\n",
    "eval_data = load_file_dataset('eval_data')\n",
    "test_data = load_file_dataset('test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_and_eval():\n",
    "    train_dataset = load_data(train_data, oversampling_ratio, class_ratio, sample_limit)\n",
    "    eval_dataset = load_data(eval_data, sample_limit=eval_sample_limit)\n",
    "    test_dataset = load_data(test_data)\n",
    "\n",
    "    # Define model\n",
    "    model = FineTuningModel(base_model)\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "        num_warmup_steps=int(len(train_dataset)*0.25), \n",
    "        num_training_steps=len(train_dataset)*num_epochs_)\n",
    "\n",
    "    # Prep the loaders\n",
    "    train_data_loader = data.DataLoader(train_dataset, batch_size=batch_size_, drop_last=True, shuffle=True)\n",
    "    eval_data_loader = data.DataLoader(eval_dataset, batch_size=batch_size_, drop_last=True, shuffle=True)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, optimizer, train_data_loader, loss_module, scheduler, eval_loader=eval_data_loader)\n",
    "    torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_final.pickle')\n",
    "\n",
    "    # Test the model on test subset\n",
    "    test_data_loader = data.DataLoader(test_dataset, drop_last=True, batch_size=batch_size_)\n",
    "    precision, recall = eval_model(model, test_data_loader)\n",
    "    return model, precision, recall\n",
    "\n",
    "# model, precision, recall = finetune_and_eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [07:47<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 834.5740791857243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:57<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 33/885 0.03728813559322034\n",
      "Recall 33/40 0.825\n",
      "P:885, TP:33, FP:852, FN:7, TN:1108, N:1115\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [07:15<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 297.36948271654546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 13/51 0.2549019607843137\n",
      "Recall 13/40 0.325\n",
      "P:51, TP:13, FP:38, FN:27, TN:1922, N:1949\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 56.193640563986264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 16/63 0.25396825396825395\n",
      "Recall 16/40 0.4\n",
      "P:63, TP:16, FP:47, FN:24, TN:1913, N:1937\n",
      "Model saw positive samples 7500 times and background samples 7500\n",
      "Ratio 1:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 385/2157 0.17848864163189615\n",
      "Recall 385/1267 0.30386740331491713\n",
      "P:2157, TP:385, FP:1772, FN:882, TN:32637, N:33519\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio 1\n",
      "class_ratio 1\n",
      "precision 0.17848864163189615\n",
      "recall 0.30386740331491713\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 797.6526148170233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 3/136 0.022058823529411766\n",
      "Recall 3/40 0.075\n",
      "P:136, TP:3, FP:133, FN:37, TN:1827, N:1864\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 281.135711459443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 5/16 0.3125\n",
      "Recall 5/40 0.125\n",
      "P:16, TP:5, FP:11, FN:35, TN:1949, N:1984\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 52.3525689104863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 7/33 0.21212121212121213\n",
      "Recall 7/40 0.175\n",
      "P:33, TP:7, FP:26, FN:33, TN:1934, N:1967\n",
      "Model saw positive samples 4998 times and background samples 10002\n",
      "Ratio 1:2.0012004801920766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 464/1750 0.2651428571428571\n",
      "Recall 464/1267 0.36621941594317287\n",
      "P:1750, TP:464, FP:1286, FN:803, TN:33123, N:33926\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio 1\n",
      "class_ratio 2\n",
      "precision 0.2651428571428571\n",
      "recall 0.36621941594317287\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 649.8499447703362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/0 0\n",
      "Recall 0/40 0.0\n",
      "P:0, TP:0, FP:0, FN:40, TN:1960, N:2000\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 350.5728516690433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 5/78 0.0641025641025641\n",
      "Recall 5/40 0.125\n",
      "P:78, TP:5, FP:73, FN:35, TN:1887, N:1922\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 116.49241442675702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 4/31 0.12903225806451613\n",
      "Recall 4/40 0.1\n",
      "P:31, TP:4, FP:27, FN:36, TN:1933, N:1969\n",
      "Model saw positive samples 3000 times and background samples 12000\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 189/1103 0.17135086128739802\n",
      "Recall 189/1267 0.14917127071823205\n",
      "P:1103, TP:189, FP:914, FN:1078, TN:33495, N:34573\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio 1\n",
      "class_ratio 4\n",
      "precision 0.17135086128739802\n",
      "recall 0.14917127071823205\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 484.3729382492602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/0 0\n",
      "Recall 0/40 0.0\n",
      "P:0, TP:0, FP:0, FN:40, TN:1960, N:2000\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 293.17150229681283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/2 0.0\n",
      "Recall 0/40 0.0\n",
      "P:2, TP:0, FP:2, FN:40, TN:1958, N:1998\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 92.03241161175538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 2/12 0.16666666666666666\n",
      "Recall 2/40 0.05\n",
      "P:12, TP:2, FP:10, FN:38, TN:1950, N:1988\n",
      "Model saw positive samples 1665 times and background samples 13335\n",
      "Ratio 1:8.00900900900901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 206/439 0.46924829157175396\n",
      "Recall 206/1267 0.16258879242304658\n",
      "P:439, TP:206, FP:233, FN:1061, TN:34176, N:35237\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio 1\n",
      "class_ratio 8\n",
      "precision 0.46924829157175396\n",
      "recall 0.16258879242304658\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 853.6391683220863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 10/573 0.017452006980802792\n",
      "Recall 10/40 0.25\n",
      "P:573, TP:10, FP:563, FN:30, TN:1397, N:1427\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 291.8435784452595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 2/71 0.028169014084507043\n",
      "Recall 2/40 0.05\n",
      "P:71, TP:2, FP:69, FN:38, TN:1891, N:1929\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 55.08163957239594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 1/33 0.030303030303030304\n",
      "Recall 1/40 0.025\n",
      "P:33, TP:1, FP:32, FN:39, TN:1928, N:1967\n",
      "Model saw positive samples 7500 times and background samples 7500\n",
      "Ratio 1:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 216/503 0.4294234592445328\n",
      "Recall 216/1267 0.17048145224940806\n",
      "P:503, TP:216, FP:287, FN:1051, TN:34122, N:35173\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio 2\n",
      "class_ratio 1\n",
      "precision 0.4294234592445328\n",
      "recall 0.17048145224940806\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 795.4382631182671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/3 0.0\n",
      "Recall 0/40 0.0\n",
      "P:3, TP:0, FP:3, FN:40, TN:1957, N:1997\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 386.72980129159987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 3/20 0.15\n",
      "Recall 3/40 0.075\n",
      "P:20, TP:3, FP:17, FN:37, TN:1943, N:1980\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 113.12428697571158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 7/71 0.09859154929577464\n",
      "Recall 7/40 0.175\n",
      "P:71, TP:7, FP:64, FN:33, TN:1896, N:1929\n",
      "Model saw positive samples 4998 times and background samples 10002\n",
      "Ratio 1:2.0012004801920766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 275/1305 0.210727969348659\n",
      "Recall 275/1267 0.2170481452249408\n",
      "P:1305, TP:275, FP:1030, FN:992, TN:33379, N:34371\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio 2\n",
      "class_ratio 2\n",
      "precision 0.210727969348659\n",
      "recall 0.2170481452249408\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 674.2693476304412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/0 0\n",
      "Recall 0/40 0.0\n",
      "P:0, TP:0, FP:0, FN:40, TN:1960, N:2000\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 401.07263765949756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 7/118 0.059322033898305086\n",
      "Recall 7/40 0.175\n",
      "P:118, TP:7, FP:111, FN:33, TN:1849, N:1882\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 124.33188724494539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 4/14 0.2857142857142857\n",
      "Recall 4/40 0.1\n",
      "P:14, TP:4, FP:10, FN:36, TN:1950, N:1986\n",
      "Model saw positive samples 3000 times and background samples 12000\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 79/386 0.20466321243523317\n",
      "Recall 79/1267 0.062352012628255724\n",
      "P:386, TP:79, FP:307, FN:1188, TN:34102, N:35290\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio 2\n",
      "class_ratio 4\n",
      "precision 0.20466321243523317\n",
      "recall 0.062352012628255724\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 471.6965223029256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/0 0\n",
      "Recall 0/40 0.0\n",
      "P:0, TP:0, FP:0, FN:40, TN:1960, N:2000\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 353.2985911844298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/0 0\n",
      "Recall 0/40 0.0\n",
      "P:0, TP:0, FP:0, FN:40, TN:1960, N:2000\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 154.49234365089796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 1/19 0.05263157894736842\n",
      "Recall 1/40 0.025\n",
      "P:19, TP:1, FP:18, FN:39, TN:1942, N:1981\n",
      "Model saw positive samples 1665 times and background samples 13335\n",
      "Ratio 1:8.00900900900901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 225/673 0.3343239227340267\n",
      "Recall 225/1267 0.17758484609313338\n",
      "P:673, TP:225, FP:448, FN:1042, TN:33961, N:35003\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio 2\n",
      "class_ratio 8\n",
      "precision 0.3343239227340267\n",
      "recall 0.17758484609313338\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 863.6748497486115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 19/261 0.07279693486590039\n",
      "Recall 19/40 0.475\n",
      "P:261, TP:19, FP:242, FN:21, TN:1718, N:1739\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 325.6189609277062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 6/54 0.1111111111111111\n",
      "Recall 6/40 0.15\n",
      "P:54, TP:6, FP:48, FN:34, TN:1912, N:1946\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 55.53576231840998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 6/31 0.1935483870967742\n",
      "Recall 6/40 0.15\n",
      "P:31, TP:6, FP:25, FN:34, TN:1935, N:1969\n",
      "Model saw positive samples 7500 times and background samples 7500\n",
      "Ratio 1:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 207/974 0.21252566735112938\n",
      "Recall 207/1267 0.1633780584056827\n",
      "P:974, TP:207, FP:767, FN:1060, TN:33642, N:34702\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio 4\n",
      "class_ratio 1\n",
      "precision 0.21252566735112938\n",
      "recall 0.1633780584056827\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 786.9828036725521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/47 0.0\n",
      "Recall 0/40 0.0\n",
      "P:47, TP:0, FP:47, FN:40, TN:1913, N:1953\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 434.60826200805604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 2/17 0.11764705882352941\n",
      "Recall 2/40 0.05\n",
      "P:17, TP:2, FP:15, FN:38, TN:1945, N:1983\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 111.03646565403324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 5/29 0.1724137931034483\n",
      "Recall 5/40 0.125\n",
      "P:29, TP:5, FP:24, FN:35, TN:1936, N:1971\n",
      "Model saw positive samples 4998 times and background samples 10002\n",
      "Ratio 1:2.0012004801920766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 312/1217 0.2563681183237469\n",
      "Recall 312/1267 0.2462509865824783\n",
      "P:1217, TP:312, FP:905, FN:955, TN:33504, N:34459\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio 4\n",
      "class_ratio 2\n",
      "precision 0.2563681183237469\n",
      "recall 0.2462509865824783\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [07:01<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 642.491618104279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:48<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/0 0\n",
      "Recall 0/40 0.0\n",
      "P:0, TP:0, FP:0, FN:40, TN:1960, N:2000\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:34<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 334.0277934158221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:48<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/51 0.0\n",
      "Recall 0/40 0.0\n",
      "P:51, TP:0, FP:51, FN:40, TN:1909, N:1949\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:34<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 96.0832635264378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:48<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/3 0.0\n",
      "Recall 0/40 0.0\n",
      "P:3, TP:0, FP:3, FN:40, TN:1957, N:1997\n",
      "Model saw positive samples 3000 times and background samples 12000\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:18<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 152/317 0.4794952681388013\n",
      "Recall 152/1267 0.11996842936069456\n",
      "P:317, TP:152, FP:165, FN:1115, TN:34244, N:35359\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio 4\n",
      "class_ratio 4\n",
      "precision 0.4794952681388013\n",
      "recall 0.11996842936069456\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:48<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 483.3613896332681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:48<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/0 0\n",
      "Recall 0/40 0.0\n",
      "P:0, TP:0, FP:0, FN:40, TN:1960, N:2000\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:34<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 344.3110640505329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:48<00:00, 10.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 3/13 0.23076923076923078\n",
      "Recall 3/40 0.075\n",
      "P:13, TP:3, FP:10, FN:37, TN:1950, N:1987\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:34<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 136.76308055478148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:48<00:00, 10.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 3/29 0.10344827586206896\n",
      "Recall 3/40 0.075\n",
      "P:29, TP:3, FP:26, FN:37, TN:1934, N:1971\n",
      "Model saw positive samples 1665 times and background samples 13335\n",
      "Ratio 1:8.00900900900901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [15:51<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 146/694 0.21037463976945245\n",
      "Recall 146/1267 0.11523283346487766\n",
      "P:694, TP:146, FP:548, FN:1121, TN:33861, N:34982\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio 4\n",
      "class_ratio 8\n",
      "precision 0.21037463976945245\n",
      "recall 0.11523283346487766\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 859.1159778237343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 16/526 0.030418250950570342\n",
      "Recall 16/40 0.4\n",
      "P:526, TP:16, FP:510, FN:24, TN:1450, N:1474\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 388.72910123039037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 16/122 0.13114754098360656\n",
      "Recall 16/40 0.4\n",
      "P:122, TP:16, FP:106, FN:24, TN:1854, N:1878\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 110.1561396557372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 15/81 0.18518518518518517\n",
      "Recall 15/40 0.375\n",
      "P:81, TP:15, FP:66, FN:25, TN:1894, N:1919\n",
      "Model saw positive samples 7500 times and background samples 7500\n",
      "Ratio 1:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:43<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 305/2057 0.1482741857073408\n",
      "Recall 305/1267 0.24072612470402527\n",
      "P:2057, TP:305, FP:1752, FN:962, TN:32657, N:33619\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio -1\n",
      "class_ratio 1\n",
      "precision 0.1482741857073408\n",
      "recall 0.24072612470402527\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 795.0761062949896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/34 0.0\n",
      "Recall 0/40 0.0\n",
      "P:34, TP:0, FP:34, FN:40, TN:1926, N:1966\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 318.70216929074377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 8/30 0.26666666666666666\n",
      "Recall 8/40 0.2\n",
      "P:30, TP:8, FP:22, FN:32, TN:1938, N:1970\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 55.31999122904381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 8/79 0.10126582278481013\n",
      "Recall 8/40 0.2\n",
      "P:79, TP:8, FP:71, FN:32, TN:1889, N:1921\n",
      "Model saw positive samples 4998 times and background samples 10002\n",
      "Ratio 1:2.0012004801920766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [15:35<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 394/1959 0.20112302194997447\n",
      "Recall 394/1267 0.31097079715864245\n",
      "P:1959, TP:394, FP:1565, FN:873, TN:32844, N:33717\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio -1\n",
      "class_ratio 2\n",
      "precision 0.20112302194997447\n",
      "recall 0.31097079715864245\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 629.926219008863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/0 0\n",
      "Recall 0/40 0.0\n",
      "P:0, TP:0, FP:0, FN:40, TN:1960, N:2000\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 334.55428583035246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 3/14 0.21428571428571427\n",
      "Recall 3/40 0.075\n",
      "P:14, TP:3, FP:11, FN:37, TN:1949, N:1986\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 87.46529566694517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 3/13 0.23076923076923078\n",
      "Recall 3/40 0.075\n",
      "P:13, TP:3, FP:10, FN:37, TN:1950, N:1987\n",
      "Model saw positive samples 3000 times and background samples 12000\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:43<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 205/412 0.4975728155339806\n",
      "Recall 205/1267 0.16179952644041043\n",
      "P:412, TP:205, FP:207, FN:1062, TN:34202, N:35264\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio -1\n",
      "class_ratio 4\n",
      "precision 0.4975728155339806\n",
      "recall 0.16179952644041043\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 466.21257024630904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/0 0\n",
      "Recall 0/40 0.0\n",
      "P:0, TP:0, FP:0, FN:40, TN:1960, N:2000\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 288.0803499384783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/4 0.0\n",
      "Recall 0/40 0.0\n",
      "P:4, TP:0, FP:4, FN:40, TN:1956, N:1996\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 84.03810229338706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 2/35 0.05714285714285714\n",
      "Recall 2/40 0.05\n",
      "P:35, TP:2, FP:33, FN:38, TN:1927, N:1965\n",
      "Model saw positive samples 1665 times and background samples 13335\n",
      "Ratio 1:8.00900900900901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [15:31<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 250/555 0.45045045045045046\n",
      "Recall 250/1267 0.19731649565903708\n",
      "P:555, TP:250, FP:305, FN:1017, TN:34104, N:35121\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-06\n",
      "oversampling_ratio -1\n",
      "class_ratio 8\n",
      "precision 0.45045045045045046\n",
      "recall 0.19731649565903708\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 565.400098690996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/25 0.0\n",
      "Recall 0/40 0.0\n",
      "P:25, TP:0, FP:25, FN:40, TN:1935, N:1975\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:52<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 64.73905466668657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:56<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 2/17 0.11764705882352941\n",
      "Recall 2/40 0.05\n",
      "P:17, TP:2, FP:15, FN:38, TN:1945, N:1983\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [07:19<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 16.438275128035457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:54<00:00,  9.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 15/153 0.09803921568627451\n",
      "Recall 15/40 0.375\n",
      "P:153, TP:15, FP:138, FN:25, TN:1822, N:1847\n",
      "Model saw positive samples 7500 times and background samples 7500\n",
      "Ratio 1:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:58<00:00,  9.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 321/2543 0.12622886354699175\n",
      "Recall 321/1267 0.2533543804262036\n",
      "P:2543, TP:321, FP:2222, FN:946, TN:32187, N:33133\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio 1\n",
      "class_ratio 1\n",
      "precision 0.12622886354699175\n",
      "recall 0.2533543804262036\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 476.4789611713495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 19/158 0.12025316455696203\n",
      "Recall 19/40 0.475\n",
      "P:158, TP:19, FP:139, FN:21, TN:1821, N:1842\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 64.01990477002983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 8/59 0.13559322033898305\n",
      "Recall 8/40 0.2\n",
      "P:59, TP:8, FP:51, FN:32, TN:1909, N:1941\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [07:03<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 18.343698071315885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:54<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 11/51 0.21568627450980393\n",
      "Recall 11/40 0.275\n",
      "P:51, TP:11, FP:40, FN:29, TN:1920, N:1949\n",
      "Model saw positive samples 4998 times and background samples 10002\n",
      "Ratio 1:2.0012004801920766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [15:05<00:00,  9.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 322/1184 0.2719594594594595\n",
      "Recall 322/1267 0.2541436464088398\n",
      "P:1184, TP:322, FP:862, FN:945, TN:33547, N:34492\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio 1\n",
      "class_ratio 2\n",
      "precision 0.2719594594594595\n",
      "recall 0.2541436464088398\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 410.5542025293689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 3/52 0.057692307692307696\n",
      "Recall 3/40 0.075\n",
      "P:52, TP:3, FP:49, FN:37, TN:1911, N:1948\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 72.57835809673998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/4 0.0\n",
      "Recall 0/40 0.0\n",
      "P:4, TP:0, FP:4, FN:40, TN:1956, N:1996\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 19.436014942239126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 11/61 0.18032786885245902\n",
      "Recall 11/40 0.275\n",
      "P:61, TP:11, FP:50, FN:29, TN:1910, N:1939\n",
      "Model saw positive samples 3000 times and background samples 12000\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:43<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 258/911 0.283205268935236\n",
      "Recall 258/1267 0.20363062352012629\n",
      "P:911, TP:258, FP:653, FN:1009, TN:33756, N:34765\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio 1\n",
      "class_ratio 4\n",
      "precision 0.283205268935236\n",
      "recall 0.20363062352012629\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 342.7054660996655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 16/134 0.11940298507462686\n",
      "Recall 16/40 0.4\n",
      "P:134, TP:16, FP:118, FN:24, TN:1842, N:1866\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 74.48041439316876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 7/16 0.4375\n",
      "Recall 7/40 0.175\n",
      "P:16, TP:7, FP:9, FN:33, TN:1951, N:1984\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 17.36487812352425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 1/1 1.0\n",
      "Recall 1/40 0.025\n",
      "P:1, TP:1, FP:0, FN:39, TN:1960, N:1999\n",
      "Model saw positive samples 1665 times and background samples 13335\n",
      "Ratio 1:8.00900900900901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:43<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 156/260 0.6\n",
      "Recall 156/1267 0.12312549329123915\n",
      "P:260, TP:156, FP:104, FN:1111, TN:34305, N:35416\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio 1\n",
      "class_ratio 8\n",
      "precision 0.6\n",
      "recall 0.12312549329123915\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 586.661451674765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 13/65 0.2\n",
      "Recall 13/40 0.325\n",
      "P:65, TP:13, FP:52, FN:27, TN:1908, N:1935\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 57.83421992723015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 9/49 0.1836734693877551\n",
      "Recall 9/40 0.225\n",
      "P:49, TP:9, FP:40, FN:31, TN:1920, N:1951\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 21.56464953732211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 11/56 0.19642857142857142\n",
      "Recall 11/40 0.275\n",
      "P:56, TP:11, FP:45, FN:29, TN:1915, N:1944\n",
      "Model saw positive samples 7500 times and background samples 7500\n",
      "Ratio 1:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:44<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 332/1118 0.29695885509839\n",
      "Recall 332/1267 0.26203630623520124\n",
      "P:1118, TP:332, FP:786, FN:935, TN:33623, N:34558\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio 2\n",
      "class_ratio 1\n",
      "precision 0.29695885509839\n",
      "recall 0.26203630623520124\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 580.9927543324884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 3/31 0.0967741935483871\n",
      "Recall 3/40 0.075\n",
      "P:31, TP:3, FP:28, FN:37, TN:1932, N:1969\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 78.57815654427395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/2 0.0\n",
      "Recall 0/40 0.0\n",
      "P:2, TP:0, FP:2, FN:40, TN:1958, N:1998\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 21.07377689175337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/12 0.0\n",
      "Recall 0/40 0.0\n",
      "P:12, TP:0, FP:12, FN:40, TN:1948, N:1988\n",
      "Model saw positive samples 4998 times and background samples 10002\n",
      "Ratio 1:2.0012004801920766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:43<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 242/366 0.6612021857923497\n",
      "Recall 242/1267 0.1910023677979479\n",
      "P:366, TP:242, FP:124, FN:1025, TN:34285, N:35310\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio 2\n",
      "class_ratio 2\n",
      "precision 0.6612021857923497\n",
      "recall 0.1910023677979479\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 503.3593895176891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 9/125 0.072\n",
      "Recall 9/40 0.225\n",
      "P:125, TP:9, FP:116, FN:31, TN:1844, N:1875\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 101.87183774498408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 6/48 0.125\n",
      "Recall 6/40 0.15\n",
      "P:48, TP:6, FP:42, FN:34, TN:1918, N:1952\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 21.893050622435112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 2/8 0.25\n",
      "Recall 2/40 0.05\n",
      "P:8, TP:2, FP:6, FN:38, TN:1954, N:1992\n",
      "Model saw positive samples 3000 times and background samples 12000\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:43<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 144/304 0.47368421052631576\n",
      "Recall 144/1267 0.11365430149960537\n",
      "P:304, TP:144, FP:160, FN:1123, TN:34249, N:35372\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio 2\n",
      "class_ratio 4\n",
      "precision 0.47368421052631576\n",
      "recall 0.11365430149960537\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 414.79852609569207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 5/24 0.20833333333333334\n",
      "Recall 5/40 0.125\n",
      "P:24, TP:5, FP:19, FN:35, TN:1941, N:1976\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 144.7562053304864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 7/41 0.17073170731707318\n",
      "Recall 7/40 0.175\n",
      "P:41, TP:7, FP:34, FN:33, TN:1926, N:1959\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 31.233268288808176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 3/11 0.2727272727272727\n",
      "Recall 3/40 0.075\n",
      "P:11, TP:3, FP:8, FN:37, TN:1952, N:1989\n",
      "Model saw positive samples 1665 times and background samples 13335\n",
      "Ratio 1:8.00900900900901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:44<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 69/179 0.3854748603351955\n",
      "Recall 69/1267 0.05445935280189424\n",
      "P:179, TP:69, FP:110, FN:1198, TN:34299, N:35497\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio 2\n",
      "class_ratio 8\n",
      "precision 0.3854748603351955\n",
      "recall 0.05445935280189424\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 855.8874216228724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 1/4 0.25\n",
      "Recall 1/40 0.025\n",
      "P:4, TP:1, FP:3, FN:39, TN:1957, N:1996\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 856.8398695737123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/0 0\n",
      "Recall 0/40 0.0\n",
      "P:0, TP:0, FP:0, FN:40, TN:1960, N:2000\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 287.7537180376239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 5/87 0.05747126436781609\n",
      "Recall 5/40 0.125\n",
      "P:87, TP:5, FP:82, FN:35, TN:1878, N:1913\n",
      "Model saw positive samples 7500 times and background samples 7500\n",
      "Ratio 1:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:44<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 273/1720 0.15872093023255815\n",
      "Recall 273/1267 0.2154696132596685\n",
      "P:1720, TP:273, FP:1447, FN:994, TN:32962, N:33956\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio 4\n",
      "class_ratio 1\n",
      "precision 0.15872093023255815\n",
      "recall 0.2154696132596685\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 450.47310350532643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 23/125 0.184\n",
      "Recall 23/40 0.575\n",
      "P:125, TP:23, FP:102, FN:17, TN:1858, N:1875\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 65.69221738298074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 1/12 0.08333333333333333\n",
      "Recall 1/40 0.025\n",
      "P:12, TP:1, FP:11, FN:39, TN:1949, N:1988\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 11.614759060284996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 3/45 0.06666666666666667\n",
      "Recall 3/40 0.075\n",
      "P:45, TP:3, FP:42, FN:37, TN:1918, N:1955\n",
      "Model saw positive samples 4998 times and background samples 10002\n",
      "Ratio 1:2.0012004801920766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:44<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 302/828 0.3647342995169082\n",
      "Recall 302/1267 0.23835832675611682\n",
      "P:828, TP:302, FP:526, FN:965, TN:33883, N:34848\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio 4\n",
      "class_ratio 2\n",
      "precision 0.3647342995169082\n",
      "recall 0.23835832675611682\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 471.9697584719397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 25/200 0.125\n",
      "Recall 25/40 0.625\n",
      "P:200, TP:25, FP:175, FN:15, TN:1785, N:1800\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 78.38371622035629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 5/36 0.1388888888888889\n",
      "Recall 5/40 0.125\n",
      "P:36, TP:5, FP:31, FN:35, TN:1929, N:1964\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 30.609406098541513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 3/19 0.15789473684210525\n",
      "Recall 3/40 0.075\n",
      "P:19, TP:3, FP:16, FN:37, TN:1944, N:1981\n",
      "Model saw positive samples 3000 times and background samples 12000\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:44<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 114/264 0.4318181818181818\n",
      "Recall 114/1267 0.08997632202052092\n",
      "P:264, TP:114, FP:150, FN:1153, TN:34259, N:35412\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio 4\n",
      "class_ratio 4\n",
      "precision 0.4318181818181818\n",
      "recall 0.08997632202052092\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 406.80972992558964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 1/5 0.2\n",
      "Recall 1/40 0.025\n",
      "P:5, TP:1, FP:4, FN:39, TN:1956, N:1995\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 94.50007009816181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 10/63 0.15873015873015872\n",
      "Recall 10/40 0.25\n",
      "P:63, TP:10, FP:53, FN:30, TN:1907, N:1937\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 20.4421176573087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 3/15 0.2\n",
      "Recall 3/40 0.075\n",
      "P:15, TP:3, FP:12, FN:37, TN:1948, N:1985\n",
      "Model saw positive samples 1665 times and background samples 13335\n",
      "Ratio 1:8.00900900900901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:44<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 103/233 0.44206008583690987\n",
      "Recall 103/1267 0.08129439621152329\n",
      "P:233, TP:103, FP:130, FN:1164, TN:34279, N:35443\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio 4\n",
      "class_ratio 8\n",
      "precision 0.44206008583690987\n",
      "recall 0.08129439621152329\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 506.05574544752017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 11/110 0.1\n",
      "Recall 11/40 0.275\n",
      "P:110, TP:11, FP:99, FN:29, TN:1861, N:1890\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 50.36768973863218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 2/25 0.08\n",
      "Recall 2/40 0.05\n",
      "P:25, TP:2, FP:23, FN:38, TN:1937, N:1975\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 5.348120781127363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/9 0.0\n",
      "Recall 0/40 0.0\n",
      "P:9, TP:0, FP:9, FN:40, TN:1951, N:1991\n",
      "Model saw positive samples 7500 times and background samples 7500\n",
      "Ratio 1:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:44<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 164/561 0.29233511586452765\n",
      "Recall 164/1267 0.12943962115232832\n",
      "P:561, TP:164, FP:397, FN:1103, TN:34012, N:35115\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio -1\n",
      "class_ratio 1\n",
      "precision 0.29233511586452765\n",
      "recall 0.12943962115232832\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [07:23<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 523.3046237505041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:54<00:00,  9.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 25/260 0.09615384615384616\n",
      "Recall 25/40 0.625\n",
      "P:260, TP:25, FP:235, FN:15, TN:1725, N:1740\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [07:13<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 77.34155712477514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:54<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/45 0.0\n",
      "Recall 0/40 0.0\n",
      "P:45, TP:0, FP:45, FN:40, TN:1915, N:1955\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [07:01<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 16.316617754215258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/7 0.0\n",
      "Recall 0/40 0.0\n",
      "P:7, TP:0, FP:7, FN:40, TN:1953, N:1993\n",
      "Model saw positive samples 4998 times and background samples 10002\n",
      "Ratio 1:2.0012004801920766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:43<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 150/279 0.5376344086021505\n",
      "Recall 150/1267 0.11838989739542226\n",
      "P:279, TP:150, FP:129, FN:1117, TN:34280, N:35397\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio -1\n",
      "class_ratio 2\n",
      "precision 0.5376344086021505\n",
      "recall 0.11838989739542226\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 550.7147714924067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 4/30 0.13333333333333333\n",
      "Recall 4/40 0.1\n",
      "P:30, TP:4, FP:26, FN:36, TN:1934, N:1970\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 134.636980831041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 2/13 0.15384615384615385\n",
      "Recall 2/40 0.05\n",
      "P:13, TP:2, FP:11, FN:38, TN:1949, N:1987\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 57.14346828113776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 5/31 0.16129032258064516\n",
      "Recall 5/40 0.125\n",
      "P:31, TP:5, FP:26, FN:35, TN:1934, N:1969\n",
      "Model saw positive samples 3000 times and background samples 12000\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [15:33<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 112/339 0.3303834808259587\n",
      "Recall 112/1267 0.08839779005524862\n",
      "P:339, TP:112, FP:227, FN:1155, TN:34182, N:35337\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio -1\n",
      "class_ratio 4\n",
      "precision 0.3303834808259587\n",
      "recall 0.08839779005524862\n",
      "XXX\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [07:32<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 354.98372351098806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:53<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 0/4 0.0\n",
      "Recall 0/40 0.0\n",
      "P:4, TP:0, FP:4, FN:40, TN:1956, N:1996\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [07:12<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 72.119379337586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:56<00:00,  8.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 2/15 0.13333333333333333\n",
      "Recall 2/40 0.05\n",
      "P:15, TP:2, FP:13, FN:38, TN:1947, N:1985\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [06:53<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 14.230225359176984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:49<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 2/8 0.25\n",
      "Recall 2/40 0.05\n",
      "P:8, TP:2, FP:6, FN:38, TN:1954, N:1992\n",
      "Model saw positive samples 1665 times and background samples 13335\n",
      "Ratio 1:8.00900900900901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [14:40<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 177/613 0.28874388254486133\n",
      "Recall 177/1267 0.13970007892659828\n",
      "P:613, TP:177, FP:436, FN:1090, TN:33973, N:35063\n",
      "\n",
      "XXX\n",
      "learning_rate 2e-05\n",
      "oversampling_ratio -1\n",
      "class_ratio 8\n",
      "precision 0.28874388254486133\n",
      "recall 0.13970007892659828\n",
      "XXX\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "data = []\n",
    "learning_rates = [2e-6, 2e-5]\n",
    "oversampling_ratios = [1,2,4,-1]\n",
    "class_ratios = [1,2,4,8]\n",
    "for lrx in learning_rates:\n",
    "    for orx in oversampling_ratios:\n",
    "        for crx in class_ratios:\n",
    "            learning_rate = lrx\n",
    "            oversampling_ratio = orx\n",
    "            class_ratio = crx\n",
    "            model, precision, recall = finetune_and_eval()\n",
    "            del model\n",
    "\n",
    "            print()\n",
    "            print('XXX')\n",
    "            print('learning_rate', learning_rate)\n",
    "            print('oversampling_ratio', oversampling_ratio)\n",
    "            print('class_ratio', class_ratio)\n",
    "            print('precision', precision)\n",
    "            print('recall', recall)\n",
    "            print('XXX')\n",
    "            print()\n",
    "            data.append({\n",
    "                'learning_rate': learning_rate,\n",
    "                'oversampling_ratio': oversampling_ratio,\n",
    "                'class_ratio': class_ratio,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "            })\n",
    "\n",
    "            torch.cuda.empty_cache() \n",
    "            gc.collect()\n",
    "\n",
    "with open('results-xds.json', 'w') as f:\n",
    "    json.dump(data, f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model.codebert\n",
    "for param in tokenizer.parameters():\n",
    "    param.requires_grad = False\n",
    "tokenizer.eval()\n",
    "tokenizer.to(device)\n",
    "\n",
    "print('Embedding test set')\n",
    "embed_files(tokenizer, test_data[0])\n",
    "embed_files(tokenizer, test_data[1])\n",
    "print('Embedding evaluation set')\n",
    "embed_files(tokenizer, eval_data[0])\n",
    "embed_files(tokenizer, eval_data[1])\n",
    "print('Embedding train set')\n",
    "embed_files(tokenizer, train_data[0])\n",
    "embed_files(tokenizer, train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sample files to embedded files\n",
    "train_data, eval_data, test_data\n",
    "train_data_embeded_pos = map_files_to_new_repo(train_data[0])\n",
    "train_data_embeded_bac = map_files_to_new_repo(train_data[1])\n",
    "\n",
    "eval_data_embeded_pos = map_files_to_new_repo(eval_data[0])\n",
    "eval_data_embeded_bac = map_files_to_new_repo(eval_data[1])\n",
    "\n",
    "test_data_embeded_pos = map_files_to_new_repo(test_data[0])\n",
    "test_data_embeded_bac = map_files_to_new_repo(test_data[1])\n",
    "\n",
    "\n",
    "train_dataset_embeded = CommitLevelRawDataset()\n",
    "train_dataset_embeded.load_files(train_data_embeded_pos, train_data_embeded_bac)\n",
    "train_dataset_embeded = UnderSampledDataset(train_dataset_embeded, aggregator_class_ratio)\n",
    "eval_dataset_embeded = CommitLevelRawDataset()\n",
    "eval_dataset_embeded.load_files(eval_data_embeded_pos, eval_data_embeded_bac)\n",
    "test_dataset_embeded = CommitLevelRawDataset()\n",
    "test_dataset_embeded.load_files(test_data_embeded_pos, test_data_embeded_bac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = AggregatorModel()\n",
    "loss_module = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=aggregator_learning_rate, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "    num_warmup_steps=int(len(train_dataset_embeded)*0.25), \n",
    "    num_training_steps=len(train_dataset_embeded)*num_epochs_)\n",
    "\n",
    "# Prep the loaders\n",
    "train_data_embeded_loader = data.DataLoader(train_dataset_embeded, batch_size=1, drop_last=True, shuffle=True)\n",
    "eval_data_embeded_loader = data.DataLoader(eval_dataset_embeded, batch_size=1, drop_last=True, shuffle=True)\n",
    "\n",
    "train_model2(model, optimizer, train_data_embeded_loader, loss_module, scheduler, test_loader=eval_data_embeded_loader)\n",
    "torch.save(model.state_dict(), f'{work_dir}/model_aggregator_{model_name}_final.pickle')\n",
    "\n",
    "# Test the model on eval subset\n",
    "test_data_embeded_loader = data.DataLoader(test_dataset_embeded, drop_last=True, batch_size=1)\n",
    "eval_model2(model, test_data_embeded_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
