{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "import gc\n",
    "gc.collect()\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import uuid\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_from_directory(dir, extension=None, startswith=None):\n",
    "    files_list = []\n",
    "    for root, subdirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            if extension != None and not file.endswith(extension):\n",
    "                continue\n",
    "\n",
    "            if startswith != None and not file.startswith(startswith):\n",
    "                continue\n",
    "            \n",
    "            file_path = os.path.join(root, file)\n",
    "            files_list.append(file_path)\n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_seminames(files):\n",
    "    repos = set()\n",
    "    for x in files:\n",
    "        segments = x.split('-')\n",
    "        repo_semi_full_name = f'{segments[2]}-{segments[3]}'\n",
    "        repos.add(repo_semi_full_name)\n",
    "\n",
    "    return repos\n",
    "\n",
    "\n",
    "def get_files_in_set(filenames, test_repos):\n",
    "    filtered_json_files = []\n",
    "    for x in filenames:\n",
    "        is_test = False\n",
    "        for r in test_repos:\n",
    "            if r in x:\n",
    "                is_test = True\n",
    "                break\n",
    "        if is_test:\n",
    "            filtered_json_files.append(x)\n",
    "\n",
    "    return filtered_json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "import json\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "\n",
    "# ['js', 'jsx', 'ts', 'tsx', ]\n",
    "VALID_EXTENSIONS = set(['java'])\n",
    "\n",
    "class BaseRawDataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.positive_data = []\n",
    "        self.background_data = []\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def _load_file(self, collection, json_file):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def crop_data(self, positive_count, background_count):\n",
    "        self.positive_data = random.sample(self.positive_data, min(positive_count, len(self.positive_data)))\n",
    "        self.background_data = random.sample(self.background_data, min(background_count, len(self.background_data)))\n",
    "\n",
    "\n",
    "    def limit_data(self, limit):\n",
    "        final_positive_count = int(limit*len(self.positive_data)/(len(self.positive_data)+len(self.background_data)))\n",
    "        self.positive_data = random.sample(self.positive_data, min(final_positive_count, len(self.positive_data)))\n",
    "        self.background_data = random.sample(self.background_data, min(limit-final_positive_count, len(self.background_data)))\n",
    "        \n",
    "\n",
    "    def setup_ratios(self, oversampling_ratio, class_ratio, samples_limit):\n",
    "        if oversampling_ratio == -1:\n",
    "            oversampling_ratio = int(len(self.background_data)/(class_ratio*len(self.positive_data)))\n",
    "\n",
    "        positive_count = len(self.positive_data) * oversampling_ratio\n",
    "        self.positive_data = self.positive_data * oversampling_ratio\n",
    "        background_count = class_ratio*len(self.positive_data)\n",
    "        self.background_data = random.sample(self.background_data, min(background_count, len(self.background_data)))\n",
    "\n",
    "        final_positive_count = int(samples_limit*positive_count/(positive_count+background_count))\n",
    "        self.positive_data = random.sample(self.positive_data, min(final_positive_count, len(self.positive_data)))\n",
    "        self.background_data = random.sample(self.background_data, min(samples_limit-final_positive_count, len(self.background_data)))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def split_data(self, fraction):\n",
    "        positive_cut_point = int(fraction*len(self.positive_data))\n",
    "        background_cut_point = int(fraction*len(self.background_data))\n",
    "\n",
    "        random.shuffle(self.positive_data)\n",
    "        random.shuffle(self.background_data)\n",
    "        part_a = BaseRawDataset()\n",
    "        part_a.positive_data = self.positive_data[:positive_cut_point]\n",
    "        part_a.background_data = self.background_data[:background_cut_point]\n",
    "\n",
    "        part_b = BaseRawDataset()\n",
    "        part_b.positive_data = self.positive_data[positive_cut_point:]\n",
    "        part_b.background_data = self.background_data[background_cut_point:]\n",
    "\n",
    "        return part_a, part_b\n",
    "\n",
    "\n",
    "    def load_files(self, positive_json_files, background_json_files):\n",
    "        positive_data_temp = []\n",
    "        background_data_temp = []\n",
    "\n",
    "        for filename in positive_json_files:\n",
    "            try:\n",
    "                with open(filename, 'r') as f:\n",
    "                    temp_data = json.load(f)\n",
    "                    temp_data = [x for x in temp_data if x['file_name'].split('.')[-1] in VALID_EXTENSIONS]\n",
    "                    self._load_file(positive_data_temp, temp_data)\n",
    "            except Exception as e:\n",
    "                print('Failed to load', filename)\n",
    "                print(e)\n",
    "\n",
    "        for filename in background_json_files:\n",
    "            try:\n",
    "                with open(filename, 'r') as f:\n",
    "                    temp_data = json.load(f)\n",
    "                    temp_data = [x for x in temp_data if x['file_name'].split('.')[-1] in VALID_EXTENSIONS]\n",
    "                    self._load_file(background_data_temp, temp_data)\n",
    "            except Exception as e:\n",
    "                print('Failed to load', filename)\n",
    "                print(e)\n",
    "\n",
    "        self.positive_data = positive_data_temp\n",
    "        self.background_data = background_data_temp\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.positive_data) + len(self.background_data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.positive_data):\n",
    "            data_point = self.positive_data[idx]\n",
    "            data_label = torch.Tensor([1, 0]) \n",
    "        else:\n",
    "            data_point = self.background_data[idx - len(self.positive_data)]\n",
    "            data_label = torch.Tensor([0, 1]) \n",
    "        \n",
    "        return data_point, data_label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SampleLevelRawDataset(BaseRawDataset):\n",
    "    def __init__(self):\n",
    "        # super().__init__('positive-encodings', 'background-encodings')\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def _load_file(self, collection, json_file):\n",
    "        data = [x['commit_sample'] for x in json_file\n",
    "            if 'commit_sample' in x and x['commit_sample'] != None and len(x['commit_sample']) > 0]\n",
    "            \n",
    "        if len(data) > 0:\n",
    "            tensors = torch.stack([torch.Tensor(x).int() for x in data])\n",
    "            tensors = torch.unique(tensors, dim=0)\n",
    "            collection += tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class CommitLevelRawDataset(BaseRawDataset):\n",
    "    def __init__(self):\n",
    "        # super().__init__('embedded-positive-encodings', 'embedded-background-encodings')\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def _load_file(self, collection, json_file):\n",
    "        data = [x['commit_sample'] for x in json_file\n",
    "            if 'commit_sample' in x and x['commit_sample'] != None and len(x['commit_sample']) > 0]\n",
    "\n",
    "        if len(data) > 0:  \n",
    "            collection.append([torch.Tensor(x) for x in data])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class OverSampledDataset(data.Dataset):\n",
    "    def __init__(self, base_dataset: BaseRawDataset, ratio):\n",
    "        super().__init__()\n",
    "        self.base_dataset = base_dataset\n",
    "        self.ratio = ratio\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        self._oversample_to_ratio()\n",
    "        \n",
    "\n",
    "    def _oversample_to_ratio(self):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        target_number_of_positive = int(len(self.base_dataset.background_data) / self.ratio)\n",
    "        whole_repetition = floor(target_number_of_positive/len(self.base_dataset.positive_data))\n",
    "        self.data = self.base_dataset.positive_data * whole_repetition\n",
    "        extra_sampled = random.sample(self.base_dataset.positive_data, target_number_of_positive-len(self.data))\n",
    "        self.data += extra_sampled\n",
    "\n",
    "        self.labels = [[1, 0] for _ in self.data] + [[0, 1] for _ in self.base_dataset.background_data]\n",
    "        # self.labels = [[1, -1] for _ in self.data] + [[-1, 1] for _ in background_data]\n",
    "        self.data += self.base_dataset.background_data\n",
    "        \n",
    "\n",
    "        self.labels = torch.Tensor(self.labels)\n",
    "        self.labels = self.labels.int()\n",
    "        print('Data loaded')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.data[idx]\n",
    "        data_label = self.labels[idx]\n",
    "        return data_point, data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class UnderSampledDataset(data.Dataset):\n",
    "    def __init__(self, base_dataset:BaseRawDataset, ratio):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.base_dataset = base_dataset\n",
    "\n",
    "        self._undersample_to_ratio(ratio)\n",
    "        \n",
    "\n",
    "    def _undersample_to_ratio(self, ratio):\n",
    "        target_number_of_background = int(len(self.base_dataset.positive_data) * ratio)\n",
    "        if target_number_of_background > len(self.base_dataset.background_data):\n",
    "            raise Exception(\"Cannot undersample\")\n",
    "\n",
    "        background_sampled = random.sample(self.base_dataset.background_data, target_number_of_background)\n",
    "        self.data = self.base_dataset.positive_data + background_sampled\n",
    "\n",
    "        # self.labels = [torch.Tensor([1, -1]).int() for x in positive_data] + [torch.Tensor([-1, 1]).int() for x in background_sampled]\n",
    "        self.labels = [torch.Tensor([1, 0]).int() for x in self.base_dataset.positive_data] + [torch.Tensor([0, 1]).int() for x in background_sampled]\n",
    "        print('Data loaded')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.data[idx]\n",
    "        data_label = self.labels[idx]\n",
    "        return data_point, data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertAndLinear(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.codebert = AutoModel.from_pretrained(base_model)\n",
    "        self.linear1 = nn.Linear(768, 2)\n",
    "        # self.act_fn = nn.Softmax()\n",
    "        # self.act_fn = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_1 = self.codebert(x)\n",
    "        x_2 = x_1[0]\n",
    "        x_22 = x_2[:,0,:]\n",
    "        x_3 = self.linear1(x_22)\n",
    "        # x_4 = self.act_fn(x_3)\n",
    "        return x_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import bidirectional\n",
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class LstmAggregator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=768,\n",
    "            hidden_size=512,\n",
    "            num_layers = 5,\n",
    "            # bidirectional=True,\n",
    "            dropout=0.2,\n",
    "            batch_first = False)\n",
    "        self.linear1 = nn.Linear(512, 2)\n",
    "        self.act_fn = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        xx = x.squeeze(1)\n",
    "        xx = xx.squeeze(1)\n",
    "        lenx = xx.shape[0]\n",
    "        out = self.lstm(xx.view(lenx, 1, -1))\n",
    "        x_3 = self.linear1(out[0][-1])\n",
    "        return x_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "class ConvAggregator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.max_vector_size = 100\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=(2,8), stride=(1, 4))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(4,8), stride=(2, 4))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(4, 1, kernel_size=4, stride=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=4, stride=2)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.05)\n",
    "        self.head = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_2 = torch.zeros([1, 1, self.max_vector_size, 768])\n",
    "        x_2 = x_2.to(x[0].get_device())\n",
    "        xx = x.movedim(0,-2)\n",
    "        _, _, h, w = xx.shape\n",
    "        if h > self.max_vector_size:\n",
    "            xx = xx[:,:,0:self.max_vector_size,:]\n",
    "            h = self.max_vector_size\n",
    "        x_2[0, 0, 0:h, 0:w] = xx\n",
    "\n",
    "        x_3 = self.conv1(x_2)\n",
    "        x_4 = self.relu1(x_3)\n",
    "        x_5 = self.pool1(x_4)\n",
    "        \n",
    "        x_6 = self.conv2(x_5)\n",
    "        x_7 = self.relu2(x_6)\n",
    "        x_8 = self.pool2(x_7)\n",
    "        x_9 = torch.flatten(x_8, start_dim=1, end_dim=3)\n",
    "\n",
    "        x99= self.dropout(x_9)\n",
    "        x_10 = self.head(x99)\n",
    "        return x_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MeanAggregator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear3 = nn.Sequential(\n",
    "          nn.Linear(768, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        xx = x.squeeze(1)\n",
    "        x_2 = self.linear3(xx)\n",
    "        x_3 = torch.mean(x_2, dim=0)\n",
    "\n",
    "        return x_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "base_model = 'microsoft/graphcodebert-base'\n",
    "batch_size_ = 2\n",
    "num_epochs_ = 3\n",
    "\n",
    "fraction_of_data = 1\n",
    "\n",
    "sample_limit = 1_000_000_000\n",
    "eval_sample_limit = 1_000_000_000\n",
    "test_percentage = 0.15\n",
    "eval_percentage = 0.05\n",
    "folds_count=5\n",
    "\n",
    "learning_rate = 1e-6\n",
    "oversampling_ratio = 4\n",
    "class_ratio = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator_num_epochs_ = 15\n",
    "aggregator_class_ratio = 2\n",
    "aggregator_learning_rate = 5e-6\n",
    "\n",
    "save_model_in_each_epoch = True\n",
    "eval_model_in_each_epoch = True\n",
    "\n",
    "model_guid = str(uuid.uuid4())\n",
    "model_name = model_guid\n",
    "\n",
    "work_dir = f'D:\\\\Projects\\\\aaa\\src\\\\rq5\\\\binaries\\\\{model_name}'\n",
    "results_dir = f'D:\\\\Projects\\\\aaa\\src\\\\rq5\\\\binaries\\\\data{model_name}'\n",
    "\n",
    "# Data config - Set to None if you want to use cached datasets\n",
    "# raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\CodeParserMiner_ast'\n",
    "# raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\CodeParserMiner_edit'\n",
    "raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\AddedCodeMiner'\n",
    "# raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\RollingWindowMiner'\n",
    "# raw_input_path = None\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(work_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir(results_dir) \n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, data_loader, loss_module, scheduler, eval_loader = None):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    all_samples = 0\n",
    "    positive_samples = 0\n",
    "\n",
    "    for epoch in range(num_epochs_):\n",
    "        print(f'Epoch {epoch}/{num_epochs_}')\n",
    "        accumulated_loss = 0\n",
    "\n",
    "        with tqdm.tqdm(total=len(data_loader)) as pbar:\n",
    "            for data_inputs, data_labels in data_loader:\n",
    "                # Step 0: Diagnostics :x\n",
    "                positive_samples += len([1 for x in data_labels if x[0] == 1])\n",
    "                all_samples += len(data_labels)\n",
    "                \n",
    "                # Step 1: Mode data to device\n",
    "                data_inputs = data_inputs.to(device)\n",
    "                data_labels = data_labels.to(device)\n",
    "\n",
    "                # Step 2: Calculate model output\n",
    "                preds = model(data_inputs)\n",
    "                preds = preds.squeeze(dim=0)\n",
    "\n",
    "                # Step 3: Calculate loss\n",
    "                loss = loss_module(preds, data_labels.float())\n",
    "                accumulated_loss += loss.item()\n",
    "\n",
    "                ## Step 4: Perform backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                ## Step 5: Update the parameters\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Step 6: Progress bar\n",
    "                pbar.update()\n",
    "        print('Loss in this epoch:', accumulated_loss)\n",
    "\n",
    "        if save_model_in_each_epoch:\n",
    "            torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_epoch_{epoch}.pickle')\n",
    "\n",
    "        if eval_loader != None:\n",
    "            eval_model(model, eval_loader)\n",
    "\n",
    "\n",
    "    print(f'Model saw positive samples {positive_samples} times and background samples {all_samples-positive_samples}')\n",
    "    print(f'Ratio 1:{(all_samples-positive_samples)/positive_samples}')\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    data_size = len(data_loader)\n",
    "    with tqdm.tqdm(total=data_size) as pbar:\n",
    "        for data_inputs, data_labels in data_loader:\n",
    "            data_inputs = data_inputs.to(device)\n",
    "            data_labels = data_labels.to(device)\n",
    "            preds = model(data_inputs)\n",
    "            preds = preds.squeeze(dim=0)\n",
    "\n",
    "            labels_in_memory = data_labels.cpu().detach().numpy()\n",
    "            if len(labels_in_memory.shape) == 1:\n",
    "                all_labels.append(labels_in_memory)\n",
    "            else:\n",
    "                for x in labels_in_memory:\n",
    "                    all_labels.append(x)\n",
    "                    \n",
    "            preds_in_memory = preds.cpu().detach().numpy()\n",
    "            if labels_in_memory.shape[0] == 1:\n",
    "                all_predictions.append(preds_in_memory)\n",
    "            else:\n",
    "                for x in preds_in_memory:\n",
    "                    all_predictions.append(x)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    predictions_arr = [1 if x[0]>x[1] else 0 for x in all_predictions] #TODO softmax\n",
    "    targets_arr = [1 if x[0]>x[1] else 0 for x in all_labels]\n",
    "    P = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1])\n",
    "    TP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==1])\n",
    "    FP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==0])\n",
    "    FN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==1])\n",
    "    TN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==0])\n",
    "    N = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0])\n",
    "\n",
    "    precission = TP/(TP+FP) if (TP+FP)!=0 else 0\n",
    "    recall = TP/(TP+FN) if (TP+FN)!=0 else 0\n",
    "    print('Precission:',f'{TP}/{TP+FP}', precission)\n",
    "    print('Recall', f'{TP}/{TP+FN}', recall)\n",
    "    print(f'P:{P},', f'TP:{TP},', f'FP:{FP},', f'FN:{FN},', f'TN:{TN},', f'N:{N}')\n",
    "\n",
    "    return precission, recall\n",
    "\n",
    "\n",
    "def load_files(input_path, data_fraction=1):\n",
    "    positive_json_files = get_files_in_from_directory(input_path, extension='.json', startswith='positive-encodings')\n",
    "    background_json_files = get_files_in_from_directory(input_path, extension='.json', startswith='background-encodings')\n",
    "\n",
    "    if data_fraction < 1:\n",
    "        positive_json_files = random.sample(positive_json_files, int(len(positive_json_files)*data_fraction))\n",
    "        background_json_files = random.sample(background_json_files, int(len(background_json_files)*data_fraction))\n",
    "\n",
    "\n",
    "    repos_set = get_repo_seminames(positive_json_files)\n",
    "    repos_count = len(repos_set)\n",
    "\n",
    "\n",
    "    repos_test = set(random.sample(list(repos_set), int(repos_count*test_percentage)))\n",
    "    repos_set.difference_update(repos_test)\n",
    "    repos_eval = set(random.sample(list(repos_set), int(repos_count*test_percentage)))\n",
    "    repos_set.difference_update(repos_eval)\n",
    "\n",
    "    positive_train = get_files_in_set(positive_json_files, repos_set)\n",
    "    positive_eval = get_files_in_set(positive_json_files, repos_eval)\n",
    "    positive_test = get_files_in_set(positive_json_files, repos_test)\n",
    "\n",
    "    background_train = get_files_in_set(background_json_files, repos_set)\n",
    "    background_eval = get_files_in_set(background_json_files, repos_eval)\n",
    "    background_test = get_files_in_set(background_json_files, repos_test)\n",
    "\n",
    "    return (positive_json_files, background_json_files), (positive_train, background_train), (positive_eval, background_eval), (positive_test, background_test)\n",
    "\n",
    "def divide_chunks(array, n):\n",
    "    for i in range(0, len(array), n):\n",
    "        yield array[i:i + n]\n",
    "\n",
    "def chunks(array, number_of_chunks):\n",
    "    for i in range(0, number_of_chunks):\n",
    "        yield array[i::number_of_chunks]\n",
    "\n",
    "def load_fold_data(input_path, fold_count = 5,  data_fraction=1):\n",
    "    positive_json_files = get_files_in_from_directory(input_path, extension='.json', startswith='positive-encodings')\n",
    "    background_json_files = get_files_in_from_directory(input_path, extension='.json', startswith='background-encodings')\n",
    "\n",
    "    if data_fraction < 1:\n",
    "        positive_json_files = random.sample(positive_json_files, int(len(positive_json_files)*data_fraction))\n",
    "        background_json_files = random.sample(background_json_files, int(len(background_json_files)*data_fraction))\n",
    "\n",
    "\n",
    "    repos_set = get_repo_seminames(positive_json_files)\n",
    "    repos_list = list(repos_set)\n",
    "    random.shuffle(repos_list)\n",
    "\n",
    "    result = []\n",
    "    repos_folded = chunks(repos_list, fold_count)\n",
    "    for fold in repos_folded:\n",
    "        fold_set = set(fold)\n",
    "        fold_positive = get_files_in_set(positive_json_files, fold_set)\n",
    "        fold_background = get_files_in_set(background_json_files, fold_set)\n",
    "        result.append((fold_positive, fold_background))\n",
    "\n",
    "    return result\n",
    "    \n",
    "\n",
    "def load_data(input_data, oversampling_ratio=None, class_ratio=None, sample_limit=None):\n",
    "    positive_files = input_data[0]\n",
    "    background_files = input_data[1]\n",
    "\n",
    "    dataset = SampleLevelRawDataset()\n",
    "    dataset.load_files(positive_files, background_files)\n",
    "\n",
    "    if oversampling_ratio != None and class_ratio != None and sample_limit != None:\n",
    "        dataset.setup_ratios(oversampling_ratio, class_ratio, sample_limit)   \n",
    "\n",
    "    if oversampling_ratio == None and class_ratio == None and sample_limit != None:\n",
    "        dataset.limit_data(sample_limit)   \n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def embed_files(tokenizer, data_files, marker):\n",
    "    with tqdm.tqdm(total=len(data_files)) as pbar:\n",
    "        for data_file in data_files:\n",
    "            with open(data_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            embeddings = []\n",
    "            for data_point in data:\n",
    "                if 'commit_sample' in data_point and \\\n",
    "                    data_point['commit_sample'] != None and \\\n",
    "                    len(data_point['commit_sample']) > 0:\n",
    "\n",
    "                    tensor = torch.Tensor(data_point['commit_sample']).int()\n",
    "                    tensor = tensor[None, :] # Extend to a batch mode\n",
    "                    tensor = tensor.to(device)\n",
    "                    result = tokenizer(tensor)\n",
    "                    labels = result[0][:,0,:]\n",
    "                    labels_in_memory = labels.cpu()\n",
    "                    res = {\n",
    "                        'commit_id': data_point['commit_id'],\n",
    "                        'file_name': data_point['file_name'],\n",
    "                        'is_security_related': data_point['is_security_related'],\n",
    "                        'commit_sample': labels_in_memory.tolist()\n",
    "                    }\n",
    "                    embeddings.append(res)\n",
    "\n",
    "            if len(embeddings) > 0:\n",
    "                file_name = os.path.basename(data_file)\n",
    "                new_file = os.path.join(results_dir, marker + '-embedded-' + file_name)\n",
    "                with open(new_file, 'w') as f:\n",
    "                    json.dump(embeddings, f)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "\n",
    "def map_files_to_new_repo(data_files, marker):\n",
    "    new_data_files = []\n",
    "    for data_file in data_files:\n",
    "        file_name = os.path.basename(data_file)\n",
    "        new_file = os.path.join(results_dir, marker + '-embedded-' + file_name)\n",
    "        if os.path.exists(new_file):\n",
    "            new_data_files.append(new_file)\n",
    "\n",
    "    return new_data_files\n",
    "\n",
    "\n",
    "\n",
    "def save_file_datasets(file_dataset, dataset_type):\n",
    "    data = {\n",
    "        'positive_files': file_dataset[0],\n",
    "        'background_files': file_dataset[1]\n",
    "    }\n",
    "    with open(os.path.join(results_dir, f'{dataset_type}-files.json'), 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "def load_file_dataset(dataset_type):\n",
    "    with open(os.path.join(results_dir, f'{dataset_type}-files.json'), 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return (data['positive_files'], data['background_files'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(train_data, eval_data):\n",
    "    train_dataset = load_data(train_data, oversampling_ratio, class_ratio, sample_limit)\n",
    "    eval_dataset = load_data(eval_data, sample_limit=eval_sample_limit)\n",
    "\n",
    "    # Define model\n",
    "    model = BertAndLinear(base_model)\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "        num_warmup_steps=int(len(train_dataset)*0.25), \n",
    "        num_training_steps=len(train_dataset)*num_epochs_)\n",
    "\n",
    "    # Prep the loaders\n",
    "    train_data_loader = data.DataLoader(train_dataset, batch_size=batch_size_, drop_last=True, shuffle=True)\n",
    "    eval_data_loader = data.DataLoader(eval_dataset, batch_size=batch_size_, drop_last=True, shuffle=True)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, optimizer, train_data_loader, loss_module, scheduler, eval_loader=eval_data_loader)\n",
    "    torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_final.pickle')\n",
    "    return model\n",
    "\n",
    "def embed_files_2(model, files, marker):\n",
    "    # Test the model on test subset\n",
    "    for param in model.codebert.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.codebert.eval()\n",
    "    model.codebert.to(device)\n",
    "\n",
    "    print('Embedding files with transformer from', marker)\n",
    "    embed_files(model.codebert, files, marker)\n",
    "\n",
    "def make_commit_level_datesets(train_data, eval_data, test_data, marker):\n",
    "    train_data_embeded_pos = map_files_to_new_repo(train_data[0], marker)\n",
    "    train_data_embeded_bac = map_files_to_new_repo(train_data[1], marker)\n",
    "\n",
    "    eval_data_embeded_pos = map_files_to_new_repo(eval_data[0], marker)\n",
    "    eval_data_embeded_bac = map_files_to_new_repo(eval_data[1], marker)\n",
    "\n",
    "    test_data_embeded_pos = map_files_to_new_repo(test_data[0], marker)\n",
    "    test_data_embeded_bac = map_files_to_new_repo(test_data[1], marker)\n",
    "\n",
    "\n",
    "    train_dataset_embeded = CommitLevelRawDataset()\n",
    "    train_dataset_embeded.load_files(train_data_embeded_pos, train_data_embeded_bac)\n",
    "    train_dataset_embeded = UnderSampledDataset(train_dataset_embeded, aggregator_class_ratio)\n",
    "    eval_dataset_embeded = CommitLevelRawDataset()\n",
    "    eval_dataset_embeded.load_files(eval_data_embeded_pos, eval_data_embeded_bac)\n",
    "    test_dataset_embeded = CommitLevelRawDataset()\n",
    "    test_dataset_embeded.load_files(test_data_embeded_pos, test_data_embeded_bac)\n",
    "    return train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded\n",
    "\n",
    "def train_aggregator(model, optimizer, data_loader, loss_module, scheduler, test_loader = None):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    all_samples = 0\n",
    "    positive_samples = 0\n",
    "    results = []\n",
    "\n",
    "    for epoch in range(aggregator_num_epochs_):\n",
    "        print(f'Epoch {epoch}/{aggregator_num_epochs_}')\n",
    "        accumulated_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        with tqdm.tqdm(total=len(data_loader)) as pbar:\n",
    "            for data_inputs, data_labels in data_loader:\n",
    "                # Step 0: Diagnostics :x\n",
    "                positive_samples += len([1 for x in data_labels if x[0] == 1])\n",
    "                all_samples += len(data_labels)\n",
    "\n",
    "                #TODO different commit mode and sample mode\n",
    "                data_inputs = torch.stack(data_inputs)\n",
    "                \n",
    "                # Step 1: Mode data to device \n",
    "                data_inputs = data_inputs.to(device)\n",
    "                data_labels = data_labels.to(device) \n",
    "\n",
    "                # Step 2: Calculate model output\n",
    "                preds = model(data_inputs)\n",
    "                \n",
    "                #TODO different commit mode and sample mode\n",
    "                # preds = preds.squeeze(dim=0)\n",
    "\n",
    "                # Step 3: Calculate loss\n",
    "                loss = loss_module(preds, data_labels.float())\n",
    "                accumulated_loss += loss.item()\n",
    "\n",
    "                ## Step 4: Perform backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                ## Step 5: Update the parameters\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Step 6: Progress bar\n",
    "                pbar.update()\n",
    "        print('Loss in this epoch:', accumulated_loss)\n",
    "\n",
    "        if save_model_in_each_epoch:\n",
    "            torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_epoch_{epoch}.pickle')\n",
    "\n",
    "        eval_set_loss, precission, recall = eval_aggregator(model, test_loader, loss_module)\n",
    "        results.append({\n",
    "            'epoch':epoch,\n",
    "            'eval_set_loss':eval_set_loss,\n",
    "            'precission':precission,\n",
    "            'recall':recall,\n",
    "        })\n",
    "\n",
    "\n",
    "    print(f'Model saw positive samples {positive_samples} times and background samples {all_samples-positive_samples}')\n",
    "    print(f'Ratio 1:{(all_samples-positive_samples)/positive_samples}')\n",
    "    return results\n",
    "\n",
    "\n",
    "def eval_aggregator(model, data_loader, loss_module):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    data_size = len(data_loader)\n",
    "    accumulated_loss = 0\n",
    "    with tqdm.tqdm(total=data_size) as pbar:\n",
    "        for data_inputs, data_labels in data_loader:\n",
    "\n",
    "            #TODO different commit mode and sample mode\n",
    "            data_inputs = torch.stack(data_inputs)\n",
    "\n",
    "            data_inputs = data_inputs.to(device)\n",
    "            data_labels = data_labels.to(device)\n",
    "            preds = model(data_inputs)\n",
    "            \n",
    "            loss = loss_module(preds, data_labels.float())\n",
    "            lossxd = float(loss.item())\n",
    "            accumulated_loss += lossxd\n",
    "\n",
    "            #TODO different commit mode and sample mode\n",
    "            # preds = preds.squeeze(dim=0)\n",
    "\n",
    "            labels_in_memory = data_labels.cpu().detach().numpy()\n",
    "            if len(labels_in_memory.shape) == 1:\n",
    "                all_labels.append(labels_in_memory)\n",
    "            else:\n",
    "                for x in labels_in_memory:\n",
    "                    all_labels.append(x)\n",
    "                    \n",
    "            preds_in_memory = preds.cpu().detach().numpy()\n",
    "            if labels_in_memory.shape[0] == 1:\n",
    "                all_predictions.append(preds_in_memory)\n",
    "            else:\n",
    "                for x in preds_in_memory:\n",
    "                    all_predictions.append(x)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    #TODO different commit mode and sample mode\n",
    "    predictions_arr = [1 if x[0,0]>x[0,1] else 0 for x in all_predictions]\n",
    "    targets_arr = [1 if x[0]>x[1] else 0 for x in all_labels]\n",
    "    P = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1])\n",
    "    TP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==1])\n",
    "    FP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==0])\n",
    "    FN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==1])\n",
    "    TN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==0])\n",
    "    N = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0])\n",
    "\n",
    "    precission = TP/(TP+FP) if (TP+FP)!=0 else 0\n",
    "    recall = TP/(TP+FN) if (TP+FN)!=0 else 0\n",
    "    print('Loss:', accumulated_loss)\n",
    "    print('Precission:',f'{TP}/{TP+FP}', precission)\n",
    "    print('Recall', f'{TP}/{TP+FN}', recall)\n",
    "    print(f'P:{P},', f'TP:{TP},', f'FP:{FP},', f'FN:{FN},', f'TN:{TN},', f'N:{N}')\n",
    "\n",
    "    return accumulated_loss, precission, recall\n",
    "\n",
    "\n",
    "def train_and_eval_aggregator(model, train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded):\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=aggregator_learning_rate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "        num_warmup_steps=int(len(train_dataset_embeded)*0.25), \n",
    "        num_training_steps=len(train_dataset_embeded)*num_epochs_)\n",
    "\n",
    "    # Prep the loaders\n",
    "    train_data_embeded_loader = data.DataLoader(train_dataset_embeded, batch_size=1, drop_last=True, shuffle=True)\n",
    "    eval_data_embeded_loader = data.DataLoader(eval_dataset_embeded, batch_size=1, drop_last=True, shuffle=True)\n",
    "\n",
    "    performance_results = train_aggregator(model, optimizer, train_data_embeded_loader, loss_module, scheduler, test_loader=eval_data_embeded_loader)\n",
    "\n",
    "    lowest_loss = 1_000_000\n",
    "    lowest_loss_epoch = 0\n",
    "    for res in performance_results:\n",
    "        if lowest_loss > res['eval_set_loss']:\n",
    "            lowest_loss = res['eval_set_loss']\n",
    "            lowest_loss_epoch = res['epoch']\n",
    "\n",
    "\n",
    "    model_path = f'{work_dir}/model_{model_name}_epoch_{lowest_loss_epoch}.pickle'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    test_data_embeded_loader = data.DataLoader(test_dataset_embeded, drop_last=True, batch_size=1)\n",
    "    accumulated_loss, precission, recall = eval_aggregator(model, test_data_embeded_loader, loss_module)\n",
    "\n",
    "    return accumulated_loss, precission, recall\n",
    "\n",
    "\n",
    "def evaluate_aggregators(train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded):\n",
    "    lstm_results = []\n",
    "    model = LstmAggregator()\n",
    "    lstm_results = train_and_eval_aggregator(model, train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded)\n",
    "    del model\n",
    "\n",
    "    # conv_results = []\n",
    "    model = ConvAggregator()\n",
    "    conv_results = train_and_eval_aggregator(model, train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded)\n",
    "    del model\n",
    "\n",
    "    # mean_results = []\n",
    "    model = MeanAggregator()\n",
    "    mean_results = train_and_eval_aggregator(model, train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded)\n",
    "    del model\n",
    "\n",
    "    return (lstm_results, conv_results, mean_results)\n",
    "\n",
    "def print_summary(resutls, folds):\n",
    "    print('accumulated_loss', sum([x[0] for x in resutls])/folds)\n",
    "    precission = sum([x[1] for x in resutls])/folds\n",
    "    recall = sum([x[2] for x in resutls])/folds\n",
    "    print('precission', precission)\n",
    "    print('recall', recall)\n",
    "    print('f1', 2*precission*recall/(precission+recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_a_fold(i):\n",
    "    data_files = []\n",
    "    for x in range(folds_count):\n",
    "        data_files.append(load_file_dataset(f'fold-{x}'))\n",
    "    \n",
    "    all_files = []\n",
    "    for x in data_files:\n",
    "        all_files += x[0]\n",
    "        all_files += x[1]\n",
    "\n",
    "\n",
    "    with_offset = [(x+i)%folds_count for x in range(folds_count)]\n",
    "    train_data_p = []\n",
    "    train_data_n = []\n",
    "    for x in range(len(data_files)-2):\n",
    "        train_data_p += data_files[with_offset[x]][0]\n",
    "        train_data_n += data_files[with_offset[x]][1]\n",
    "\n",
    "    train_data = [train_data_p, train_data_n]\n",
    "    eval_data = data_files[with_offset[-2]]\n",
    "    test_data = data_files[with_offset[-1]]\n",
    "\n",
    "    print('FOLD', i)\n",
    "    print(len(train_data[0]), len(train_data[1]))\n",
    "    print(len(eval_data[0]), len(eval_data[1]))\n",
    "    print(len(test_data[0]), len(test_data[1]))\n",
    "    print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "    model = fine_tune(train_data, eval_data)\n",
    "    embed_files_2(model, all_files, f'epoch{i}')\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print('FOLD', 'Embedded', i)\n",
    "    print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "    train_dataset, eval_dataset, test_dataset = make_commit_level_datesets(train_data, eval_data, test_data, f'epoch{i}')\n",
    "\n",
    "    lstm_results, conv_results, mean_results = evaluate_aggregators(train_dataset, eval_dataset, test_dataset)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return lstm_results, conv_results, mean_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_stuff():\n",
    "    data_files = load_fold_data(raw_input_path, fold_count=folds_count, data_fraction=fraction_of_data)\n",
    "\n",
    "    for i in range(folds_count):\n",
    "        save_file_datasets(data_files[i], f'fold-{i}')\n",
    "    \n",
    "    lstm_folds_results = []\n",
    "    conv_folds_results = []\n",
    "    mean_folds_results = []\n",
    "\n",
    "    for i in range(folds_count):\n",
    "        lstm_results, conv_results, mean_results = do_a_fold(i)\n",
    "        lstm_folds_results.append(lstm_results)\n",
    "        conv_folds_results.append(conv_results)\n",
    "        mean_folds_results.append(mean_results)\n",
    "\n",
    "\n",
    "    print('LSTM')\n",
    "    print_summary(lstm_folds_results, folds_count)\n",
    "    print('CONV')\n",
    "    print_summary(conv_folds_results, folds_count)\n",
    "    print('MEAN')\n",
    "    print_summary(mean_folds_results, folds_count)\n",
    "    return lstm_folds_results, conv_folds_results, mean_folds_results\n",
    "\n",
    "\n",
    "lstm_folds_results, conv_folds_results, mean_folds_results = do_stuff()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')\n",
    "print('LSTM')\n",
    "print_summary(lstm_folds_results, folds_count)\n",
    "print('CONV')\n",
    "print_summary(conv_folds_results, folds_count)\n",
    "print('MEAN')\n",
    "print_summary(mean_folds_results, folds_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
