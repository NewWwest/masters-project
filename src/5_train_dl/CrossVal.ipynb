{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "import gc\n",
    "gc.collect()\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import uuid\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.insert(0, r'PATH_TO_REPO')\n",
    "# repo_dir = r'PATH_TO_REPO'\n",
    "sys.path.insert(0, r'D:\\Projects\\aaadoc')\n",
    "repo_dir = f'D:\\\\Projects\\\\aaadoc'\n",
    "\n",
    "from src.utils.utils import get_files_in_from_directory\n",
    "from src.dl.dl_utils import get_repo_seminames, get_files_in_set, chunks\n",
    "from src.dl.datasets.BaseRawDataset import BaseRawDataset\n",
    "from src.dl.datasets.SampleLevelRawDataset import SampleLevelRawDataset\n",
    "from src.dl.datasets.CommitLevelRawDataset import CommitLevelRawDataset\n",
    "from src.dl.datasets.sampling.OverSampledDataset import OverSampledDataset\n",
    "from src.dl.datasets.sampling.UnderSampledDataset import UnderSampledDataset\n",
    "\n",
    "\n",
    "from src.dl.models.BertAndLinear import BertAndLinear\n",
    "from src.dl.models.LstmAggregator import LstmAggregator\n",
    "from src.dl.models.ConvAggregator import ConvAggregator\n",
    "from src.dl.models.MeanAggregator import MeanAggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "base_model = 'microsoft/graphcodebert-base'\n",
    "batch_size_ = 2\n",
    "num_epochs_ = 3\n",
    "\n",
    "fraction_of_data = 1\n",
    "\n",
    "sample_limit = 1_000_000\n",
    "eval_sample_limit = 1_000_000\n",
    "# test_percentage = 0.15\n",
    "# eval_percentage = 0.05\n",
    "folds_count = 5\n",
    "current_fold = -1\n",
    "\n",
    "learning_rate = 2e-6\n",
    "oversampling_ratio = None # if None, no ratio controll will be applied\n",
    "class_ratio = 5\n",
    "\n",
    "aggregator_num_epochs_ = 5\n",
    "aggregator_class_ratio = 5\n",
    "aggregator_learning_rate = 2e-4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_in_each_epoch = True\n",
    "eval_model_in_each_epoch = True\n",
    "\n",
    "model_guid = 'debug_run'\n",
    "model_name = model_guid\n",
    "\n",
    "work_dir = f'{repo_dir}\\\\src\\\\5_train_dl\\\\binaries\\\\{model_name}'\n",
    "results_dir = f'{repo_dir}\\\\src\\\\5_train_dl\\\\binaries\\\\data{model_name}'\n",
    "raw_input_path = f'{repo_dir}\\\\src\\\\5_train_dl\\\\binaries\\\\debug_test_folds'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(work_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir(results_dir) \n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, data_loader, loss_module, scheduler, eval_loader = None):\n",
    "    torch.cuda.empty_cache()\n",
    "    global current_fold\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    all_samples = 0\n",
    "    positive_samples = 0\n",
    "\n",
    "    for epoch in range(num_epochs_):\n",
    "        print(f'Epoch {epoch}/{num_epochs_}')\n",
    "        accumulated_loss = 0\n",
    "\n",
    "        with tqdm.tqdm(total=len(data_loader)) as pbar:\n",
    "            for data_inputs_x, data_labels in data_loader:\n",
    "                commit_ids = data_inputs_x[0]\n",
    "                data_inputs = data_inputs_x[1]\n",
    "\n",
    "                # Step 0: Diagnostics :x\n",
    "                positive_samples += len([1 for x in data_labels if x[0] == 1])\n",
    "                all_samples += len(data_labels)\n",
    "                \n",
    "                # Step 1: Mode data to device\n",
    "                data_inputs = data_inputs.to(device)\n",
    "                data_labels = data_labels.to(device)\n",
    "\n",
    "                # Step 2: Calculate model output\n",
    "                preds = model(data_inputs)\n",
    "                preds = preds.squeeze(dim=0)\n",
    "\n",
    "                # Step 3: Calculate loss\n",
    "                loss = loss_module(preds, data_labels.float())\n",
    "                accumulated_loss += loss.item()\n",
    "\n",
    "                ## Step 4: Perform backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                ## Step 5: Update the parameters\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Step 6: Progress bar\n",
    "                pbar.update()\n",
    "        print('Loss in this epoch:', accumulated_loss)\n",
    "\n",
    "        if save_model_in_each_epoch:\n",
    "            torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_fold_{current_fold}_epoch_{epoch}.pickle')\n",
    "\n",
    "        if eval_loader != None:\n",
    "            eval_model(model, eval_loader)\n",
    "\n",
    "\n",
    "    print(f'Model saw positive samples {positive_samples} times and background samples {all_samples-positive_samples}')\n",
    "    print(f'Ratio 1:{(all_samples-positive_samples)/positive_samples}')\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    data_size = len(data_loader)\n",
    "    with tqdm.tqdm(total=data_size) as pbar:\n",
    "        for data_inputs_x, data_labels in data_loader:\n",
    "            commits_ids = data_inputs_x[0]\n",
    "            data_inputs = data_inputs_x[1]\n",
    "\n",
    "            data_inputs = data_inputs.to(device)\n",
    "            data_labels = data_labels.to(device)\n",
    "            preds = model(data_inputs)\n",
    "            preds = preds.squeeze(dim=0)\n",
    "\n",
    "            labels_in_memory = data_labels.cpu().detach().numpy()\n",
    "            if len(labels_in_memory.shape) == 1:\n",
    "                all_labels.append(labels_in_memory)\n",
    "            else:\n",
    "                for x in labels_in_memory:\n",
    "                    all_labels.append(x)\n",
    "                    \n",
    "            preds_in_memory = preds.cpu().detach().numpy()\n",
    "            if labels_in_memory.shape[0] == 1:\n",
    "                all_predictions.append(preds_in_memory)\n",
    "            else:\n",
    "                for x in preds_in_memory:\n",
    "                    all_predictions.append(x)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    predictions_arr = [1 if x[0]>x[1] else 0 for x in all_predictions] #TODO softmax\n",
    "    targets_arr = [1 if x[0]>x[1] else 0 for x in all_labels]\n",
    "    P = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1])\n",
    "    TP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==1])\n",
    "    FP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==0])\n",
    "    FN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==1])\n",
    "    TN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==0])\n",
    "    N = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0])\n",
    "\n",
    "    precission = TP/(TP+FP) if (TP+FP)!=0 else 0\n",
    "    recall = TP/(TP+FN) if (TP+FN)!=0 else 0\n",
    "    print('Precission:',f'{TP}/{TP+FP}', precission)\n",
    "    print('Recall', f'{TP}/{TP+FN}', recall)\n",
    "    print(f'P:{P},', f'TP:{TP},', f'FP:{FP},', f'FN:{FN},', f'TN:{TN},', f'N:{N}')\n",
    "\n",
    "    return precission, recall\n",
    "\n",
    "\n",
    "def load_data(input_data, oversampling_ratio=None, class_ratio=None, sample_limit=None):\n",
    "    positive_files = input_data[0]\n",
    "    background_files = input_data[1]\n",
    "\n",
    "    dataset = SampleLevelRawDataset()\n",
    "    dataset.load_files(positive_files, background_files)\n",
    "\n",
    "    if oversampling_ratio != None and class_ratio != None and sample_limit != None:\n",
    "        dataset.setup_ratios(oversampling_ratio, class_ratio, sample_limit)   \n",
    "\n",
    "    if oversampling_ratio == None and class_ratio == None and sample_limit != None:\n",
    "        dataset.limit_data(sample_limit)   \n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def embed_files(tokenizer, data_files, marker):\n",
    "    with tqdm.tqdm(total=len(data_files)) as pbar:\n",
    "        for data_file in data_files:\n",
    "            with open(data_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            embeddings = []\n",
    "            for data_point in data:\n",
    "                if 'commit_sample' in data_point and \\\n",
    "                    data_point['commit_sample'] != None and \\\n",
    "                    len(data_point['commit_sample']) > 0:\n",
    "\n",
    "                    tensor = torch.Tensor(data_point['commit_sample']).int()\n",
    "                    tensor = tensor[None, :] # Extend to a batch mode\n",
    "                    tensor = tensor.to(device)\n",
    "                    result = tokenizer(tensor)\n",
    "                    labels = result[0][:,0,:]\n",
    "                    labels_in_memory = labels.cpu()\n",
    "                    res = {\n",
    "                        'commit_id': data_point['commit_id'],\n",
    "                        'file_name': data_point['file_name'],\n",
    "                        'is_security_related': data_point['is_security_related'],\n",
    "                        'commit_sample': labels_in_memory.tolist()\n",
    "                    }\n",
    "                    embeddings.append(res)\n",
    "\n",
    "            if len(embeddings) > 0:\n",
    "                file_name = os.path.basename(data_file)\n",
    "                new_file = os.path.join(results_dir, marker + '-embedded-' + file_name)\n",
    "                with open(new_file, 'w') as f:\n",
    "                    json.dump(embeddings, f)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "\n",
    "def map_files_to_new_repo(data_files, marker):\n",
    "    new_data_files = []\n",
    "    for data_file in data_files:\n",
    "        file_name = os.path.basename(data_file)\n",
    "        new_file = os.path.join(results_dir, marker + '-embedded-' + file_name)\n",
    "        if os.path.exists(new_file):\n",
    "            new_data_files.append(new_file)\n",
    "\n",
    "    return new_data_files\n",
    "\n",
    "\n",
    "def load_fold_info(fold_number):\n",
    "    with open(os.path.join(raw_input_path, f'fold-{fold_number}-files.json'), 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    all_positives = data['positive_files']\n",
    "    all_backgrounds = data['background_files']\n",
    "    # all_positives = [os.path.join(repo_dir, x) for x in all_positives]\n",
    "    # all_backgrounds = [os.path.join(repo_dir, x) for x in all_backgrounds]\n",
    "    \n",
    "    positives = random.sample(all_positives, int(fraction_of_data*len(all_positives)))\n",
    "    backgrounds = random.sample(all_backgrounds, int(fraction_of_data*len(all_backgrounds)))\n",
    "    return (positives, backgrounds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(train_dataset, eval_dataset):\n",
    "    global current_fold\n",
    "\n",
    "    # Define model\n",
    "    model = BertAndLinear(base_model)\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "        num_warmup_steps=int(len(train_dataset)*0.25), \n",
    "        num_training_steps=len(train_dataset)*num_epochs_)\n",
    "\n",
    "    # Prep the loaders\n",
    "    train_data_loader = data.DataLoader(train_dataset, batch_size=batch_size_, drop_last=True, shuffle=True)\n",
    "    eval_data_loader = data.DataLoader(eval_dataset, batch_size=batch_size_, drop_last=True, shuffle=True)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, optimizer, train_data_loader, loss_module, scheduler, eval_loader=eval_data_loader)\n",
    "    torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_fold_{current_fold}_final.pickle')\n",
    "    return model\n",
    "\n",
    "\n",
    "def embed_files_with_model(model, files, marker):\n",
    "    # Test the model on test subset\n",
    "    for param in model.codebert.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.codebert.eval()\n",
    "    model.codebert.to(device)\n",
    "\n",
    "    print('Embedding files with transformer from', marker)\n",
    "    embed_files(model.codebert, files, marker)\n",
    "\n",
    "\n",
    "def make_commit_level_datesets(train_data, eval_data, test_data, marker):\n",
    "    train_data_embeded_pos = map_files_to_new_repo(train_data[0], marker)\n",
    "    train_data_embeded_bac = map_files_to_new_repo(train_data[1], marker)\n",
    "\n",
    "    eval_data_embeded_pos = map_files_to_new_repo(eval_data[0], marker)\n",
    "    eval_data_embeded_bac = map_files_to_new_repo(eval_data[1], marker)\n",
    "\n",
    "    test_data_embeded_pos = map_files_to_new_repo(test_data[0], marker)\n",
    "    test_data_embeded_bac = map_files_to_new_repo(test_data[1], marker)\n",
    "\n",
    "\n",
    "    train_dataset_embeded = CommitLevelRawDataset()\n",
    "    train_dataset_embeded.load_files(train_data_embeded_pos, train_data_embeded_bac)\n",
    "    train_dataset_embeded = UnderSampledDataset(train_dataset_embeded, aggregator_class_ratio)\n",
    "    eval_dataset_embeded = CommitLevelRawDataset()\n",
    "    eval_dataset_embeded.load_files(eval_data_embeded_pos, eval_data_embeded_bac)\n",
    "    test_dataset_embeded = CommitLevelRawDataset()\n",
    "    test_dataset_embeded.load_files(test_data_embeded_pos, test_data_embeded_bac)\n",
    "    return train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded\n",
    "\n",
    "\n",
    "def train_aggregator(model, optimizer, data_loader, loss_module, scheduler, test_loader = None):\n",
    "    global current_fold\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    all_samples = 0\n",
    "    positive_samples = 0\n",
    "    results = []\n",
    "\n",
    "    for epoch in range(aggregator_num_epochs_):\n",
    "        print(f'Epoch {epoch}/{aggregator_num_epochs_}')\n",
    "        accumulated_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        with tqdm.tqdm(total=len(data_loader)) as pbar:\n",
    "            for data_inputs_x, data_labels in data_loader:\n",
    "                commits_id = data_inputs_x[0]\n",
    "                data_inputs = data_inputs_x[1]\n",
    "\n",
    "                # Step 0: Diagnostics :x\n",
    "                positive_samples += len([1 for x in data_labels if x[0] == 1])\n",
    "                all_samples += len(data_labels)\n",
    "\n",
    "                #TODO different commit mode and sample mode\n",
    "                data_inputs = torch.stack(data_inputs)\n",
    "                \n",
    "                # Step 1: Mode data to device \n",
    "                data_inputs = data_inputs.to(device)\n",
    "                data_labels = data_labels.to(device) \n",
    "\n",
    "                # Step 2: Calculate model output\n",
    "                preds = model(data_inputs)\n",
    "                \n",
    "                #TODO different commit mode and sample mode\n",
    "                # preds = preds.squeeze(dim=0)\n",
    "\n",
    "                # Step 3: Calculate loss\n",
    "                loss = loss_module(preds, data_labels.float())\n",
    "                accumulated_loss += loss.item()\n",
    "\n",
    "                ## Step 4: Perform backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                ## Step 5: Update the parameters\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Step 6: Progress bar\n",
    "                pbar.update()\n",
    "        print('Loss in this epoch:', accumulated_loss)\n",
    "\n",
    "        if save_model_in_each_epoch:\n",
    "            torch.save(model.state_dict(), f'{work_dir}/model_agg_{model_name}_fold_{current_fold}_epoch_{epoch}.pickle')\n",
    "\n",
    "        eval_set_loss, precission, recall = eval_aggregator(model, test_loader, loss_module)\n",
    "        results.append({\n",
    "            'epoch':epoch,\n",
    "            'eval_set_loss':eval_set_loss,\n",
    "            'precission':precission,\n",
    "            'recall':recall,\n",
    "        })\n",
    "\n",
    "\n",
    "    print(f'Model saw positive samples {positive_samples} times and background samples {all_samples-positive_samples}')\n",
    "    print(f'Ratio 1:{(all_samples-positive_samples)/positive_samples}')\n",
    "    return results\n",
    "\n",
    "\n",
    "def eval_aggregator(model, data_loader, loss_module):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    data_size = len(data_loader)\n",
    "    accumulated_loss = 0\n",
    "    with tqdm.tqdm(total=data_size) as pbar:\n",
    "        for data_inputs_x, data_labels in data_loader:\n",
    "            commits_id = data_inputs_x[0]\n",
    "            data_inputs = data_inputs_x[1]\n",
    "\n",
    "            #TODO different commit mode and sample mode\n",
    "            data_inputs = torch.stack(data_inputs)\n",
    "\n",
    "            data_inputs = data_inputs.to(device)\n",
    "            data_labels = data_labels.to(device)\n",
    "            preds = model(data_inputs)\n",
    "            \n",
    "            loss = loss_module(preds, data_labels.float())\n",
    "            lossxd = float(loss.item())\n",
    "            accumulated_loss += lossxd\n",
    "\n",
    "            #TODO different commit mode and sample mode\n",
    "            # preds = preds.squeeze(dim=0)\n",
    "\n",
    "            labels_in_memory = data_labels.cpu().detach().numpy()\n",
    "            if len(labels_in_memory.shape) == 1:\n",
    "                all_labels.append(labels_in_memory)\n",
    "            else:\n",
    "                for x in labels_in_memory:\n",
    "                    all_labels.append(x)\n",
    "                    \n",
    "            preds_in_memory = preds.cpu().detach().numpy()\n",
    "            if labels_in_memory.shape[0] == 1:\n",
    "                all_predictions.append(preds_in_memory)\n",
    "            else:\n",
    "                for x in preds_in_memory:\n",
    "                    all_predictions.append(x)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    #TODO different commit mode and sample mode\n",
    "    predictions_arr = [1 if x[0,0]>x[0,1] else 0 for x in all_predictions]\n",
    "    targets_arr = [1 if x[0]>x[1] else 0 for x in all_labels]\n",
    "    P = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1])\n",
    "    TP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==1])\n",
    "    FP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==0])\n",
    "    FN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==1])\n",
    "    TN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==0])\n",
    "    N = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0])\n",
    "\n",
    "    precission = TP/(TP+FP) if (TP+FP)!=0 else 0\n",
    "    recall = TP/(TP+FN) if (TP+FN)!=0 else 0\n",
    "    print('Loss:', accumulated_loss)\n",
    "    print('Precission:',f'{TP}/{TP+FP}', precission)\n",
    "    print('Recall', f'{TP}/{TP+FN}', recall)\n",
    "    print(f'P:{P},', f'TP:{TP},', f'FP:{FP},', f'FN:{FN},', f'TN:{TN},', f'N:{N}')\n",
    "\n",
    "    return accumulated_loss, precission, recall\n",
    "\n",
    "\n",
    "def train_and_eval_aggregator(model, train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded):\n",
    "    global current_fold\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=aggregator_learning_rate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "        num_warmup_steps=int(len(train_dataset_embeded)*0.25), \n",
    "        num_training_steps=len(train_dataset_embeded)*num_epochs_)\n",
    "\n",
    "    # Prep the loaders\n",
    "    train_data_embeded_loader = data.DataLoader(train_dataset_embeded, batch_size=1, drop_last=True, shuffle=True)\n",
    "    eval_data_embeded_loader = data.DataLoader(eval_dataset_embeded, batch_size=1, drop_last=True, shuffle=True)\n",
    "\n",
    "    performance_results = train_aggregator(model, optimizer, train_data_embeded_loader, loss_module, scheduler, test_loader=eval_data_embeded_loader)\n",
    "\n",
    "    lowest_loss = 1_000_000\n",
    "    lowest_loss_epoch = 0\n",
    "    for res in performance_results:\n",
    "        if lowest_loss > res['eval_set_loss']:\n",
    "            lowest_loss = res['eval_set_loss']\n",
    "            lowest_loss_epoch = res['epoch']\n",
    "\n",
    "\n",
    "    model_path = f'{work_dir}/model_agg_{model_name}_fold_{current_fold}_epoch_{lowest_loss_epoch}.pickle'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    test_data_embeded_loader = data.DataLoader(test_dataset_embeded, drop_last=True, batch_size=1)\n",
    "    accumulated_loss, precission, recall = eval_aggregator(model, test_data_embeded_loader, loss_module)\n",
    "\n",
    "    return accumulated_loss, precission, recall\n",
    "\n",
    "\n",
    "def evaluate_aggregators(train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded):\n",
    "    lstm_results = []\n",
    "    model = LstmAggregator()\n",
    "    lstm_results = train_and_eval_aggregator(model, train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded)\n",
    "    del model\n",
    "\n",
    "    conv_results = []\n",
    "    model = ConvAggregator()\n",
    "    conv_results = train_and_eval_aggregator(model, train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded)\n",
    "    del model\n",
    "\n",
    "    mean_results = []\n",
    "    model = MeanAggregator()\n",
    "    mean_results = train_and_eval_aggregator(model, train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded)\n",
    "    del model\n",
    "\n",
    "    return (lstm_results, conv_results, mean_results)\n",
    "\n",
    "\n",
    "def temp_f1_score(precission, recall):\n",
    "    return 0 if precission+recall==0  else 2*precission*recall/(precission+recall)\n",
    "\n",
    "\n",
    "def print_summary(resutls, folds):\n",
    "    print('avg_accumulated_loss', sum([x[0] for x in resutls])/folds)\n",
    "    avg_precission = sum([x[1] for x in resutls])/folds\n",
    "    avg_recall = sum([x[2] for x in resutls])/folds\n",
    "    print('avg_precission', avg_precission)\n",
    "    print('avg_recall', avg_recall)\n",
    "    f1_scores = sum([temp_f1_score(x[1], x[2]) for x in resutls])/folds\n",
    "    print('avg_f1', f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_a_fold(i):\n",
    "    global current_fold\n",
    "    current_fold = i\n",
    "    data_files = []\n",
    "    for x in range(folds_count):\n",
    "        data_files.append(load_fold_info(x))\n",
    "    \n",
    "\n",
    "    with_offset = [(x+i)%folds_count for x in range(folds_count)]\n",
    "    train_data_p = []\n",
    "    train_data_n = []\n",
    "    for x in range(len(data_files)-2):\n",
    "        train_data_p += data_files[with_offset[x]][0]\n",
    "        train_data_n += data_files[with_offset[x]][1]\n",
    "\n",
    "    train_data = [train_data_p, train_data_n]\n",
    "    eval_data = data_files[with_offset[-2]]\n",
    "    test_data = data_files[with_offset[-1]]\n",
    "\n",
    "    train_dataset = load_data(train_data, oversampling_ratio, class_ratio, sample_limit)\n",
    "    eval_dataset = load_data(eval_data, sample_limit=eval_sample_limit)\n",
    "\n",
    "    file_to_embed = []\n",
    "    file_to_embed += eval_data[1]\n",
    "    file_to_embed += eval_data[0]\n",
    "    file_to_embed += test_data[1]\n",
    "    file_to_embed += test_data[0]\n",
    "    \n",
    "    all_files = []\n",
    "    for x in data_files:\n",
    "        all_files += x[0]\n",
    "        all_files += x[1]\n",
    "\n",
    "\n",
    "    print('FOLD', i)\n",
    "    print(len(train_data[0]), len(train_data[1]))\n",
    "    print(len(eval_data[0]), len(eval_data[1]))\n",
    "    print(len(test_data[0]), len(test_data[1]))\n",
    "    print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "    model = fine_tune(train_dataset, eval_dataset)\n",
    "    embed_files_with_model(model, all_files, f'fold-{i}')\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fold_finetuning():\n",
    "    for i in range(folds_count):\n",
    "        do_a_fold(i)\n",
    "\n",
    "do_fold_finetuning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_a_fold_aggregator(i):\n",
    "    data_files = []\n",
    "    for x in range(folds_count):\n",
    "        data_files.append(load_fold_info(x))\n",
    "    \n",
    "    all_files = []\n",
    "    for x in data_files:\n",
    "        all_files += x[0]\n",
    "        all_files += x[1]\n",
    "\n",
    "\n",
    "    with_offset = [(x+i)%folds_count for x in range(folds_count)]\n",
    "    train_data_p = []\n",
    "    train_data_n = []\n",
    "    for x in range(len(data_files)-2):\n",
    "        train_data_p += data_files[with_offset[x]][0]\n",
    "        train_data_n += data_files[with_offset[x]][1]\n",
    "\n",
    "    train_data = [train_data_p, train_data_n]\n",
    "    eval_data = data_files[with_offset[-2]]\n",
    "    test_data = data_files[with_offset[-1]]\n",
    "    \n",
    "    print('FOLD', 'Embedded', i)\n",
    "    print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "    train_dataset, eval_dataset, test_dataset = make_commit_level_datesets(train_data, eval_data, test_data, f'fold-{i}')\n",
    "    lstm_results, conv_results, mean_results = evaluate_aggregators(train_dataset, eval_dataset, test_dataset)\n",
    "\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return lstm_results, conv_results, mean_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train aggregators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_folds_aggregators():    \n",
    "    global current_fold\n",
    "    lstm_folds_results = []\n",
    "    conv_folds_results = []\n",
    "    mean_folds_results = []\n",
    "    for i in range(folds_count):\n",
    "        current_fold = i\n",
    "        lstm_results, conv_results, mean_results = do_a_fold_aggregator(i)\n",
    "        lstm_folds_results.append(lstm_results)\n",
    "        conv_folds_results.append(conv_results)\n",
    "        mean_folds_results.append(mean_results)\n",
    "        \n",
    "    return lstm_folds_results, conv_folds_results, mean_folds_results\n",
    "\n",
    "lstm_folds_results, conv_folds_results, mean_folds_results = do_folds_aggregators()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')\n",
    "print('LSTM')\n",
    "print_summary(lstm_folds_results, folds_count)\n",
    "print('CONV')\n",
    "print_summary(conv_folds_results, folds_count)\n",
    "print('MEAN')\n",
    "print_summary(mean_folds_results, folds_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
