{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "import gc\n",
    "gc.collect()\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import uuid\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_from_directory(dir, extension=None, startswith=None):\n",
    "    files_list = []\n",
    "    for root, subdirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            if extension != None and not file.endswith(extension):\n",
    "                continue\n",
    "\n",
    "            if startswith != None and not file.startswith(startswith):\n",
    "                continue\n",
    "            \n",
    "            file_path = os.path.join(root, file)\n",
    "            files_list.append(file_path)\n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_seminames(files):\n",
    "    repos = set()\n",
    "    for x in files:\n",
    "        segments = x.split('-')\n",
    "        repo_semi_full_name = f'{segments[2]}-{segments[3]}'\n",
    "        repos.add(repo_semi_full_name)\n",
    "\n",
    "    return repos\n",
    "\n",
    "\n",
    "def get_files_in_set(filenames, test_repos):\n",
    "    filtered_json_files = []\n",
    "    for x in filenames:\n",
    "        is_test = False\n",
    "        for r in test_repos:\n",
    "            if r in x:\n",
    "                is_test = True\n",
    "                break\n",
    "        if is_test:\n",
    "            filtered_json_files.append(x)\n",
    "\n",
    "    return filtered_json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "import json\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "\n",
    "# ['js', 'jsx', 'ts', 'tsx', ]\n",
    "VALID_EXTENSIONS = set(['java'])\n",
    "\n",
    "class BaseRawDataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.positive_data = []\n",
    "        self.background_data = []\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def _load_file(self, collection, json_file):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def crop_data(self, positive_count, background_count):\n",
    "        self.positive_data = random.sample(self.positive_data, min(positive_count, len(self.positive_data)))\n",
    "        self.background_data = random.sample(self.background_data, min(background_count, len(self.background_data)))\n",
    "\n",
    "\n",
    "    def limit_data(self, limit):\n",
    "        final_positive_count = int(limit*len(self.positive_data)/(len(self.positive_data)+len(self.background_data)))\n",
    "        self.positive_data = random.sample(self.positive_data, min(final_positive_count, len(self.positive_data)))\n",
    "        self.background_data = random.sample(self.background_data, min(limit-final_positive_count, len(self.background_data)))\n",
    "        \n",
    "\n",
    "    def setup_ratios(self, oversampling_ratio, class_ratio, samples_limit):\n",
    "        if oversampling_ratio == -1:\n",
    "            oversampling_ratio = int(len(self.background_data)/(class_ratio*len(self.positive_data)))\n",
    "\n",
    "        positive_count = len(self.positive_data) * oversampling_ratio\n",
    "        self.positive_data = self.positive_data * oversampling_ratio\n",
    "        background_count = class_ratio*len(self.positive_data)\n",
    "        self.background_data = random.sample(self.background_data, min(background_count, len(self.background_data)))\n",
    "\n",
    "        final_positive_count = int(samples_limit*positive_count/(positive_count+background_count))\n",
    "        self.positive_data = random.sample(self.positive_data, min(final_positive_count, len(self.positive_data)))\n",
    "        self.background_data = random.sample(self.background_data, min(samples_limit-final_positive_count, len(self.background_data)))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def split_data(self, fraction):\n",
    "        positive_cut_point = int(fraction*len(self.positive_data))\n",
    "        background_cut_point = int(fraction*len(self.background_data))\n",
    "\n",
    "        random.shuffle(self.positive_data)\n",
    "        random.shuffle(self.background_data)\n",
    "        part_a = BaseRawDataset()\n",
    "        part_a.positive_data = self.positive_data[:positive_cut_point]\n",
    "        part_a.background_data = self.background_data[:background_cut_point]\n",
    "\n",
    "        part_b = BaseRawDataset()\n",
    "        part_b.positive_data = self.positive_data[positive_cut_point:]\n",
    "        part_b.background_data = self.background_data[background_cut_point:]\n",
    "\n",
    "        return part_a, part_b\n",
    "\n",
    "\n",
    "    def load_files(self, positive_json_files, background_json_files):\n",
    "        positive_data_temp = []\n",
    "        background_data_temp = []\n",
    "\n",
    "        for filename in positive_json_files:\n",
    "            try:\n",
    "                with open(filename, 'r') as f:\n",
    "                    temp_data = json.load(f)\n",
    "                    temp_data = [x for x in temp_data if x['file_name'].split('.')[-1] in VALID_EXTENSIONS]\n",
    "                    self._load_file(positive_data_temp, temp_data)\n",
    "            except Exception as e:\n",
    "                print('Failed to load', filename)\n",
    "                print(e)\n",
    "\n",
    "        for filename in background_json_files:\n",
    "            try:\n",
    "                with open(filename, 'r') as f:\n",
    "                    temp_data = json.load(f)\n",
    "                    temp_data = [x for x in temp_data if x['file_name'].split('.')[-1] in VALID_EXTENSIONS]\n",
    "                    self._load_file(background_data_temp, temp_data)\n",
    "            except Exception as e:\n",
    "                print('Failed to load', filename)\n",
    "                print(e)\n",
    "\n",
    "        self.positive_data = positive_data_temp\n",
    "        self.background_data = background_data_temp\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.positive_data) + len(self.background_data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.positive_data):\n",
    "            data_point = self.positive_data[idx]\n",
    "            data_label = torch.Tensor([1, 0]) \n",
    "        else:\n",
    "            data_point = self.background_data[idx - len(self.positive_data)]\n",
    "            data_label = torch.Tensor([0, 1]) \n",
    "        \n",
    "        return data_point, data_label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SampleLevelRawDataset(BaseRawDataset):\n",
    "    def __init__(self):\n",
    "        # super().__init__('positive-encodings', 'background-encodings')\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def _load_file(self, collection, json_file):\n",
    "        data = [x['commit_sample'] for x in json_file\n",
    "            if 'commit_sample' in x and x['commit_sample'] != None and len(x['commit_sample']) > 0]\n",
    "            \n",
    "        if len(data) > 0:\n",
    "            tensors = torch.stack([torch.Tensor(x).int() for x in data])\n",
    "            tensors = torch.unique(tensors, dim=0)\n",
    "            collection += tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class CommitLevelRawDataset(BaseRawDataset):\n",
    "    def __init__(self):\n",
    "        # super().__init__('embedded-positive-encodings', 'embedded-background-encodings')\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def _load_file(self, collection, json_file):\n",
    "        data = [x['commit_sample'] for x in json_file\n",
    "            if 'commit_sample' in x and x['commit_sample'] != None and len(x['commit_sample']) > 0]\n",
    "\n",
    "        if len(data) > 0:  \n",
    "            collection.append([torch.Tensor(x) for x in data])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class OverSampledDataset(data.Dataset):\n",
    "    def __init__(self, base_dataset: BaseRawDataset, ratio):\n",
    "        super().__init__()\n",
    "        self.base_dataset = base_dataset\n",
    "        self.ratio = ratio\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        self._oversample_to_ratio()\n",
    "        \n",
    "\n",
    "    def _oversample_to_ratio(self):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        target_number_of_positive = int(len(self.base_dataset.background_data) / self.ratio)\n",
    "        whole_repetition = floor(target_number_of_positive/len(self.base_dataset.positive_data))\n",
    "        self.data = self.base_dataset.positive_data * whole_repetition\n",
    "        extra_sampled = random.sample(self.base_dataset.positive_data, target_number_of_positive-len(self.data))\n",
    "        self.data += extra_sampled\n",
    "\n",
    "        self.labels = [[1, 0] for _ in self.data] + [[0, 1] for _ in self.base_dataset.background_data]\n",
    "        # self.labels = [[1, -1] for _ in self.data] + [[-1, 1] for _ in background_data]\n",
    "        self.data += self.base_dataset.background_data\n",
    "        \n",
    "\n",
    "        self.labels = torch.Tensor(self.labels)\n",
    "        self.labels = self.labels.int()\n",
    "        print('Data loaded')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.data[idx]\n",
    "        data_label = self.labels[idx]\n",
    "        return data_point, data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class UnderSampledDataset(data.Dataset):\n",
    "    def __init__(self, base_dataset:BaseRawDataset, ratio):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.base_dataset = base_dataset\n",
    "\n",
    "        self._undersample_to_ratio(ratio)\n",
    "        \n",
    "\n",
    "    def _undersample_to_ratio(self, ratio):\n",
    "        target_number_of_background = int(len(self.base_dataset.positive_data) * ratio)\n",
    "        if target_number_of_background > len(self.base_dataset.background_data):\n",
    "            raise Exception(\"Cannot undersample\")\n",
    "\n",
    "        background_sampled = random.sample(self.base_dataset.background_data, target_number_of_background)\n",
    "        self.data = self.base_dataset.positive_data + background_sampled\n",
    "\n",
    "        # self.labels = [torch.Tensor([1, -1]).int() for x in positive_data] + [torch.Tensor([-1, 1]).int() for x in background_sampled]\n",
    "        self.labels = [torch.Tensor([1, 0]).int() for x in self.base_dataset.positive_data] + [torch.Tensor([0, 1]).int() for x in background_sampled]\n",
    "        print('Data loaded')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.data[idx]\n",
    "        data_label = self.labels[idx]\n",
    "        return data_point, data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertAndLinear(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.codebert = AutoModel.from_pretrained(base_model)\n",
    "        self.linear1 = nn.Linear(768, 2)\n",
    "        # self.act_fn = nn.Softmax()\n",
    "        # self.act_fn = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_1 = self.codebert(x)\n",
    "        x_2 = x_1[0]\n",
    "        x_22 = x_2[:,0,:]\n",
    "        x_3 = self.linear1(x_22)\n",
    "        # x_4 = self.act_fn(x_3)\n",
    "        return x_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import bidirectional\n",
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class LstmAggregator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=768,\n",
    "            hidden_size=512,\n",
    "            num_layers = 5,\n",
    "            # bidirectional=True,\n",
    "            dropout=0.2,\n",
    "            batch_first = False)\n",
    "        self.linear1 = nn.Linear(512, 2)\n",
    "        self.act_fn = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        xx = x.squeeze(1)\n",
    "        xx = xx.squeeze(1)\n",
    "        lenx = xx.shape[0]\n",
    "        out = self.lstm(xx.view(lenx, 1, -1))\n",
    "        x_3 = self.linear1(out[0][-1])\n",
    "        return x_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "class ConvAggregator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.max_vector_size = 100\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=(2,8), stride=(1, 4))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(4,8), stride=(2, 4))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(4, 1, kernel_size=4, stride=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=4, stride=2)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.05)\n",
    "        self.head = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_2 = torch.zeros([1, 1, self.max_vector_size, 768])\n",
    "        x_2 = x_2.to(x[0].get_device())\n",
    "        xx = x.movedim(0,-2)\n",
    "        _, _, h, w = xx.shape\n",
    "        if h > self.max_vector_size:\n",
    "            xx = xx[:,:,0:self.max_vector_size,:]\n",
    "            h = self.max_vector_size\n",
    "        x_2[0, 0, 0:h, 0:w] = xx\n",
    "\n",
    "        x_3 = self.conv1(x_2)\n",
    "        x_4 = self.relu1(x_3)\n",
    "        x_5 = self.pool1(x_4)\n",
    "        \n",
    "        x_6 = self.conv2(x_5)\n",
    "        x_7 = self.relu2(x_6)\n",
    "        x_8 = self.pool2(x_7)\n",
    "        x_9 = torch.flatten(x_8, start_dim=1, end_dim=3)\n",
    "\n",
    "        x99= self.dropout(x_9)\n",
    "        x_10 = self.head(x99)\n",
    "        return x_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MeanAggregator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear3 = nn.Sequential(\n",
    "          nn.Linear(768, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        xx = x.squeeze(1)\n",
    "        x_2 = self.linear3(xx)\n",
    "        x_3 = torch.mean(x_2, dim=0)\n",
    "\n",
    "        return x_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "base_model = 'microsoft/graphcodebert-base'\n",
    "batch_size_ = 4\n",
    "num_epochs_ = 3\n",
    "\n",
    "fraction_of_data = 1\n",
    "\n",
    "sample_limit = 1_000_000_000\n",
    "eval_sample_limit = 1_000_000_000\n",
    "# test_percentage = 0.15\n",
    "# eval_percentage = 0.05\n",
    "folds_count=5\n",
    "\n",
    "learning_rate = 2e-5\n",
    "oversampling_ratio = 4\n",
    "class_ratio = 2\n",
    "\n",
    "aggregator_num_epochs_ = 20\n",
    "aggregator_class_ratio = 2\n",
    "aggregator_learning_rate = 5e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_in_each_epoch = True\n",
    "eval_model_in_each_epoch = True\n",
    "\n",
    "model_guid = str(uuid.uuid4())\n",
    "model_name = model_guid\n",
    "\n",
    "work_dir = f'D:\\\\Projects\\\\aaa\\src\\\\rq5\\\\binaries\\\\{model_name}'\n",
    "results_dir = f'D:\\\\Projects\\\\aaa\\src\\\\rq5\\\\binaries\\\\data{model_name}'\n",
    "\n",
    "# Data config - Set to None if you want to use cached datasets\n",
    "raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\CodeParserMiner_ast'\n",
    "# raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\CodeParserMiner_edit'\n",
    "# raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\AddedCodeMiner'\n",
    "# raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\RollingWindowMiner'\n",
    "# raw_input_path = None\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(work_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir(results_dir) \n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24e887d0690>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, data_loader, loss_module, scheduler, eval_loader = None):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    all_samples = 0\n",
    "    positive_samples = 0\n",
    "\n",
    "    for epoch in range(num_epochs_):\n",
    "        print(f'Epoch {epoch}/{num_epochs_}')\n",
    "        accumulated_loss = 0\n",
    "\n",
    "        with tqdm.tqdm(total=len(data_loader)) as pbar:\n",
    "            for data_inputs, data_labels in data_loader:\n",
    "                # Step 0: Diagnostics :x\n",
    "                positive_samples += len([1 for x in data_labels if x[0] == 1])\n",
    "                all_samples += len(data_labels)\n",
    "                \n",
    "                # Step 1: Mode data to device\n",
    "                data_inputs = data_inputs.to(device)\n",
    "                data_labels = data_labels.to(device)\n",
    "\n",
    "                # Step 2: Calculate model output\n",
    "                preds = model(data_inputs)\n",
    "                preds = preds.squeeze(dim=0)\n",
    "\n",
    "                # Step 3: Calculate loss\n",
    "                loss = loss_module(preds, data_labels.float())\n",
    "                accumulated_loss += loss.item()\n",
    "\n",
    "                ## Step 4: Perform backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                ## Step 5: Update the parameters\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Step 6: Progress bar\n",
    "                pbar.update()\n",
    "        print('Loss in this epoch:', accumulated_loss)\n",
    "\n",
    "        if save_model_in_each_epoch:\n",
    "            torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_epoch_{epoch}.pickle')\n",
    "\n",
    "        if eval_loader != None:\n",
    "            eval_model(model, eval_loader)\n",
    "\n",
    "\n",
    "    print(f'Model saw positive samples {positive_samples} times and background samples {all_samples-positive_samples}')\n",
    "    print(f'Ratio 1:{(all_samples-positive_samples)/positive_samples}')\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    data_size = len(data_loader)\n",
    "    with tqdm.tqdm(total=data_size) as pbar:\n",
    "        for data_inputs, data_labels in data_loader:\n",
    "            data_inputs = data_inputs.to(device)\n",
    "            data_labels = data_labels.to(device)\n",
    "            preds = model(data_inputs)\n",
    "            preds = preds.squeeze(dim=0)\n",
    "\n",
    "            labels_in_memory = data_labels.cpu().detach().numpy()\n",
    "            if len(labels_in_memory.shape) == 1:\n",
    "                all_labels.append(labels_in_memory)\n",
    "            else:\n",
    "                for x in labels_in_memory:\n",
    "                    all_labels.append(x)\n",
    "                    \n",
    "            preds_in_memory = preds.cpu().detach().numpy()\n",
    "            if labels_in_memory.shape[0] == 1:\n",
    "                all_predictions.append(preds_in_memory)\n",
    "            else:\n",
    "                for x in preds_in_memory:\n",
    "                    all_predictions.append(x)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    predictions_arr = [1 if x[0]>x[1] else 0 for x in all_predictions] #TODO softmax\n",
    "    targets_arr = [1 if x[0]>x[1] else 0 for x in all_labels]\n",
    "    P = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1])\n",
    "    TP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==1])\n",
    "    FP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==0])\n",
    "    FN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==1])\n",
    "    TN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==0])\n",
    "    N = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0])\n",
    "\n",
    "    precission = TP/(TP+FP) if (TP+FP)!=0 else 0\n",
    "    recall = TP/(TP+FN) if (TP+FN)!=0 else 0\n",
    "    print('Precission:',f'{TP}/{TP+FP}', precission)\n",
    "    print('Recall', f'{TP}/{TP+FN}', recall)\n",
    "    print(f'P:{P},', f'TP:{TP},', f'FP:{FP},', f'FN:{FN},', f'TN:{TN},', f'N:{N}')\n",
    "\n",
    "    return precission, recall\n",
    "\n",
    "\n",
    "# def load_files(input_path, data_fraction=1):\n",
    "#     positive_json_files = get_files_in_from_directory(input_path, extension='.json', startswith='positive-encodings')\n",
    "#     background_json_files = get_files_in_from_directory(input_path, extension='.json', startswith='background-encodings')\n",
    "\n",
    "#     if data_fraction < 1:\n",
    "#         positive_json_files = random.sample(positive_json_files, int(len(positive_json_files)*data_fraction))\n",
    "#         background_json_files = random.sample(background_json_files, int(len(background_json_files)*data_fraction))\n",
    "\n",
    "\n",
    "#     repos_set = get_repo_seminames(positive_json_files)\n",
    "#     repos_count = len(repos_set)\n",
    "\n",
    "\n",
    "#     repos_test = set(random.sample(list(repos_set), int(repos_count*test_percentage)))\n",
    "#     repos_set.difference_update(repos_test)\n",
    "#     repos_eval = set(random.sample(list(repos_set), int(repos_count*test_percentage)))\n",
    "#     repos_set.difference_update(repos_eval)\n",
    "\n",
    "#     positive_train = get_files_in_set(positive_json_files, repos_set)\n",
    "#     positive_eval = get_files_in_set(positive_json_files, repos_eval)\n",
    "#     positive_test = get_files_in_set(positive_json_files, repos_test)\n",
    "\n",
    "#     background_train = get_files_in_set(background_json_files, repos_set)\n",
    "#     background_eval = get_files_in_set(background_json_files, repos_eval)\n",
    "#     background_test = get_files_in_set(background_json_files, repos_test)\n",
    "\n",
    "#     return (positive_json_files, background_json_files), (positive_train, background_train), (positive_eval, background_eval), (positive_test, background_test)\n",
    "\n",
    "def divide_chunks(array, n):\n",
    "    for i in range(0, len(array), n):\n",
    "        yield array[i:i + n]\n",
    "\n",
    "def chunks(array, number_of_chunks):\n",
    "    for i in range(0, number_of_chunks):\n",
    "        yield array[i::number_of_chunks]\n",
    "\n",
    "def load_fold_data(input_path, fold_count = 5,  data_fraction=1):\n",
    "    positive_json_files = get_files_in_from_directory(input_path, extension='.json', startswith='positive-encodings')\n",
    "    background_json_files = get_files_in_from_directory(input_path, extension='.json', startswith='background-encodings')\n",
    "\n",
    "    if data_fraction < 1:\n",
    "        positive_json_files = random.sample(positive_json_files, int(len(positive_json_files)*data_fraction))\n",
    "        background_json_files = random.sample(background_json_files, int(len(background_json_files)*data_fraction))\n",
    "\n",
    "\n",
    "    repos_set = get_repo_seminames(positive_json_files)\n",
    "    repos_list = list(repos_set)\n",
    "    random.shuffle(repos_list)\n",
    "\n",
    "    result = []\n",
    "    repos_folded = chunks(repos_list, fold_count)\n",
    "    for fold in repos_folded:\n",
    "        fold_set = set(fold)\n",
    "        fold_positive = get_files_in_set(positive_json_files, fold_set)\n",
    "        fold_background = get_files_in_set(background_json_files, fold_set)\n",
    "        result.append((fold_positive, fold_background))\n",
    "\n",
    "    return result\n",
    "    \n",
    "\n",
    "def load_data(input_data, oversampling_ratio=None, class_ratio=None, sample_limit=None):\n",
    "    positive_files = input_data[0]\n",
    "    background_files = input_data[1]\n",
    "\n",
    "    dataset = SampleLevelRawDataset()\n",
    "    dataset.load_files(positive_files, background_files)\n",
    "\n",
    "    if oversampling_ratio != None and class_ratio != None and sample_limit != None:\n",
    "        dataset.setup_ratios(oversampling_ratio, class_ratio, sample_limit)   \n",
    "\n",
    "    if oversampling_ratio == None and class_ratio == None and sample_limit != None:\n",
    "        dataset.limit_data(sample_limit)   \n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def embed_files(tokenizer, data_files, marker):\n",
    "    with tqdm.tqdm(total=len(data_files)) as pbar:\n",
    "        for data_file in data_files:\n",
    "            with open(data_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            embeddings = []\n",
    "            for data_point in data:\n",
    "                if 'commit_sample' in data_point and \\\n",
    "                    data_point['commit_sample'] != None and \\\n",
    "                    len(data_point['commit_sample']) > 0:\n",
    "\n",
    "                    tensor = torch.Tensor(data_point['commit_sample']).int()\n",
    "                    tensor = tensor[None, :] # Extend to a batch mode\n",
    "                    tensor = tensor.to(device)\n",
    "                    result = tokenizer(tensor)\n",
    "                    labels = result[0][:,0,:]\n",
    "                    labels_in_memory = labels.cpu()\n",
    "                    res = {\n",
    "                        'commit_id': data_point['commit_id'],\n",
    "                        'file_name': data_point['file_name'],\n",
    "                        'is_security_related': data_point['is_security_related'],\n",
    "                        'commit_sample': labels_in_memory.tolist()\n",
    "                    }\n",
    "                    embeddings.append(res)\n",
    "\n",
    "            if len(embeddings) > 0:\n",
    "                file_name = os.path.basename(data_file)\n",
    "                new_file = os.path.join(results_dir, marker + '-embedded-' + file_name)\n",
    "                with open(new_file, 'w') as f:\n",
    "                    json.dump(embeddings, f)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "\n",
    "def map_files_to_new_repo(data_files, marker):\n",
    "    new_data_files = []\n",
    "    for data_file in data_files:\n",
    "        file_name = os.path.basename(data_file)\n",
    "        new_file = os.path.join(results_dir, marker + '-embedded-' + file_name)\n",
    "        if os.path.exists(new_file):\n",
    "            new_data_files.append(new_file)\n",
    "\n",
    "    return new_data_files\n",
    "\n",
    "\n",
    "\n",
    "def save_file_datasets(file_dataset, dataset_type):\n",
    "    data = {\n",
    "        'positive_files': file_dataset[0],\n",
    "        'background_files': file_dataset[1]\n",
    "    }\n",
    "    with open(os.path.join(results_dir, f'{dataset_type}-files.json'), 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "def load_file_dataset(dataset_type):\n",
    "    with open(os.path.join(results_dir, f'{dataset_type}-files.json'), 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return (data['positive_files'], data['background_files'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(train_data, eval_data):\n",
    "    train_dataset = load_data(train_data, oversampling_ratio, class_ratio, sample_limit)\n",
    "    eval_dataset = load_data(eval_data, sample_limit=eval_sample_limit)\n",
    "\n",
    "    # Define model\n",
    "    model = BertAndLinear(base_model)\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "        num_warmup_steps=int(len(train_dataset)*0.25), \n",
    "        num_training_steps=len(train_dataset)*num_epochs_)\n",
    "\n",
    "    # Prep the loaders\n",
    "    train_data_loader = data.DataLoader(train_dataset, batch_size=batch_size_, drop_last=True, shuffle=True)\n",
    "    eval_data_loader = data.DataLoader(eval_dataset, batch_size=batch_size_, drop_last=True, shuffle=True)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, optimizer, train_data_loader, loss_module, scheduler, eval_loader=eval_data_loader)\n",
    "    torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_final.pickle')\n",
    "    return model\n",
    "\n",
    "def embed_files_2(model, files, marker):\n",
    "    # Test the model on test subset\n",
    "    for param in model.codebert.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.codebert.eval()\n",
    "    model.codebert.to(device)\n",
    "\n",
    "    print('Embedding files with transformer from', marker)\n",
    "    embed_files(model.codebert, files, marker)\n",
    "\n",
    "def make_commit_level_datesets(train_data, eval_data, test_data, marker):\n",
    "    train_data_embeded_pos = map_files_to_new_repo(train_data[0], marker)\n",
    "    train_data_embeded_bac = map_files_to_new_repo(train_data[1], marker)\n",
    "\n",
    "    eval_data_embeded_pos = map_files_to_new_repo(eval_data[0], marker)\n",
    "    eval_data_embeded_bac = map_files_to_new_repo(eval_data[1], marker)\n",
    "\n",
    "    test_data_embeded_pos = map_files_to_new_repo(test_data[0], marker)\n",
    "    test_data_embeded_bac = map_files_to_new_repo(test_data[1], marker)\n",
    "\n",
    "\n",
    "    train_dataset_embeded = CommitLevelRawDataset()\n",
    "    train_dataset_embeded.load_files(train_data_embeded_pos, train_data_embeded_bac)\n",
    "    train_dataset_embeded = UnderSampledDataset(train_dataset_embeded, aggregator_class_ratio)\n",
    "    eval_dataset_embeded = CommitLevelRawDataset()\n",
    "    eval_dataset_embeded.load_files(eval_data_embeded_pos, eval_data_embeded_bac)\n",
    "    test_dataset_embeded = CommitLevelRawDataset()\n",
    "    test_dataset_embeded.load_files(test_data_embeded_pos, test_data_embeded_bac)\n",
    "    return train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded\n",
    "\n",
    "\n",
    "def train_aggregator(model, optimizer, data_loader, loss_module, scheduler, test_loader = None):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    all_samples = 0\n",
    "    positive_samples = 0\n",
    "    results = []\n",
    "\n",
    "    for epoch in range(aggregator_num_epochs_):\n",
    "        print(f'Epoch {epoch}/{aggregator_num_epochs_}')\n",
    "        accumulated_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        with tqdm.tqdm(total=len(data_loader)) as pbar:\n",
    "            for data_inputs, data_labels in data_loader:\n",
    "                # Step 0: Diagnostics :x\n",
    "                positive_samples += len([1 for x in data_labels if x[0] == 1])\n",
    "                all_samples += len(data_labels)\n",
    "\n",
    "                #TODO different commit mode and sample mode\n",
    "                data_inputs = torch.stack(data_inputs)\n",
    "                \n",
    "                # Step 1: Mode data to device \n",
    "                data_inputs = data_inputs.to(device)\n",
    "                data_labels = data_labels.to(device) \n",
    "\n",
    "                # Step 2: Calculate model output\n",
    "                preds = model(data_inputs)\n",
    "                \n",
    "                #TODO different commit mode and sample mode\n",
    "                # preds = preds.squeeze(dim=0)\n",
    "\n",
    "                # Step 3: Calculate loss\n",
    "                loss = loss_module(preds, data_labels.float())\n",
    "                accumulated_loss += loss.item()\n",
    "\n",
    "                ## Step 4: Perform backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                ## Step 5: Update the parameters\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Step 6: Progress bar\n",
    "                pbar.update()\n",
    "        print('Loss in this epoch:', accumulated_loss)\n",
    "\n",
    "        if save_model_in_each_epoch:\n",
    "            torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_epoch_{epoch}.pickle')\n",
    "\n",
    "        eval_set_loss, precission, recall = eval_aggregator(model, test_loader, loss_module)\n",
    "        results.append({\n",
    "            'epoch':epoch,\n",
    "            'eval_set_loss':eval_set_loss,\n",
    "            'precission':precission,\n",
    "            'recall':recall,\n",
    "        })\n",
    "\n",
    "\n",
    "    print(f'Model saw positive samples {positive_samples} times and background samples {all_samples-positive_samples}')\n",
    "    print(f'Ratio 1:{(all_samples-positive_samples)/positive_samples}')\n",
    "    return results\n",
    "\n",
    "\n",
    "def eval_aggregator(model, data_loader, loss_module):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    data_size = len(data_loader)\n",
    "    accumulated_loss = 0\n",
    "    with tqdm.tqdm(total=data_size) as pbar:\n",
    "        for data_inputs, data_labels in data_loader:\n",
    "\n",
    "            #TODO different commit mode and sample mode\n",
    "            data_inputs = torch.stack(data_inputs)\n",
    "\n",
    "            data_inputs = data_inputs.to(device)\n",
    "            data_labels = data_labels.to(device)\n",
    "            preds = model(data_inputs)\n",
    "            \n",
    "            loss = loss_module(preds, data_labels.float())\n",
    "            lossxd = float(loss.item())\n",
    "            accumulated_loss += lossxd\n",
    "\n",
    "            #TODO different commit mode and sample mode\n",
    "            # preds = preds.squeeze(dim=0)\n",
    "\n",
    "            labels_in_memory = data_labels.cpu().detach().numpy()\n",
    "            if len(labels_in_memory.shape) == 1:\n",
    "                all_labels.append(labels_in_memory)\n",
    "            else:\n",
    "                for x in labels_in_memory:\n",
    "                    all_labels.append(x)\n",
    "                    \n",
    "            preds_in_memory = preds.cpu().detach().numpy()\n",
    "            if labels_in_memory.shape[0] == 1:\n",
    "                all_predictions.append(preds_in_memory)\n",
    "            else:\n",
    "                for x in preds_in_memory:\n",
    "                    all_predictions.append(x)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    #TODO different commit mode and sample mode\n",
    "    predictions_arr = [1 if x[0,0]>x[0,1] else 0 for x in all_predictions]\n",
    "    targets_arr = [1 if x[0]>x[1] else 0 for x in all_labels]\n",
    "    P = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1])\n",
    "    TP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==1])\n",
    "    FP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==0])\n",
    "    FN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==1])\n",
    "    TN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==0])\n",
    "    N = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0])\n",
    "\n",
    "    precission = TP/(TP+FP) if (TP+FP)!=0 else 0\n",
    "    recall = TP/(TP+FN) if (TP+FN)!=0 else 0\n",
    "    print('Loss:', accumulated_loss)\n",
    "    print('Precission:',f'{TP}/{TP+FP}', precission)\n",
    "    print('Recall', f'{TP}/{TP+FN}', recall)\n",
    "    print(f'P:{P},', f'TP:{TP},', f'FP:{FP},', f'FN:{FN},', f'TN:{TN},', f'N:{N}')\n",
    "\n",
    "    return accumulated_loss, precission, recall\n",
    "\n",
    "\n",
    "def train_and_eval_aggregator(model, train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded):\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=aggregator_learning_rate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "        num_warmup_steps=int(len(train_dataset_embeded)*0.25), \n",
    "        num_training_steps=len(train_dataset_embeded)*num_epochs_)\n",
    "\n",
    "    # Prep the loaders\n",
    "    train_data_embeded_loader = data.DataLoader(train_dataset_embeded, batch_size=1, drop_last=True, shuffle=True)\n",
    "    eval_data_embeded_loader = data.DataLoader(eval_dataset_embeded, batch_size=1, drop_last=True, shuffle=True)\n",
    "\n",
    "    performance_results = train_aggregator(model, optimizer, train_data_embeded_loader, loss_module, scheduler, test_loader=eval_data_embeded_loader)\n",
    "\n",
    "    lowest_loss = 1_000_000\n",
    "    lowest_loss_epoch = 0\n",
    "    for res in performance_results:\n",
    "        if lowest_loss > res['eval_set_loss']:\n",
    "            lowest_loss = res['eval_set_loss']\n",
    "            lowest_loss_epoch = res['epoch']\n",
    "\n",
    "\n",
    "    model_path = f'{work_dir}/model_{model_name}_epoch_{lowest_loss_epoch}.pickle'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    test_data_embeded_loader = data.DataLoader(test_dataset_embeded, drop_last=True, batch_size=1)\n",
    "    accumulated_loss, precission, recall = eval_aggregator(model, test_data_embeded_loader, loss_module)\n",
    "\n",
    "    return accumulated_loss, precission, recall\n",
    "\n",
    "\n",
    "def evaluate_aggregators(train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded):\n",
    "    lstm_results = []\n",
    "    model = LstmAggregator()\n",
    "    lstm_results = train_and_eval_aggregator(model, train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded)\n",
    "    del model\n",
    "\n",
    "    # conv_results = []\n",
    "    model = ConvAggregator()\n",
    "    conv_results = train_and_eval_aggregator(model, train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded)\n",
    "    del model\n",
    "\n",
    "    # mean_results = []\n",
    "    model = MeanAggregator()\n",
    "    mean_results = train_and_eval_aggregator(model, train_dataset_embeded, eval_dataset_embeded, test_dataset_embeded)\n",
    "    del model\n",
    "\n",
    "    return (lstm_results, conv_results, mean_results)\n",
    "\n",
    "def print_summary(resutls, folds):\n",
    "    print('accumulated_loss', sum([x[0] for x in resutls])/folds)\n",
    "    precission = sum([x[1] for x in resutls])/folds\n",
    "    recall = sum([x[2] for x in resutls])/folds\n",
    "    print('precission', precission)\n",
    "    print('recall', recall)\n",
    "    print('f1', 2*precission*recall/(precission+recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_a_fold(i):\n",
    "    data_files = []\n",
    "    for x in range(folds_count):\n",
    "        data_files.append(load_file_dataset(f'fold-{x}'))\n",
    "    \n",
    "    all_files = []\n",
    "    for x in data_files:\n",
    "        all_files += x[0]\n",
    "        all_files += x[1]\n",
    "\n",
    "\n",
    "    with_offset = [(x+i)%folds_count for x in range(folds_count)]\n",
    "    train_data_p = []\n",
    "    train_data_n = []\n",
    "    for x in range(len(data_files)-2):\n",
    "        train_data_p += data_files[with_offset[x]][0]\n",
    "        train_data_n += data_files[with_offset[x]][1]\n",
    "\n",
    "    train_data = [train_data_p, train_data_n]\n",
    "    eval_data = data_files[with_offset[-2]]\n",
    "    test_data = data_files[with_offset[-1]]\n",
    "\n",
    "    print('FOLD', i)\n",
    "    print(len(train_data[0]), len(train_data[1]))\n",
    "    print(len(eval_data[0]), len(eval_data[1]))\n",
    "    print(len(test_data[0]), len(test_data[1]))\n",
    "    print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "    model = fine_tune(train_data, eval_data)\n",
    "    embed_files_2(model, all_files, f'fold-{i}')\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "179 8197\n",
      "49 2104\n",
      "33 1258\n",
      "torch.cuda.memory_allocated: 0.000000GB\n",
      "torch.cuda.memory_reserved: 0.000000GB\n",
      "torch.cuda.max_memory_reserved: 0.000000GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14571/14571 [1:23:55<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 1651.9805237702203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13543/13543 [22:47<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 236/854 0.27634660421545665\n",
      "Recall 236/1327 0.17784476262245666\n",
      "P:854, TP:236, FP:618, FN:1091, TN:52227, N:53318\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14571/14571 [1:18:49<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 136.5450792371057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13543/13543 [22:43<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 117/620 0.18870967741935485\n",
      "Recall 117/1327 0.08816880180859081\n",
      "P:620, TP:117, FP:503, FN:1210, TN:52342, N:53552\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14571/14571 [1:18:33<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 88.41195790138045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13543/13543 [22:53<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 79/253 0.31225296442687744\n",
      "Recall 79/1327 0.059532780708364735\n",
      "P:253, TP:79, FP:174, FN:1248, TN:52671, N:53919\n",
      "Model saw positive samples 58284 times and background samples 116568\n",
      "Ratio 1:2.0\n",
      "Embedding files with transformer from fold-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11820/11820 [8:13:15<00:00,  2.50s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "176 8000\n",
      "33 1258\n",
      "52 2301\n",
      "torch.cuda.memory_allocated: 0.000000GB\n",
      "torch.cuda.memory_reserved: 0.000000GB\n",
      "torch.cuda.max_memory_reserved: 5.794922GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14061/14061 [1:18:12<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 1929.526531862517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8212/8212 [13:29<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 165/390 0.4230769230769231\n",
      "Recall 165/791 0.20859671302149177\n",
      "P:390, TP:165, FP:225, FN:626, TN:31832, N:32458\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14061/14061 [1:15:25<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 203.70128585532984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8212/8212 [13:29<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 164/465 0.35268817204301073\n",
      "Recall 164/791 0.20733249051833122\n",
      "P:465, TP:164, FP:301, FN:627, TN:31756, N:32383\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14061/14061 [1:16:57<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 75.02209035083433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8212/8212 [13:42<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 164/372 0.44086021505376344\n",
      "Recall 164/791 0.20733249051833122\n",
      "P:372, TP:164, FP:208, FN:627, TN:31849, N:32476\n",
      "Model saw positive samples 56244 times and background samples 112488\n",
      "Ratio 1:2.0\n",
      "Embedding files with transformer from fold-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11820/11820 [8:11:57<00:00,  2.50s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 2\n",
      "144 5433\n",
      "52 2301\n",
      "65 3825\n",
      "torch.cuda.memory_allocated: 0.000000GB\n",
      "torch.cuda.memory_reserved: 0.000000GB\n",
      "torch.cuda.max_memory_reserved: 5.794922GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10371/10371 [57:41<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 1367.9037976319814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17580/17580 [28:53<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 183/1191 0.15365239294710328\n",
      "Recall 183/1497 0.12224448897795591\n",
      "P:1191, TP:183, FP:1008, FN:1314, TN:67815, N:69129\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10371/10371 [55:37<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 93.26066483600425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17580/17580 [28:53<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 493/5986 0.08235883728700301\n",
      "Recall 493/1497 0.3293253173012692\n",
      "P:5986, TP:493, FP:5493, FN:1004, TN:63330, N:64334\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10371/10371 [56:35<00:00,  3.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 63.76827558885179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17580/17580 [28:51<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 158/1563 0.10108765195137556\n",
      "Recall 158/1497 0.10554442217768871\n",
      "P:1563, TP:158, FP:1405, FN:1339, TN:67418, N:68757\n",
      "Model saw positive samples 41484 times and background samples 82968\n",
      "Ratio 1:2.0\n",
      "Embedding files with transformer from fold-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11820/11820 [8:12:10<00:00,  2.50s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 3\n",
      "134 5663\n",
      "65 3825\n",
      "62 2071\n",
      "torch.cuda.memory_allocated: 0.000000GB\n",
      "torch.cuda.memory_reserved: 0.000000GB\n",
      "torch.cuda.max_memory_reserved: 5.794922GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10845/10845 [1:00:16<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 1317.9571882204182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23527/23527 [38:39<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 549/820 0.6695121951219513\n",
      "Recall 549/2021 0.27164769915883225\n",
      "P:820, TP:549, FP:271, FN:1472, TN:91816, N:93288\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10845/10845 [58:43<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 118.2757373971026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23527/23527 [38:37<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 344/640 0.5375\n",
      "Recall 344/2021 0.1702127659574468\n",
      "P:640, TP:344, FP:296, FN:1677, TN:91791, N:93468\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10845/10845 [58:33<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 65.51858709466569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23527/23527 [38:37<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 526/1205 0.43651452282157677\n",
      "Recall 526/2021 0.260267194458189\n",
      "P:1205, TP:526, FP:679, FN:1495, TN:91408, N:92903\n",
      "Model saw positive samples 43380 times and background samples 86760\n",
      "Ratio 1:2.0\n",
      "Embedding files with transformer from fold-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11820/11820 [8:14:51<00:00,  2.51s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 4\n",
      "150 7384\n",
      "62 2071\n",
      "49 2104\n",
      "torch.cuda.memory_allocated: 0.000000GB\n",
      "torch.cuda.memory_reserved: 0.000000GB\n",
      "torch.cuda.max_memory_reserved: 5.794922GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12927/12927 [1:12:45<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 1419.397973633917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10856/10856 [17:49<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 76/335 0.22686567164179106\n",
      "Recall 76/1339 0.05675877520537715\n",
      "P:335, TP:76, FP:259, FN:1263, TN:41826, N:43089\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12927/12927 [1:09:57<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 130.918083845394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10856/10856 [19:16<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 8/278 0.02877697841726619\n",
      "Recall 8/1339 0.00597460791635549\n",
      "P:278, TP:8, FP:270, FN:1331, TN:41815, N:43146\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12927/12927 [1:09:20<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 84.39886860628212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10856/10856 [17:50<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precission: 7/103 0.06796116504854369\n",
      "Recall 7/1339 0.005227781926811053\n",
      "P:103, TP:7, FP:96, FN:1332, TN:41989, N:43321\n",
      "Model saw positive samples 51708 times and background samples 103416\n",
      "Ratio 1:2.0\n",
      "Embedding files with transformer from fold-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11820/11820 [8:14:11<00:00,  2.51s/it]  \n"
     ]
    }
   ],
   "source": [
    "def do_stuff():\n",
    "    data_files = load_fold_data(raw_input_path, fold_count=folds_count, data_fraction=fraction_of_data)\n",
    "\n",
    "    for i in range(folds_count):\n",
    "        save_file_datasets(data_files[i], f'fold-{i}')\n",
    "    \n",
    "    for i in range(folds_count):\n",
    "        do_a_fold(i)\n",
    "\n",
    "do_stuff()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator_num_epochs_ = 5\n",
    "aggregator_class_ratio = 4\n",
    "aggregator_learning_rate = 2e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_a_fold_aggregator(i):\n",
    "    data_files = []\n",
    "    for x in range(folds_count):\n",
    "        data_files.append(load_file_dataset(f'fold-{x}'))\n",
    "    \n",
    "    all_files = []\n",
    "    for x in data_files:\n",
    "        all_files += x[0]\n",
    "        all_files += x[1]\n",
    "\n",
    "\n",
    "    with_offset = [(x+i)%folds_count for x in range(folds_count)]\n",
    "    train_data_p = []\n",
    "    train_data_n = []\n",
    "    for x in range(len(data_files)-2):\n",
    "        train_data_p += data_files[with_offset[x]][0]\n",
    "        train_data_n += data_files[with_offset[x]][1]\n",
    "\n",
    "    train_data = [train_data_p, train_data_n]\n",
    "    eval_data = data_files[with_offset[-2]]\n",
    "    test_data = data_files[with_offset[-1]]\n",
    "    \n",
    "    print('FOLD', 'Embedded', i)\n",
    "    print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "    train_dataset, eval_dataset, test_dataset = make_commit_level_datesets(train_data, eval_data, test_data, f'fold-{i}')\n",
    "    lstm_results, conv_results, mean_results = evaluate_aggregators(train_dataset, eval_dataset, test_dataset)\n",
    "\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return lstm_results, conv_results, mean_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD Embedded 0\n",
      "torch.cuda.memory_allocated: 0.166975GB\n",
      "torch.cuda.memory_reserved: 0.277344GB\n",
      "torch.cuda.max_memory_reserved: 5.794922GB\n",
      "Data loaded\n",
      "Epoch 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [01:12<00:00, 11.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 328.99767933180556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:47<00:00, 40.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 265.1001003666315\n",
      "Precission: 5/13 0.38461538461538464\n",
      "Recall 5/46 0.10869565217391304\n",
      "P:13, TP:5, FP:8, FN:41, TN:1838, N:1879\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [01:11<00:00, 11.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 70.458182174596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:46<00:00, 40.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 310.6324025992071\n",
      "Precission: 5/14 0.35714285714285715\n",
      "Recall 5/46 0.10869565217391304\n",
      "P:14, TP:5, FP:9, FN:41, TN:1837, N:1878\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [01:11<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 45.97915441222722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:47<00:00, 40.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 314.19830388430273\n",
      "Precission: 5/14 0.35714285714285715\n",
      "Recall 5/46 0.10869565217391304\n",
      "P:14, TP:5, FP:9, FN:41, TN:1837, N:1878\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [01:12<00:00, 11.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 42.611475533398334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:46<00:00, 40.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 314.19830388430273\n",
      "Precission: 5/14 0.35714285714285715\n",
      "Recall 5/46 0.10869565217391304\n",
      "P:14, TP:5, FP:9, FN:41, TN:1837, N:1878\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [01:12<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 42.65222766296938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:47<00:00, 40.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 314.19830388430273\n",
      "Precission: 5/14 0.35714285714285715\n",
      "Recall 5/46 0.10869565217391304\n",
      "P:14, TP:5, FP:9, FN:41, TN:1837, N:1878\n",
      "Model saw positive samples 820 times and background samples 3280\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:28<00:00, 40.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 159.12247125105932\n",
      "Precission: 3/9 0.3333333333333333\n",
      "Recall 3/29 0.10344827586206896\n",
      "P:9, TP:3, FP:6, FN:26, TN:1116, N:1142\n",
      "Epoch 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [00:05<00:00, 158.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 606.2839582562447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:04<00:00, 409.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1408.2351824045181\n",
      "Precission: 45/1785 0.025210084033613446\n",
      "Recall 45/46 0.9782608695652174\n",
      "P:1785, TP:45, FP:1740, FN:1, TN:106, N:107\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [00:02<00:00, 291.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 597.676876783371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:04<00:00, 405.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1385.8239375948906\n",
      "Precission: 41/1714 0.023920653442240373\n",
      "Recall 41/46 0.8913043478260869\n",
      "P:1714, TP:41, FP:1673, FN:5, TN:173, N:178\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [00:02<00:00, 289.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 592.5018702745438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:04<00:00, 410.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1377.6803340315819\n",
      "Precission: 40/1672 0.023923444976076555\n",
      "Recall 40/46 0.8695652173913043\n",
      "P:1672, TP:40, FP:1632, FN:6, TN:214, N:220\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [00:02<00:00, 289.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 591.5508860945702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:04<00:00, 409.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1377.6803340315819\n",
      "Precission: 40/1672 0.023923444976076555\n",
      "Recall 40/46 0.8695652173913043\n",
      "P:1672, TP:40, FP:1632, FN:6, TN:214, N:220\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [00:02<00:00, 291.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 592.4082746505737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:04<00:00, 407.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1377.6803340315819\n",
      "Precission: 40/1672 0.023923444976076555\n",
      "Recall 40/46 0.8695652173913043\n",
      "P:1672, TP:40, FP:1632, FN:6, TN:214, N:220\n",
      "Model saw positive samples 820 times and background samples 3280\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:02<00:00, 409.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 836.5698900222778\n",
      "Precission: 23/972 0.023662551440329218\n",
      "Recall 23/29 0.7931034482758621\n",
      "P:972, TP:23, FP:949, FN:6, TN:173, N:179\n",
      "Epoch 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [00:01<00:00, 538.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 439.18605077266693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:02<00:00, 714.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 693.2690169513226\n",
      "Precission: 4/11 0.36363636363636365\n",
      "Recall 4/46 0.08695652173913043\n",
      "P:11, TP:4, FP:7, FN:42, TN:1839, N:1881\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [00:01<00:00, 534.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 254.71959340572357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:02<00:00, 710.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 489.9174295067787\n",
      "Precission: 4/12 0.3333333333333333\n",
      "Recall 4/46 0.08695652173913043\n",
      "P:12, TP:4, FP:8, FN:42, TN:1838, N:1880\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [00:01<00:00, 535.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 196.1291035413742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:02<00:00, 718.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 440.6922703385353\n",
      "Precission: 4/12 0.3333333333333333\n",
      "Recall 4/46 0.08695652173913043\n",
      "P:12, TP:4, FP:8, FN:42, TN:1838, N:1880\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [00:01<00:00, 533.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 187.63469678163528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:02<00:00, 717.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 440.6922703385353\n",
      "Precission: 4/12 0.3333333333333333\n",
      "Recall 4/46 0.08695652173913043\n",
      "P:12, TP:4, FP:8, FN:42, TN:1838, N:1880\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [00:01<00:00, 531.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 187.63469678163528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:02<00:00, 722.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 440.6922703385353\n",
      "Precission: 4/12 0.3333333333333333\n",
      "Recall 4/46 0.08695652173913043\n",
      "P:12, TP:4, FP:8, FN:42, TN:1838, N:1880\n",
      "Model saw positive samples 820 times and background samples 3280\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:01<00:00, 726.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 274.39929743111134\n",
      "Precission: 2/7 0.2857142857142857\n",
      "Recall 2/29 0.06896551724137931\n",
      "P:7, TP:2, FP:5, FN:27, TN:1117, N:1144\n",
      "FOLD Embedded 1\n",
      "torch.cuda.memory_allocated: 0.166975GB\n",
      "torch.cuda.memory_reserved: 0.232422GB\n",
      "torch.cuda.max_memory_reserved: 5.794922GB\n",
      "Data loaded\n",
      "Epoch 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [01:10<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 332.7820421559736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:28<00:00, 40.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 150.36817732313648\n",
      "Precission: 5/11 0.45454545454545453\n",
      "Recall 5/29 0.1724137931034483\n",
      "P:11, TP:5, FP:6, FN:24, TN:1116, N:1140\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [01:10<00:00, 11.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 69.43901983671822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:28<00:00, 40.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 177.18538286560215\n",
      "Precission: 5/12 0.4166666666666667\n",
      "Recall 5/29 0.1724137931034483\n",
      "P:12, TP:5, FP:7, FN:24, TN:1115, N:1139\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [01:10<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 43.74544923368376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:28<00:00, 40.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 180.59673585160635\n",
      "Precission: 5/12 0.4166666666666667\n",
      "Recall 5/29 0.1724137931034483\n",
      "P:12, TP:5, FP:7, FN:24, TN:1115, N:1139\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [01:12<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 40.19272527343128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:29<00:00, 38.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 180.59673585160635\n",
      "Precission: 5/12 0.4166666666666667\n",
      "Recall 5/29 0.1724137931034483\n",
      "P:12, TP:5, FP:7, FN:24, TN:1115, N:1139\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [01:10<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 40.13725080783479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:28<00:00, 40.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 180.59673585160635\n",
      "Precission: 5/12 0.4166666666666667\n",
      "Recall 5/29 0.1724137931034483\n",
      "P:12, TP:5, FP:7, FN:24, TN:1115, N:1139\n",
      "Model saw positive samples 810 times and background samples 3240\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:55<00:00, 38.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 266.57415987947024\n",
      "Precission: 3/11 0.2727272727272727\n",
      "Recall 3/48 0.0625\n",
      "P:11, TP:3, FP:8, FN:45, TN:2081, N:2126\n",
      "Epoch 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:02<00:00, 358.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 560.796325802803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:02<00:00, 486.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 795.8119873404503\n",
      "Precission: 2/135 0.014814814814814815\n",
      "Recall 2/29 0.06896551724137931\n",
      "P:135, TP:2, FP:133, FN:27, TN:989, N:1016\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:02<00:00, 283.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 560.406339764595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:02<00:00, 391.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 795.2094148993492\n",
      "Precission: 2/126 0.015873015873015872\n",
      "Recall 2/29 0.06896551724137931\n",
      "P:126, TP:2, FP:124, FN:27, TN:998, N:1025\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:02<00:00, 280.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 560.2004117965698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:02<00:00, 389.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 795.0000729560852\n",
      "Precission: 1/123 0.008130081300813009\n",
      "Recall 1/29 0.034482758620689655\n",
      "P:123, TP:1, FP:122, FN:28, TN:1000, N:1028\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:02<00:00, 280.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 560.2421205639839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:02<00:00, 392.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 795.0000729560852\n",
      "Precission: 1/123 0.008130081300813009\n",
      "Recall 1/29 0.034482758620689655\n",
      "P:123, TP:1, FP:122, FN:28, TN:1000, N:1028\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:02<00:00, 277.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 560.1917116641998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:03<00:00, 381.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 795.0000729560852\n",
      "Precission: 1/123 0.008130081300813009\n",
      "Recall 1/29 0.034482758620689655\n",
      "P:123, TP:1, FP:122, FN:28, TN:1000, N:1028\n",
      "Model saw positive samples 810 times and background samples 3240\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:05<00:00, 377.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1476.2979181408882\n",
      "Precission: 4/202 0.019801980198019802\n",
      "Recall 4/48 0.08333333333333333\n",
      "P:202, TP:4, FP:198, FN:44, TN:1891, N:1935\n",
      "Epoch 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:01<00:00, 511.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 649.3458800017834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:01<00:00, 684.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 677.1605713963509\n",
      "Precission: 11/97 0.1134020618556701\n",
      "Recall 11/29 0.3793103448275862\n",
      "P:97, TP:11, FP:86, FN:18, TN:1036, N:1054\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:01<00:00, 514.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 383.1195678412914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:01<00:00, 682.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 471.2106402814388\n",
      "Precission: 7/22 0.3181818181818182\n",
      "Recall 7/29 0.2413793103448276\n",
      "P:22, TP:7, FP:15, FN:22, TN:1107, N:1129\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:01<00:00, 514.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 293.179741576314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:01<00:00, 690.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 418.2107851803303\n",
      "Precission: 7/16 0.4375\n",
      "Recall 7/29 0.2413793103448276\n",
      "P:16, TP:7, FP:9, FN:22, TN:1113, N:1135\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:01<00:00, 514.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 279.97515338659286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:01<00:00, 683.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 418.2107851803303\n",
      "Precission: 7/16 0.4375\n",
      "Recall 7/29 0.2413793103448276\n",
      "P:16, TP:7, FP:9, FN:22, TN:1113, N:1135\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 810/810 [00:01<00:00, 515.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 279.97515338659286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1151/1151 [00:01<00:00, 686.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 418.2107851803303\n",
      "Precission: 7/16 0.4375\n",
      "Recall 7/29 0.2413793103448276\n",
      "P:16, TP:7, FP:9, FN:22, TN:1113, N:1135\n",
      "Model saw positive samples 810 times and background samples 3240\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:03<00:00, 667.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 772.9525383114815\n",
      "Precission: 7/34 0.20588235294117646\n",
      "Recall 7/48 0.14583333333333334\n",
      "P:34, TP:7, FP:27, FN:41, TN:2062, N:2103\n",
      "FOLD Embedded 2\n",
      "torch.cuda.memory_allocated: 0.166975GB\n",
      "torch.cuda.memory_reserved: 0.232422GB\n",
      "torch.cuda.max_memory_reserved: 5.794922GB\n",
      "Data loaded\n",
      "Epoch 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:53<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 325.8346008211374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:52<00:00, 40.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 263.98020835686475\n",
      "Precission: 2/26 0.07692307692307693\n",
      "Recall 2/48 0.041666666666666664\n",
      "P:26, TP:2, FP:24, FN:46, TN:2065, N:2111\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:53<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 70.30144167272374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:52<00:00, 40.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 334.01875845342875\n",
      "Precission: 3/37 0.08108108108108109\n",
      "Recall 3/48 0.0625\n",
      "P:37, TP:3, FP:34, FN:45, TN:2055, N:2100\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:53<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 53.09861161222216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:52<00:00, 40.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 349.31609358184505\n",
      "Precission: 3/38 0.07894736842105263\n",
      "Recall 3/48 0.0625\n",
      "P:38, TP:3, FP:35, FN:45, TN:2054, N:2099\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:53<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 50.43944866582751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:52<00:00, 40.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 349.31609358184505\n",
      "Precission: 3/38 0.07894736842105263\n",
      "Recall 3/48 0.0625\n",
      "P:38, TP:3, FP:35, FN:45, TN:2054, N:2099\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:53<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 50.47677817806834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:52<00:00, 41.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 349.31609358184505\n",
      "Precission: 3/38 0.07894736842105263\n",
      "Recall 3/48 0.0625\n",
      "P:38, TP:3, FP:35, FN:45, TN:2054, N:2099\n",
      "Model saw positive samples 660 times and background samples 2640\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [01:23<00:00, 41.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 313.5519404569641\n",
      "Precission: 6/28 0.21428571428571427\n",
      "Recall 6/59 0.1016949152542373\n",
      "P:28, TP:6, FP:22, FN:53, TN:3348, N:3401\n",
      "Epoch 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:01<00:00, 351.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 451.08284294605255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:05<00:00, 413.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1427.7843658924103\n",
      "Precission: 3/210 0.014285714285714285\n",
      "Recall 3/48 0.0625\n",
      "P:210, TP:3, FP:207, FN:45, TN:1882, N:1927\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:02<00:00, 284.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 449.4297215938568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:05<00:00, 381.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1421.50547093153\n",
      "Precission: 1/140 0.007142857142857143\n",
      "Recall 1/48 0.020833333333333332\n",
      "P:140, TP:1, FP:139, FN:47, TN:1950, N:1997\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:02<00:00, 283.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 447.4124284386635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:05<00:00, 380.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1419.4958703517914\n",
      "Precission: 1/130 0.007692307692307693\n",
      "Recall 1/48 0.020833333333333332\n",
      "P:130, TP:1, FP:129, FN:47, TN:1960, N:2007\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:02<00:00, 281.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 447.7356114387512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:05<00:00, 381.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1419.4958703517914\n",
      "Precission: 1/130 0.007692307692307693\n",
      "Recall 1/48 0.020833333333333332\n",
      "P:130, TP:1, FP:129, FN:47, TN:1960, N:2007\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:02<00:00, 281.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 447.91714811325073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:05<00:00, 378.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1419.4958703517914\n",
      "Precission: 1/130 0.007692307692307693\n",
      "Recall 1/48 0.020833333333333332\n",
      "P:130, TP:1, FP:129, FN:47, TN:1960, N:2007\n",
      "Model saw positive samples 660 times and background samples 2640\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [00:09<00:00, 378.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2269.6001196801662\n",
      "Precission: 0/185 0.0\n",
      "Recall 0/59 0.0\n",
      "P:185, TP:0, FP:185, FN:59, TN:3185, N:3244\n",
      "Epoch 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:01<00:00, 525.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 216.91720527410507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:03<00:00, 665.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 621.471215814352\n",
      "Precission: 3/42 0.07142857142857142\n",
      "Recall 3/48 0.0625\n",
      "P:42, TP:3, FP:39, FN:45, TN:2050, N:2095\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:01<00:00, 513.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 144.57336097210646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:03<00:00, 654.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 496.53652749955654\n",
      "Precission: 2/35 0.05714285714285714\n",
      "Recall 2/48 0.041666666666666664\n",
      "P:35, TP:2, FP:33, FN:46, TN:2056, N:2102\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:01<00:00, 515.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 119.36597369611263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:03<00:00, 650.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 464.2784032821655\n",
      "Precission: 2/33 0.06060606060606061\n",
      "Recall 2/48 0.041666666666666664\n",
      "P:33, TP:2, FP:31, FN:46, TN:2058, N:2104\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:01<00:00, 504.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 115.59472229331732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:03<00:00, 641.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 464.2784032821655\n",
      "Precission: 2/33 0.06060606060606061\n",
      "Recall 2/48 0.041666666666666664\n",
      "P:33, TP:2, FP:31, FN:46, TN:2058, N:2104\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:01<00:00, 509.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 115.59472229331732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [00:03<00:00, 652.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 464.2784032821655\n",
      "Precission: 2/33 0.06060606060606061\n",
      "Recall 2/48 0.041666666666666664\n",
      "P:33, TP:2, FP:31, FN:46, TN:2058, N:2104\n",
      "Model saw positive samples 660 times and background samples 2640\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [00:05<00:00, 646.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 703.764436930418\n",
      "Precission: 8/38 0.21052631578947367\n",
      "Recall 8/59 0.13559322033898305\n",
      "P:38, TP:8, FP:30, FN:51, TN:3340, N:3391\n",
      "FOLD Embedded 3\n",
      "torch.cuda.memory_allocated: 0.166975GB\n",
      "torch.cuda.memory_reserved: 0.232422GB\n",
      "torch.cuda.max_memory_reserved: 5.794922GB\n",
      "Data loaded\n",
      "Epoch 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:49<00:00, 12.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 305.7235915334895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [01:23<00:00, 40.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 303.22414669860154\n",
      "Precission: 7/11 0.6363636363636364\n",
      "Recall 7/59 0.11864406779661017\n",
      "P:11, TP:7, FP:4, FN:52, TN:3366, N:3418\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:49<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 69.02999663981609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [01:23<00:00, 40.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 371.2367298703175\n",
      "Precission: 7/19 0.3684210526315789\n",
      "Recall 7/59 0.11864406779661017\n",
      "P:19, TP:7, FP:12, FN:52, TN:3358, N:3410\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:49<00:00, 12.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 51.79366463521728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [01:23<00:00, 40.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 384.50009135337314\n",
      "Precission: 7/25 0.28\n",
      "Recall 7/59 0.11864406779661017\n",
      "P:25, TP:7, FP:18, FN:52, TN:3352, N:3404\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:49<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 48.44187934527872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [01:23<00:00, 40.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 384.50009135337314\n",
      "Precission: 7/25 0.28\n",
      "Recall 7/59 0.11864406779661017\n",
      "P:25, TP:7, FP:18, FN:52, TN:3352, N:3404\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:49<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 48.51553594239522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [01:23<00:00, 40.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 384.50009135337314\n",
      "Precission: 7/25 0.28\n",
      "Recall 7/59 0.11864406779661017\n",
      "P:25, TP:7, FP:18, FN:52, TN:3352, N:3404\n",
      "Model saw positive samples 615 times and background samples 2460\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:45<00:00, 40.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 273.6634945375845\n",
      "Precission: 3/8 0.375\n",
      "Recall 3/57 0.05263157894736842\n",
      "P:8, TP:3, FP:5, FN:54, TN:1773, N:1827\n",
      "Epoch 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:01<00:00, 369.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 432.0261743068695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [00:08<00:00, 424.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2406.496928215027\n",
      "Precission: 39/2677 0.014568546880836758\n",
      "Recall 39/59 0.6610169491525424\n",
      "P:2677, TP:39, FP:2638, FN:20, TN:732, N:752\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:02<00:00, 293.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 428.5645799636841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [00:08<00:00, 397.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2384.7483047246933\n",
      "Precission: 31/2020 0.015346534653465346\n",
      "Recall 31/59 0.5254237288135594\n",
      "P:2020, TP:31, FP:1989, FN:28, TN:1381, N:1409\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:02<00:00, 295.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 426.83560210466385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [00:08<00:00, 398.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2378.321114361286\n",
      "Precission: 27/1780 0.015168539325842697\n",
      "Recall 27/59 0.4576271186440678\n",
      "P:1780, TP:27, FP:1753, FN:32, TN:1617, N:1649\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:02<00:00, 291.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 426.50659811496735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [00:08<00:00, 396.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2378.321114361286\n",
      "Precission: 27/1780 0.015168539325842697\n",
      "Recall 27/59 0.4576271186440678\n",
      "P:1780, TP:27, FP:1753, FN:32, TN:1617, N:1649\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:02<00:00, 290.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 426.932833135128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [00:08<00:00, 398.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2378.321114361286\n",
      "Precission: 27/1780 0.015168539325842697\n",
      "Recall 27/59 0.4576271186440678\n",
      "P:1780, TP:27, FP:1753, FN:32, TN:1617, N:1649\n",
      "Model saw positive samples 615 times and background samples 2460\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:04<00:00, 394.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1273.4100090265274\n",
      "Precission: 33/972 0.033950617283950615\n",
      "Recall 33/57 0.5789473684210527\n",
      "P:972, TP:33, FP:939, FN:24, TN:839, N:863\n",
      "Epoch 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:01<00:00, 546.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 362.08590257167816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [00:05<00:00, 673.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1697.9356300234795\n",
      "Precission: 15/189 0.07936507936507936\n",
      "Recall 15/59 0.2542372881355932\n",
      "P:189, TP:15, FP:174, FN:44, TN:3196, N:3240\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:01<00:00, 528.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 247.08433510363102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [00:05<00:00, 663.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1295.8762555122375\n",
      "Precission: 12/70 0.17142857142857143\n",
      "Recall 12/59 0.2033898305084746\n",
      "P:70, TP:12, FP:58, FN:47, TN:3312, N:3359\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:01<00:00, 524.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 204.68527035415173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [00:05<00:00, 664.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1185.5653326809406\n",
      "Precission: 11/53 0.20754716981132076\n",
      "Recall 11/59 0.1864406779661017\n",
      "P:53, TP:11, FP:42, FN:48, TN:3328, N:3376\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:01<00:00, 522.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 198.22979913651943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [00:05<00:00, 660.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1185.5653326809406\n",
      "Precission: 11/53 0.20754716981132076\n",
      "Recall 11/59 0.1864406779661017\n",
      "P:53, TP:11, FP:42, FN:48, TN:3328, N:3376\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [00:01<00:00, 528.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 198.22979913651943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3429/3429 [00:05<00:00, 658.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1185.5653326809406\n",
      "Precission: 11/53 0.20754716981132076\n",
      "Recall 11/59 0.1864406779661017\n",
      "P:53, TP:11, FP:42, FN:48, TN:3328, N:3376\n",
      "Model saw positive samples 615 times and background samples 2460\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:02<00:00, 646.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 654.3571068644524\n",
      "Precission: 7/34 0.20588235294117646\n",
      "Recall 7/57 0.12280701754385964\n",
      "P:34, TP:7, FP:27, FN:50, TN:1751, N:1801\n",
      "FOLD Embedded 4\n",
      "torch.cuda.memory_allocated: 0.166975GB\n",
      "torch.cuda.memory_reserved: 0.232422GB\n",
      "torch.cuda.max_memory_reserved: 5.794922GB\n",
      "Data loaded\n",
      "Epoch 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [00:55<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 311.376224482432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:45<00:00, 40.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 299.0191290588118\n",
      "Precission: 2/2 1.0\n",
      "Recall 2/57 0.03508771929824561\n",
      "P:2, TP:2, FP:0, FN:55, TN:1778, N:1833\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [00:55<00:00, 12.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 73.34688040462788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:45<00:00, 40.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 381.1958814243553\n",
      "Precission: 2/3 0.6666666666666666\n",
      "Recall 2/57 0.03508771929824561\n",
      "P:3, TP:2, FP:1, FN:55, TN:1777, N:1832\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [00:55<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 57.42104491248028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:45<00:00, 40.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 394.21558388735866\n",
      "Precission: 2/3 0.6666666666666666\n",
      "Recall 2/57 0.03508771929824561\n",
      "P:3, TP:2, FP:1, FN:55, TN:1777, N:1832\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [00:55<00:00, 12.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 54.61597868188983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:45<00:00, 40.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 394.21558388735866\n",
      "Precission: 2/3 0.6666666666666666\n",
      "Recall 2/57 0.03508771929824561\n",
      "P:3, TP:2, FP:1, FN:55, TN:1777, N:1832\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [00:55<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 54.54867101460695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:45<00:00, 40.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 394.21558388735866\n",
      "Precission: 2/3 0.6666666666666666\n",
      "Recall 2/57 0.03508771929824561\n",
      "P:3, TP:2, FP:1, FN:55, TN:1777, N:1832\n",
      "Model saw positive samples 680 times and background samples 2720\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:44<00:00, 42.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 247.6087867631577\n",
      "Precission: 3/3 1.0\n",
      "Recall 3/46 0.06521739130434782\n",
      "P:3, TP:3, FP:0, FN:43, TN:1846, N:1889\n",
      "Epoch 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [00:01<00:00, 369.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 503.6975285410881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:04<00:00, 437.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1350.8901352286339\n",
      "Precission: 56/1834 0.030534351145038167\n",
      "Recall 56/57 0.9824561403508771\n",
      "P:1834, TP:56, FP:1778, FN:1, TN:0, N:1\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [00:02<00:00, 288.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 495.6560915708542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:04<00:00, 390.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1335.6000443696976\n",
      "Precission: 56/1831 0.030584380120152924\n",
      "Recall 56/57 0.9824561403508771\n",
      "P:1831, TP:56, FP:1775, FN:1, TN:3, N:4\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [00:02<00:00, 288.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 492.5077426433563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:04<00:00, 388.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1331.0398265123367\n",
      "Precission: 56/1831 0.030584380120152924\n",
      "Recall 56/57 0.9824561403508771\n",
      "P:1831, TP:56, FP:1775, FN:1, TN:3, N:4\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [00:02<00:00, 289.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 491.6448568701744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:04<00:00, 392.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1331.0398265123367\n",
      "Precission: 56/1831 0.030584380120152924\n",
      "Recall 56/57 0.9824561403508771\n",
      "P:1831, TP:56, FP:1775, FN:1, TN:3, N:4\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [00:02<00:00, 287.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 491.8528990149498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:04<00:00, 391.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1331.0398265123367\n",
      "Precission: 56/1831 0.030584380120152924\n",
      "Recall 56/57 0.9824561403508771\n",
      "P:1831, TP:56, FP:1775, FN:1, TN:3, N:4\n",
      "Model saw positive samples 680 times and background samples 2720\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:04<00:00, 402.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1377.265360057354\n",
      "Precission: 46/1890 0.02433862433862434\n",
      "Recall 46/46 1.0\n",
      "P:1890, TP:46, FP:1844, FN:0, TN:2, N:2\n",
      "Epoch 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [00:01<00:00, 533.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 239.33152472227812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:02<00:00, 667.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 590.9308129400015\n",
      "Precission: 4/9 0.4444444444444444\n",
      "Recall 4/57 0.07017543859649122\n",
      "P:9, TP:4, FP:5, FN:53, TN:1773, N:1826\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [00:01<00:00, 524.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 148.7612915635109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:02<00:00, 657.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 456.3772820532322\n",
      "Precission: 4/9 0.4444444444444444\n",
      "Recall 4/57 0.07017543859649122\n",
      "P:9, TP:4, FP:5, FN:53, TN:1773, N:1826\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [00:01<00:00, 519.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 118.44757320731878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:02<00:00, 635.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 421.9426532536745\n",
      "Precission: 4/9 0.4444444444444444\n",
      "Recall 4/57 0.07017543859649122\n",
      "P:9, TP:4, FP:5, FN:53, TN:1773, N:1826\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [00:01<00:00, 494.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 113.9505993872881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:02<00:00, 618.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 421.9426532536745\n",
      "Precission: 4/9 0.4444444444444444\n",
      "Recall 4/57 0.07017543859649122\n",
      "P:9, TP:4, FP:5, FN:53, TN:1773, N:1826\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 680/680 [00:01<00:00, 495.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this epoch: 113.9505993872881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:02<00:00, 620.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 421.9426532536745\n",
      "Precission: 4/9 0.4444444444444444\n",
      "Recall 4/57 0.07017543859649122\n",
      "P:9, TP:4, FP:5, FN:53, TN:1773, N:1826\n",
      "Model saw positive samples 680 times and background samples 2720\n",
      "Ratio 1:4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1892/1892 [00:03<00:00, 629.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 410.3888857215643\n",
      "Precission: 3/12 0.25\n",
      "Recall 3/46 0.06521739130434782\n",
      "P:12, TP:3, FP:9, FN:43, TN:1837, N:1880\n",
      "LSTM\n",
      "accumulated_loss 252.10417057764715\n",
      "precission 0.43906926406926405\n",
      "recall 0.0770984322736045\n",
      "f1 0.13116493790335676\n",
      "CONV\n",
      "accumulated_loss 1446.6286593854427\n",
      "precission 0.020350754652184798\n",
      "recall 0.4910768300060496\n",
      "f1 0.0390819126015825\n",
      "MEAN\n",
      "accumulated_loss 563.1724530518055\n",
      "precission 0.23160106147722245\n",
      "recall 0.10768329595238064\n",
      "f1 0.14701276436601926\n"
     ]
    }
   ],
   "source": [
    "def do_stuff2():\n",
    "    # data_files = load_fold_data(raw_input_path, fold_count=folds_count, data_fraction=fraction_of_data)\n",
    "\n",
    "    # for i in range(folds_count):\n",
    "    #     save_file_datasets(data_files[i], f'fold-{i}')\n",
    "    \n",
    "    lstm_folds_results = []\n",
    "    conv_folds_results = []\n",
    "    mean_folds_results = []\n",
    "    for i in range(folds_count):\n",
    "        lstm_results, conv_results, mean_results = do_a_fold_aggregator(i)\n",
    "        lstm_folds_results.append(lstm_results)\n",
    "        conv_folds_results.append(conv_results)\n",
    "        mean_folds_results.append(mean_results)\n",
    "\n",
    "    print('LSTM')\n",
    "    print_summary(lstm_folds_results, folds_count)\n",
    "    print('CONV')\n",
    "    print_summary(conv_folds_results, folds_count)\n",
    "    print('MEAN')\n",
    "    print_summary(mean_folds_results, folds_count)\n",
    "    return lstm_folds_results, conv_folds_results, mean_folds_results\n",
    "\n",
    "lstm_folds_results, conv_folds_results, mean_folds_results = do_stuff2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')\n",
    "print('LSTM')\n",
    "print_summary(lstm_folds_results, folds_count)\n",
    "print('CONV')\n",
    "print_summary(conv_folds_results, folds_count)\n",
    "print('MEAN')\n",
    "print_summary(mean_folds_results, folds_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
