Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 0/3
100%|██████████| 1250/1250 [07:47<00:00,  2.68it/s]
Loss in this epoch: 834.5740791857243
100%|██████████| 500/500 [00:57<00:00,  8.70it/s]
Precission: 33/885 0.03728813559322034
Recall 33/40 0.825
P:885, TP:33, FP:852, FN:7, TN:1108, N:1115
Epoch 1/3
100%|██████████| 1250/1250 [07:15<00:00,  2.87it/s]
Loss in this epoch: 297.36948271654546
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 13/51 0.2549019607843137
Recall 13/40 0.325
P:51, TP:13, FP:38, FN:27, TN:1922, N:1949
Epoch 2/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 56.193640563986264
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 16/63 0.25396825396825395
Recall 16/40 0.4
P:63, TP:16, FP:47, FN:24, TN:1913, N:1937
Model saw positive samples 7500 times and background samples 7500
Ratio 1:1.0
100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]
Precission: 385/2157 0.17848864163189615
Recall 385/1267 0.30386740331491713
P:2157, TP:385, FP:1772, FN:882, TN:32637, N:33519

XXX
learning_rate 2e-06
oversampling_ratio 1
class_ratio 1
precision 0.17848864163189615
recall 0.30386740331491713
XXX

Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 0/3
100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]
Loss in this epoch: 797.6526148170233
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 3/136 0.022058823529411766
Recall 3/40 0.075
P:136, TP:3, FP:133, FN:37, TN:1827, N:1864
Epoch 1/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 281.135711459443
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 5/16 0.3125
Recall 5/40 0.125
P:16, TP:5, FP:11, FN:35, TN:1949, N:1984
Epoch 2/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 52.3525689104863
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 7/33 0.21212121212121213
Recall 7/40 0.175
P:33, TP:7, FP:26, FN:33, TN:1934, N:1967
Model saw positive samples 4998 times and background samples 10002
Ratio 1:2.0012004801920766
100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]
Precission: 464/1750 0.2651428571428571
Recall 464/1267 0.36621941594317287
P:1750, TP:464, FP:1286, FN:803, TN:33123, N:33926

XXX
learning_rate 2e-06
oversampling_ratio 1
class_ratio 2
precision 0.2651428571428571
recall 0.36621941594317287
XXX

Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 0/3
100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]
Loss in this epoch: 649.8499447703362
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 0/0 0
Recall 0/40 0.0
P:0, TP:0, FP:0, FN:40, TN:1960, N:2000
Epoch 1/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 350.5728516690433
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 5/78 0.0641025641025641
Recall 5/40 0.125
P:78, TP:5, FP:73, FN:35, TN:1887, N:1922
Epoch 2/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 116.49241442675702
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 4/31 0.12903225806451613
Recall 4/40 0.1
P:31, TP:4, FP:27, FN:36, TN:1933, N:1969
Model saw positive samples 3000 times and background samples 12000
Ratio 1:4.0
100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]
Precission: 189/1103 0.17135086128739802
Recall 189/1267 0.14917127071823205
P:1103, TP:189, FP:914, FN:1078, TN:33495, N:34573

XXX
learning_rate 2e-06
oversampling_ratio 1
class_ratio 4
precision 0.17135086128739802
recall 0.14917127071823205
XXX

Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 0/3
100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]
Loss in this epoch: 484.3729382492602
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 0/0 0
Recall 0/40 0.0
P:0, TP:0, FP:0, FN:40, TN:1960, N:2000
Epoch 1/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 293.17150229681283
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 0/2 0.0
Recall 0/40 0.0
P:2, TP:0, FP:2, FN:40, TN:1958, N:1998
Epoch 2/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 92.03241161175538
100%|██████████| 500/500 [00:49<00:00, 10.09it/s]
Precission: 2/12 0.16666666666666666
Recall 2/40 0.05
P:12, TP:2, FP:10, FN:38, TN:1950, N:1988
Model saw positive samples 1665 times and background samples 13335
Ratio 1:8.00900900900901
100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]
Precission: 206/439 0.46924829157175396
Recall 206/1267 0.16258879242304658
P:439, TP:206, FP:233, FN:1061, TN:34176, N:35237

XXX
learning_rate 2e-06
oversampling_ratio 1
class_ratio 8
precision 0.46924829157175396
recall 0.16258879242304658
XXX

Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 0/3
100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]
Loss in this epoch: 853.6391683220863
100%|██████████| 500/500 [00:49<00:00, 10.09it/s]
Precission: 10/573 0.017452006980802792
Recall 10/40 0.25
P:573, TP:10, FP:563, FN:30, TN:1397, N:1427
Epoch 1/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 291.8435784452595
100%|██████████| 500/500 [00:49<00:00, 10.09it/s]
Precission: 2/71 0.028169014084507043
Recall 2/40 0.05
P:71, TP:2, FP:69, FN:38, TN:1891, N:1929
Epoch 2/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 55.08163957239594
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 1/33 0.030303030303030304
Recall 1/40 0.025
P:33, TP:1, FP:32, FN:39, TN:1928, N:1967
Model saw positive samples 7500 times and background samples 7500
Ratio 1:1.0
100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]
Precission: 216/503 0.4294234592445328
Recall 216/1267 0.17048145224940806
P:503, TP:216, FP:287, FN:1051, TN:34122, N:35173

XXX
learning_rate 2e-06
oversampling_ratio 2
class_ratio 1
precision 0.4294234592445328
recall 0.17048145224940806
XXX

Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 0/3
100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]
Loss in this epoch: 795.4382631182671
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 0/3 0.0
Recall 0/40 0.0
P:3, TP:0, FP:3, FN:40, TN:1957, N:1997
Epoch 1/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 386.72980129159987
100%|██████████| 500/500 [00:49<00:00, 10.09it/s]
Precission: 3/20 0.15
Recall 3/40 0.075
P:20, TP:3, FP:17, FN:37, TN:1943, N:1980
Epoch 2/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 113.12428697571158
100%|██████████| 500/500 [00:49<00:00, 10.09it/s]
Precission: 7/71 0.09859154929577464
Recall 7/40 0.175
P:71, TP:7, FP:64, FN:33, TN:1896, N:1929
Model saw positive samples 4998 times and background samples 10002
Ratio 1:2.0012004801920766
100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]
Precission: 275/1305 0.210727969348659
Recall 275/1267 0.2170481452249408
P:1305, TP:275, FP:1030, FN:992, TN:33379, N:34371

XXX
learning_rate 2e-06
oversampling_ratio 2
class_ratio 2
precision 0.210727969348659
recall 0.2170481452249408
XXX

Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 0/3
100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]
Loss in this epoch: 674.2693476304412
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 0/0 0
Recall 0/40 0.0
P:0, TP:0, FP:0, FN:40, TN:1960, N:2000
Epoch 1/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 401.07263765949756
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 7/118 0.059322033898305086
Recall 7/40 0.175
P:118, TP:7, FP:111, FN:33, TN:1849, N:1882
Epoch 2/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 124.33188724494539
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 4/14 0.2857142857142857
Recall 4/40 0.1
P:14, TP:4, FP:10, FN:36, TN:1950, N:1986
Model saw positive samples 3000 times and background samples 12000
Ratio 1:4.0
100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]
Precission: 79/386 0.20466321243523317
Recall 79/1267 0.062352012628255724
P:386, TP:79, FP:307, FN:1188, TN:34102, N:35290

XXX
learning_rate 2e-06
oversampling_ratio 2
class_ratio 4
precision 0.20466321243523317
recall 0.062352012628255724
XXX

Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 0/3
100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]
Loss in this epoch: 471.6965223029256
100%|██████████| 500/500 [00:49<00:00, 10.09it/s]
Precission: 0/0 0
Recall 0/40 0.0
P:0, TP:0, FP:0, FN:40, TN:1960, N:2000
Epoch 1/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 353.2985911844298
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 0/0 0
Recall 0/40 0.0
P:0, TP:0, FP:0, FN:40, TN:1960, N:2000
Epoch 2/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 154.49234365089796
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 1/19 0.05263157894736842
Recall 1/40 0.025
P:19, TP:1, FP:18, FN:39, TN:1942, N:1981
Model saw positive samples 1665 times and background samples 13335
Ratio 1:8.00900900900901
100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]
Precission: 225/673 0.3343239227340267
Recall 225/1267 0.17758484609313338
P:673, TP:225, FP:448, FN:1042, TN:33961, N:35003

XXX
learning_rate 2e-06
oversampling_ratio 2
class_ratio 8
precision 0.3343239227340267
recall 0.17758484609313338
XXX

Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 0/3
100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]
Loss in this epoch: 863.6748497486115
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 19/261 0.07279693486590039
Recall 19/40 0.475
P:261, TP:19, FP:242, FN:21, TN:1718, N:1739
Epoch 1/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 325.6189609277062
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 6/54 0.1111111111111111
Recall 6/40 0.15
P:54, TP:6, FP:48, FN:34, TN:1912, N:1946
Epoch 2/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 55.53576231840998
100%|██████████| 500/500 [00:49<00:00, 10.09it/s]
Precission: 6/31 0.1935483870967742
Recall 6/40 0.15
P:31, TP:6, FP:25, FN:34, TN:1935, N:1969
Model saw positive samples 7500 times and background samples 7500
Ratio 1:1.0
100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]
Precission: 207/974 0.21252566735112938
Recall 207/1267 0.1633780584056827
P:974, TP:207, FP:767, FN:1060, TN:33642, N:34702

XXX
learning_rate 2e-06
oversampling_ratio 4
class_ratio 1
precision 0.21252566735112938
recall 0.1633780584056827
XXX

Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 0/3
100%|██████████| 1250/1250 [06:58<00:00,  2.99it/s]
Loss in this epoch: 786.9828036725521
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 0/47 0.0
Recall 0/40 0.0
P:47, TP:0, FP:47, FN:40, TN:1913, N:1953
Epoch 1/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 434.60826200805604
100%|██████████| 500/500 [00:49<00:00, 10.10it/s]
Precission: 2/17 0.11764705882352941
Recall 2/40 0.05
P:17, TP:2, FP:15, FN:38, TN:1945, N:1983
Epoch 2/3
100%|██████████| 1250/1250 [06:43<00:00,  3.10it/s]
Loss in this epoch: 111.03646565403324
100%|██████████| 500/500 [00:49<00:00, 10.09it/s]
Precission: 5/29 0.1724137931034483
Recall 5/40 0.125
P:29, TP:5, FP:24, FN:35, TN:1936, N:1971
Model saw positive samples 4998 times and background samples 10002
Ratio 1:2.0012004801920766
100%|██████████| 8919/8919 [14:42<00:00, 10.10it/s]
Precission: 312/1217 0.2563681183237469
Recall 312/1267 0.2462509865824783
P:1217, TP:312, FP:905, FN:955, TN:33504, N:34459

XXX
learning_rate 2e-06
oversampling_ratio 4
class_ratio 2
precision 0.2563681183237469
recall 0.2462509865824783
XXX
