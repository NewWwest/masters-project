import json
import tqdm
import os

from src.utils.utils import is_hexadecimal_string, info_log, warn_log
from src.loaders.OmniLoader import OmniLoader
from src.proxies.GitHubProxy import GithubProxy

class GitHubReferencesMiner:
    def __init__(self, githubProxy:GithubProxy, checkpoint_path, checkpoint_frequency=100  ) -> None:
        self.githubProxy = githubProxy
        self.checkpoint_path = checkpoint_path
        self.checkpoint_frequency = checkpoint_frequency


    def make_ref(self, repo_owner:str, repo_name:str, reference_type:str, reference_value:str) -> str:
        return f'{repo_owner}/{repo_name}/{reference_type}/{reference_value}'


    def _get_standarized_refs(self, github_refs):
        refs_to_process = []

        for url in github_refs:
            segments = url.split('#')[0].split('/')
            if len(segments) < 7:
                continue

            repo_owner = segments[3]
            repo_name = segments[4]
            reference_type = segments[5]
            reference_value = segments[6]

            if reference_type == 'commit' or reference_type == 'commits':
                res = self.make_ref(repo_owner, repo_name, 'commit', reference_value)
                refs_to_process.append(res)
                pass
            elif '/issue/' in url or '/issues/' in url or '/pull/' in url:
                res = self.make_ref(repo_owner, repo_name, 'issue', reference_value)
                refs_to_process.append(res)
                pass
            elif '/compare/' in url:
                res = self.make_ref(repo_owner, repo_name, 'compare', reference_value)
                refs_to_process.append(res)
                pass 
            elif '/blob/' in url or '/tree/' in url:
                if is_hexadecimal_string(reference_value):
                    res = self.make_ref(repo_owner, repo_name, 'commit', reference_value)
                    refs_to_process.append(res)
                pass

        unique_refs = set(refs_to_process)
        return list(unique_refs)


    def _make_csv_obj(self, report_id, repo_owner:str, repo_name:str, reference_type:str, reference_value:str, data, aliases):
        csv_object = {}
        csv_object['report_id'] = report_id
        csv_object['aliases'] = json.dumps(aliases)
        csv_object['repo_owner'] = repo_owner
        csv_object['repo_name'] = repo_name
        csv_object['reference_type'] = reference_type
        csv_object['reference_value'] = reference_value
        csv_object['data'] = json.dumps(data)
        return csv_object


    def _download_ref_data_for_vulnerability(self, ref):
        data = ''
        segments = ref.split('/')
        repo_owner = segments[0]
        repo_name = segments[1]
        reference_type = segments[2]
        reference_value = segments[3]

        if reference_type == 'commit':
            data = self.githubProxy.get_commit_data(repo_owner, repo_name, reference_value)
        elif reference_type == 'issue':
            data = self.githubProxy.get_issue_data(repo_owner, repo_name, reference_value)
        elif reference_type == 'compare':
            data = self.githubProxy.get_compare_data(repo_owner, repo_name, reference_value)
        else:
            raise Exception(f'Unknown reference_type {ref}')
        
        return data

    def _connect_ref_data_to_reports(self, report_id, ref, data, aliases):
        segments = ref.split('/')
        data_csv_ready = self._make_csv_obj(report_id, segments[0], segments[1], segments[2], segments[3], data, aliases)
        return data_csv_ready


    def process_vulnerabilities(self, omniLoader:OmniLoader, intermediate_result_csv = None):
        refs_per_vulnerabilities = {}
        with tqdm.tqdm(total=len(omniLoader.reports)) as pbar:
            for report_id in omniLoader.reports:
                try:
                    refs = omniLoader.references_from_report_list(omniLoader.reports[report_id])
                    github_refs = [x for x in refs if '/github.com/' in x]
                    refs = self._get_standarized_refs(github_refs)
                    if refs != None and len(refs) > 0:
                        refs_per_vulnerabilities[report_id] = refs
                except Exception as e:
                    warn_log(f'Exception in processing vulnerability {report_id}')
                    warn_log(e)

                pbar.update()


        if intermediate_result_csv != None: 
            with open(os.path.join(self.checkpoint_path, intermediate_result_csv), 'w') as f:
                json.dump(refs_per_vulnerabilities, f, indent=2)

        vulnerabilities_per_ref = {}
        for report_id in refs_per_vulnerabilities:
            for ref in refs_per_vulnerabilities[report_id]:
                if ref not in vulnerabilities_per_ref:
                    vulnerabilities_per_ref[ref] = []
                vulnerabilities_per_ref[ref].append(report_id)
                
        info_log("Done extracting references from vulnerabilities")

        processed_ref_count = 0 
        current_data = {}
        with tqdm.tqdm(total=len(vulnerabilities_per_ref)) as pbar:
            for ref in vulnerabilities_per_ref:
                processed_ref_count += 1
                try:
                    data = self._download_ref_data_for_vulnerability(ref)
                    if data == None:
                        continue
                    
                    for report_id in vulnerabilities_per_ref[ref]:
                        resolved_ref = self._connect_ref_data_to_reports(report_id, ref, data, omniLoader.related_ids[report_id])
                        if report_id not in current_data:
                            current_data[report_id] = []
                        
                        current_data[report_id].append(resolved_ref)
                except Exception as e:
                    warn_log(f'Exception in processing vulnerability {report_id}')
                    warn_log(e)

                if processed_ref_count % self.checkpoint_frequency == 0:
                    with open(f'{self.checkpoint_path}/data_{processed_ref_count}.json', 'w') as f:
                        json.dump(current_data, f)
                pbar.update()

        with open(f'{self.checkpoint_path}/data_full.json', 'w') as f:
            json.dump(current_data, f)

        return current_data

