{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r'D:\\Projects\\aaa')\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "from src.rq5.datasets.CommitLevelRawDataset import CommitLevelRawDataset\n",
    "from src.rq5.datasets.SampleLevelRawDataset import SampleLevelRawDataset\n",
    "from src.rq5.datasets.supporting.CsvDataset import CsvDataset\n",
    "from src.rq5.datasets.sampling.OverSampledDataset import OverSampledDataset\n",
    "from src.rq5.datasets.sampling.UnderSampledDataset import UnderSampledDataset\n",
    "\n",
    "from src.rq5.datasets.load import load_sample_level\n",
    "from src.rq5.dl_utils import save_dataset, read_dataset, get_repo_seminames, get_files_in_set\n",
    "from src.utils.utils import get_files_in_from_directory\n",
    "from src.rq5.models.BertAndLinear import BertAndLinear as FineTuningModel\n",
    "from src.rq5.models.LstmAggregator import LstmAggregator as AggregatorModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "base_model = 'microsoft/graphcodebert-base'\n",
    "batch_size_ = 4\n",
    "num_epochs_ = 3\n",
    "\n",
    "fraction_of_data = 1\n",
    "\n",
    "sample_limit = 5_000\n",
    "eval_sample_limit = 2_000\n",
    "test_percentage = 0.15\n",
    "eval_percentage = 0.05\n",
    "\n",
    "learning_rate = 1e-6\n",
    "oversampling_ratio = 2\n",
    "class_ratio = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator_num_epochs_ = 15\n",
    "aggregator_class_ratio = 2\n",
    "aggregator_learning_rate = 5e-6\n",
    "\n",
    "save_model_in_each_epoch = True\n",
    "eval_model_in_each_epoch = True\n",
    "\n",
    "model_guid = str(uuid.uuid4())\n",
    "model_name = model_guid\n",
    "\n",
    "work_dir = f'D:\\\\Projects\\\\aaa\\src\\\\rq5\\\\binaries\\\\{model_name}'\n",
    "results_dir = f'D:\\\\Projects\\\\aaa\\src\\\\rq5\\\\binaries\\\\data{model_name}'\n",
    "\n",
    "# Data config - Set to None if you want to use cached datasets\n",
    "raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\CodeParserMiner_ast'\n",
    "# raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\CodeParserMiner_edit'\n",
    "# raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\AddedCodeMiner'\n",
    "# raw_input_path = 'D:\\\\Projects\\\\aaa\\\\results\\\\dl\\\\java2\\\\RollingWindowMiner'\n",
    "# raw_input_path = None\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(work_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir(results_dir) \n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, data_loader, loss_module, scheduler, eval_loader = None):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    all_samples = 0\n",
    "    positive_samples = 0\n",
    "\n",
    "    for epoch in range(num_epochs_):\n",
    "        print(f'Epoch {epoch}/{num_epochs_}')\n",
    "        accumulated_loss = 0\n",
    "\n",
    "        with tqdm.tqdm(total=len(data_loader)) as pbar:\n",
    "            for data_inputs, data_labels in data_loader:\n",
    "                # Step 0: Diagnostics :x\n",
    "                positive_samples += len([1 for x in data_labels if x[0] == 1])\n",
    "                all_samples += len(data_labels)\n",
    "                \n",
    "                # Step 1: Mode data to device\n",
    "                data_inputs = data_inputs.to(device)\n",
    "                data_labels = data_labels.to(device)\n",
    "\n",
    "                # Step 2: Calculate model output\n",
    "                preds = model(data_inputs)\n",
    "                preds = preds.squeeze(dim=0)\n",
    "\n",
    "                # Step 3: Calculate loss\n",
    "                loss = loss_module(preds, data_labels.float())\n",
    "                accumulated_loss += loss.item()\n",
    "\n",
    "                ## Step 4: Perform backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                ## Step 5: Update the parameters\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Step 6: Progress bar\n",
    "                pbar.update()\n",
    "        print('Loss in this epoch:', accumulated_loss)\n",
    "\n",
    "        if save_model_in_each_epoch:\n",
    "            torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_epoch_{epoch}.pickle')\n",
    "\n",
    "        if eval_loader != None:\n",
    "            eval_model(model, eval_loader)\n",
    "\n",
    "\n",
    "    print(f'Model saw positive samples {positive_samples} times and background samples {all_samples-positive_samples}')\n",
    "    print(f'Ratio 1:{(all_samples-positive_samples)/positive_samples}')\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    data_size = len(data_loader)\n",
    "    with tqdm.tqdm(total=data_size) as pbar:\n",
    "        for data_inputs, data_labels in data_loader:\n",
    "            data_inputs = data_inputs.to(device)\n",
    "            data_labels = data_labels.to(device)\n",
    "            preds = model(data_inputs)\n",
    "            preds = preds.squeeze(dim=0)\n",
    "\n",
    "            labels_in_memory = data_labels.cpu().detach().numpy()\n",
    "            if len(labels_in_memory.shape) == 1:\n",
    "                all_labels.append(labels_in_memory)\n",
    "            else:\n",
    "                for x in labels_in_memory:\n",
    "                    all_labels.append(x)\n",
    "                    \n",
    "            preds_in_memory = preds.cpu().detach().numpy()\n",
    "            if labels_in_memory.shape[0] == 1:\n",
    "                all_predictions.append(preds_in_memory)\n",
    "            else:\n",
    "                for x in preds_in_memory:\n",
    "                    all_predictions.append(x)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    predictions_arr = [1 if x[0]>x[1] else 0 for x in all_predictions] #TODO softmax\n",
    "    targets_arr = [1 if x[0]>x[1] else 0 for x in all_labels]\n",
    "    P = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1])\n",
    "    TP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==1])\n",
    "    FP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==0])\n",
    "    FN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==1])\n",
    "    TN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==0])\n",
    "    N = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0])\n",
    "\n",
    "    precission = TP/(TP+FP) if (TP+FP)!=0 else 0\n",
    "    recall = TP/(TP+FN) if (TP+FN)!=0 else 0\n",
    "    print('Precission:',f'{TP}/{TP+FP}', precission)\n",
    "    print('Recall', f'{TP}/{TP+FN}', recall)\n",
    "    print(f'P:{P},', f'TP:{TP},', f'FP:{FP},', f'FN:{FN},', f'TN:{TN},', f'N:{N}')\n",
    "\n",
    "    return precission, recall\n",
    "\n",
    "\n",
    "def load_files(input_path, data_fraction=1):\n",
    "    positive_json_files = get_files_in_from_directory(input_path, extension='.json', startswith='positive-encodings')\n",
    "    background_json_files = get_files_in_from_directory(input_path, extension='.json', startswith='background-encodings')\n",
    "\n",
    "    if data_fraction < 1:\n",
    "        positive_json_files = random.sample(positive_json_files, int(len(positive_json_files)*data_fraction))\n",
    "        background_json_files = random.sample(background_json_files, int(len(background_json_files)*data_fraction))\n",
    "\n",
    "\n",
    "    repos_set = get_repo_seminames(positive_json_files)\n",
    "    repos_count = len(repos_set)\n",
    "\n",
    "\n",
    "    repos_test = set(random.sample(list(repos_set), int(repos_count*test_percentage)))\n",
    "    repos_set.difference_update(repos_test)\n",
    "    repos_eval = set(random.sample(list(repos_set), int(repos_count*test_percentage)))\n",
    "    repos_set.difference_update(repos_eval)\n",
    "\n",
    "    positive_train = get_files_in_set(positive_json_files, repos_set)\n",
    "    positive_eval = get_files_in_set(positive_json_files, repos_eval)\n",
    "    positive_test = get_files_in_set(positive_json_files, repos_test)\n",
    "\n",
    "    background_train = get_files_in_set(background_json_files, repos_set)\n",
    "    background_eval = get_files_in_set(background_json_files, repos_eval)\n",
    "    background_test = get_files_in_set(background_json_files, repos_test)\n",
    "\n",
    "    return (positive_json_files, background_json_files), (positive_train, background_train), (positive_eval, background_eval), (positive_test, background_test)\n",
    "\n",
    "\n",
    "def load_data(input_data, oversampling_ratio=None, class_ratio=None, sample_limit=None):\n",
    "    positive_files = input_data[0]\n",
    "    background_files = input_data[1]\n",
    "\n",
    "    dataset = SampleLevelRawDataset()\n",
    "    dataset.load_files(positive_files, background_files)\n",
    "\n",
    "    if oversampling_ratio != None and class_ratio != None and sample_limit != None:\n",
    "        dataset.setup_ratios(oversampling_ratio, class_ratio, sample_limit)   \n",
    "\n",
    "    if oversampling_ratio == None and class_ratio == None and sample_limit != None:\n",
    "        dataset.limit_data(sample_limit)   \n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def embed_files(tokenizer, data_files):\n",
    "    with tqdm.tqdm(total=len(data_files)) as pbar:\n",
    "        for data_file in data_files:\n",
    "            with open(data_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            embeddings = []\n",
    "            for data_point in data:\n",
    "                if 'commit_sample' in data_point and \\\n",
    "                    data_point['commit_sample'] != None and \\\n",
    "                    len(data_point['commit_sample']) > 0:\n",
    "\n",
    "                    tensor = torch.Tensor(data_point['commit_sample']).int()\n",
    "                    tensor = tensor[None, :] # Extend to a batch mode\n",
    "                    tensor = tensor.to(device)\n",
    "                    result = tokenizer(tensor)\n",
    "                    labels = result[0][:,0,:]\n",
    "                    labels_in_memory = labels.cpu()\n",
    "                    res = {\n",
    "                        'commit_id': data_point['commit_id'],\n",
    "                        'file_name': data_point['file_name'],\n",
    "                        'is_security_related': data_point['is_security_related'],\n",
    "                        'commit_sample': labels_in_memory.tolist()\n",
    "                    }\n",
    "                    embeddings.append(res)\n",
    "\n",
    "            if len(embeddings) > 0:\n",
    "                file_name = os.path.basename(data_file)\n",
    "                new_file = os.path.join(results_dir, 'embedded-' + file_name)\n",
    "                with open(new_file, 'w') as f:\n",
    "                    json.dump(embeddings, f)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "\n",
    "def map_files_to_new_repo(data_files):\n",
    "    new_data_files = []\n",
    "    for data_file in data_files:\n",
    "        file_name = os.path.basename(data_file)\n",
    "        new_file = os.path.join(results_dir, 'embedded-' + file_name)\n",
    "        if os.path.exists(new_file):\n",
    "            new_data_files.append(new_file)\n",
    "\n",
    "    return new_data_files\n",
    "\n",
    "\n",
    "def train_model2(model, optimizer, data_loader, loss_module, scheduler, test_loader = None):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    all_samples = 0\n",
    "    positive_samples = 0\n",
    "\n",
    "    for epoch in range(aggregator_num_epochs_):\n",
    "        print(f'Epoch {epoch}/{aggregator_num_epochs_}')\n",
    "        accumulated_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        with tqdm.tqdm(total=len(data_loader)) as pbar:\n",
    "            for data_inputs, data_labels in data_loader:\n",
    "                # Step 0: Diagnostics :x\n",
    "                positive_samples += len([1 for x in data_labels if x[0] == 1])\n",
    "                all_samples += len(data_labels)\n",
    "\n",
    "                #TODO different commit mode and sample mode\n",
    "                data_inputs = torch.stack(data_inputs)\n",
    "                \n",
    "                # Step 1: Mode data to device \n",
    "                data_inputs = data_inputs.to(device)\n",
    "                data_labels = data_labels.to(device) \n",
    "\n",
    "                # Step 2: Calculate model output\n",
    "                preds = model(data_inputs)\n",
    "                \n",
    "                #TODO different commit mode and sample mode\n",
    "                # preds = preds.squeeze(dim=0)\n",
    "\n",
    "                # Step 3: Calculate loss\n",
    "                loss = loss_module(preds, data_labels.float())\n",
    "                accumulated_loss += loss.item()\n",
    "\n",
    "                ## Step 4: Perform backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                ## Step 5: Update the parameters\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Step 6: Progress bar\n",
    "                pbar.update()\n",
    "        print('Loss in this epoch:', accumulated_loss)\n",
    "\n",
    "        if save_model_in_each_epoch:\n",
    "            torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_epoch_{epoch}.pickle')\n",
    "\n",
    "        if test_loader != None:\n",
    "            eval_model2(model, test_loader)\n",
    "\n",
    "\n",
    "    print(f'Model saw positive samples {positive_samples} times and background samples {all_samples-positive_samples}')\n",
    "    print(f'Ratio 1:{(all_samples-positive_samples)/positive_samples}')\n",
    "\n",
    "\n",
    "def eval_model2(model, data_loader):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    data_size = len(data_loader)\n",
    "    with tqdm.tqdm(total=data_size) as pbar:\n",
    "        for data_inputs, data_labels in data_loader:\n",
    "\n",
    "            #TODO different commit mode and sample mode\n",
    "            data_inputs = torch.stack(data_inputs)\n",
    "\n",
    "            data_inputs = data_inputs.to(device)\n",
    "            data_labels = data_labels.to(device)\n",
    "            preds = model(data_inputs)\n",
    "            #TODO different commit mode and sample mode\n",
    "            # preds = preds.squeeze(dim=0)\n",
    "\n",
    "            labels_in_memory = data_labels.cpu().detach().numpy()\n",
    "            if len(labels_in_memory.shape) == 1:\n",
    "                all_labels.append(labels_in_memory)\n",
    "            else:\n",
    "                for x in labels_in_memory:\n",
    "                    all_labels.append(x)\n",
    "                    \n",
    "            preds_in_memory = preds.cpu().detach().numpy()\n",
    "            if labels_in_memory.shape[0] == 1:\n",
    "                all_predictions.append(preds_in_memory)\n",
    "            else:\n",
    "                for x in preds_in_memory:\n",
    "                    all_predictions.append(x)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    #TODO different commit mode and sample mode\n",
    "    predictions_arr = [1 if x[0,0]>x[0,1] else 0 for x in all_predictions]\n",
    "    targets_arr = [1 if x[0]>x[1] else 0 for x in all_labels]\n",
    "    P = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1])\n",
    "    TP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==1])\n",
    "    FP = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==1 and targets_arr[x]==0])\n",
    "    FN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==1])\n",
    "    TN = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0 and targets_arr[x]==0])\n",
    "    N = len([1 for x in range(len(predictions_arr)) if predictions_arr[x]==0])\n",
    "\n",
    "    precission = TP/(TP+FP) if (TP+FP)!=0 else 0\n",
    "    recall = TP/(TP+FN) if (TP+FN)!=0 else 0\n",
    "    print('Precission:',f'{TP}/{TP+FP}', precission)\n",
    "    print('Recall', f'{TP}/{TP+FN}', recall)\n",
    "    print(f'P:{P},', f'TP:{TP},', f'FP:{FP},', f'FN:{FN},', f'TN:{TN},', f'N:{N}')\n",
    "\n",
    "    return precission, recall\n",
    "\n",
    "\n",
    "def save_file_datasets(file_dataset, dataset_type):\n",
    "    data = {\n",
    "        'positive_files': file_dataset[0],\n",
    "        'background_files': file_dataset[1]\n",
    "    }\n",
    "    with open(os.path.join(results_dir, f'{dataset_type}-files.json'), 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "def load_file_dataset(dataset_type):\n",
    "    with open(os.path.join(results_dir, f'{dataset_type}-files.json'), 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return (data['positive_files'], data['background_files'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, train_data, eval_data, test_data = load_files(raw_input_path, fraction_of_data)\n",
    "save_file_datasets(train_data, 'train_data')\n",
    "save_file_datasets(eval_data, 'eval_data')\n",
    "save_file_datasets(test_data, 'test_data')\n",
    "\n",
    "train_data = load_file_dataset('train_data')\n",
    "eval_data = load_file_dataset('eval_data')\n",
    "test_data = load_file_dataset('test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_and_eval():\n",
    "    train_dataset = load_data(train_data, oversampling_ratio, class_ratio, sample_limit)\n",
    "    eval_dataset = load_data(eval_data, sample_limit=eval_sample_limit)\n",
    "    test_dataset = load_data(test_data)\n",
    "\n",
    "    # Define model\n",
    "    model = FineTuningModel(base_model)\n",
    "    loss_module = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "        num_warmup_steps=int(len(train_dataset)*0.25), \n",
    "        num_training_steps=len(train_dataset)*num_epochs_)\n",
    "\n",
    "    # Prep the loaders\n",
    "    train_data_loader = data.DataLoader(train_dataset, batch_size=batch_size_, drop_last=True, shuffle=True)\n",
    "    eval_data_loader = data.DataLoader(eval_dataset, batch_size=batch_size_, drop_last=True, shuffle=True)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, optimizer, train_data_loader, loss_module, scheduler, eval_loader=eval_data_loader)\n",
    "    torch.save(model.state_dict(), f'{work_dir}/model_{model_name}_final.pickle')\n",
    "\n",
    "    # Test the model on test subset\n",
    "    test_data_loader = data.DataLoader(test_dataset, drop_last=True, batch_size=batch_size_)\n",
    "    precision, recall = eval_model(model, test_data_loader)\n",
    "    return model, precision, recall\n",
    "\n",
    "# model, precision, recall = finetune_and_eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "learning_rates = [2e-6, 2e-5]\n",
    "oversampling_ratios = [1,2,4,-1]\n",
    "class_ratios = [1,2,4,8]\n",
    "for lrx in learning_rates:\n",
    "    for orx in oversampling_ratios:\n",
    "        for crx in class_ratios:\n",
    "            learning_rate = lrx\n",
    "            oversampling_ratio = orx\n",
    "            class_ratio = crx\n",
    "            model, precision, recall = finetune_and_eval()\n",
    "            del model\n",
    "\n",
    "            print()\n",
    "            print('XXX')\n",
    "            print('learning_rate', learning_rate)\n",
    "            print('oversampling_ratio', oversampling_ratio)\n",
    "            print('class_ratio', class_ratio)\n",
    "            print('precision', precision)\n",
    "            print('recall', recall)\n",
    "            print('XXX')\n",
    "            print()\n",
    "\n",
    "            torch.cuda.empty_cache() \n",
    "            gc.collect()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model.codebert\n",
    "for param in tokenizer.parameters():\n",
    "    param.requires_grad = False\n",
    "tokenizer.eval()\n",
    "tokenizer.to(device)\n",
    "\n",
    "print('Embedding test set')\n",
    "embed_files(tokenizer, test_data[0])\n",
    "embed_files(tokenizer, test_data[1])\n",
    "print('Embedding evaluation set')\n",
    "embed_files(tokenizer, eval_data[0])\n",
    "embed_files(tokenizer, eval_data[1])\n",
    "print('Embedding train set')\n",
    "embed_files(tokenizer, train_data[0])\n",
    "embed_files(tokenizer, train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sample files to embedded files\n",
    "train_data, eval_data, test_data\n",
    "train_data_embeded_pos = map_files_to_new_repo(train_data[0])\n",
    "train_data_embeded_bac = map_files_to_new_repo(train_data[1])\n",
    "\n",
    "eval_data_embeded_pos = map_files_to_new_repo(eval_data[0])\n",
    "eval_data_embeded_bac = map_files_to_new_repo(eval_data[1])\n",
    "\n",
    "test_data_embeded_pos = map_files_to_new_repo(test_data[0])\n",
    "test_data_embeded_bac = map_files_to_new_repo(test_data[1])\n",
    "\n",
    "\n",
    "train_dataset_embeded = CommitLevelRawDataset()\n",
    "train_dataset_embeded.load_files(train_data_embeded_pos, train_data_embeded_bac)\n",
    "train_dataset_embeded = UnderSampledDataset(train_dataset_embeded, aggregator_class_ratio)\n",
    "eval_dataset_embeded = CommitLevelRawDataset()\n",
    "eval_dataset_embeded.load_files(eval_data_embeded_pos, eval_data_embeded_bac)\n",
    "test_dataset_embeded = CommitLevelRawDataset()\n",
    "test_dataset_embeded.load_files(test_data_embeded_pos, test_data_embeded_bac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = AggregatorModel()\n",
    "loss_module = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=aggregator_learning_rate, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "    num_warmup_steps=int(len(train_dataset_embeded)*0.25), \n",
    "    num_training_steps=len(train_dataset_embeded)*num_epochs_)\n",
    "\n",
    "# Prep the loaders\n",
    "train_data_embeded_loader = data.DataLoader(train_dataset_embeded, batch_size=1, drop_last=True, shuffle=True)\n",
    "eval_data_embeded_loader = data.DataLoader(eval_dataset_embeded, batch_size=1, drop_last=True, shuffle=True)\n",
    "\n",
    "train_model2(model, optimizer, train_data_embeded_loader, loss_module, scheduler, test_loader=eval_data_embeded_loader)\n",
    "torch.save(model.state_dict(), f'{work_dir}/model_aggregator_{model_name}_final.pickle')\n",
    "\n",
    "# Test the model on eval subset\n",
    "test_data_embeded_loader = data.DataLoader(test_dataset_embeded, drop_last=True, batch_size=1)\n",
    "eval_model2(model, test_data_embeded_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
